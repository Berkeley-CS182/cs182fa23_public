{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be5DnGB_QyR3"
   },
   "source": [
    "## Grokking modular arithmetic\n",
    "\n",
    "[[2301.02679] Grokking modular arithmetic](https://arxiv.org/abs/2301.02679)\n",
    "\n",
    "Given a natural number $N$, we have modular arithmetic on $\\mathbb Z_N = \\{0, 1, ..., N-1\\}$. For example, $\\mathbb Z_{12}$ is the \"clock face modular arithmetic\". The problem for our neural network is to learn binary functions on $\\mathbb Z_N$.\n",
    "\n",
    "Each such binary function can be exactly specified by a $N\\times N$ table. There are $N^{N^2}$ possible tables. Most of them are completely random and uninteresting, both for us and for neural networks, but a few are very interesting, and modular addition is one such interesting function.\n",
    "\n",
    "For example, modular addition in $\\mathbb Z_6$ has the following multiplicative table:\n",
    "\n",
    "| +  | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "|----|---|---|---|---|---|---|\n",
    "| 0  | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "| 1  | 1 | 2 | 3 | 4 | 5 | 0 |\n",
    "| 2  | 2 | 3 | 4 | 5 | 0 | 1 |\n",
    "| 3  | 3 | 4 | 5 | 0 | 1 | 2 |\n",
    "| 4  | 4 | 5 | 0 | 1 | 2 | 3 |\n",
    "| 5  | 5 | 0 | 1 | 2 | 3 | 4 |\n",
    "\n",
    "Since the dataset is exactly known, we can define a ratio $\\alpha = \\frac{|D_{train}|}{|D|}$, where $D$ is the full dataset (the multiplication table), and $D_{train}$ is the training dataset. We expect the network to learn better with higher $\\alpha$.\n",
    "\n",
    "The network architecture we would use has 3 layers:\n",
    "* Input is $x = [x^{(1)}, x^{(2)}]$. Both $x^{(1)}, x^{(2)} \\in \\mathbb R^N$ are one-hot encodings of $\\mathbb Z_N$.\n",
    "* Hidden layer activation is $z = \\phi(\\frac{1}{\\sqrt M} W^{(1)}z)$, where $\\phi$ is the activation function. Here $z \\in \\mathbb R^M$ can be of any width.\n",
    "* Output is $y = \\frac 1N W^{(2)}z$, where $y \\in \\mathbb R^N$ should be a one-hot encoding of $\\mathbb Z_N$.\n",
    "* All entries of $W^{(1)}, W^{(2)} \\sim \\mathcal N(0, 1)$ are initialized as standard gaussians.\n",
    "* $W^{(1)}, W^{(2)}$ are all the parameters of the network. There is no bias. Thus the network has $3MN$ parameters in total.\n",
    "\n",
    "An example network is drawn below, with $N=3, M = 10$.\n",
    "\n",
    "\n",
    "![](network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYTncc5amTrJ"
   },
   "source": [
    "Grokking occurs under different choices of activation functions $\\phi$, different training methods (SGD, Adam, etc), and different training set ratio $\\alpha$.\n",
    "\n",
    "The simplest example where grokking occurs is with\n",
    "* Quadratic activation function: $\\phi(t) = t^2$.\n",
    "* Full gradient descent (the entire training set is used every gradient descent step).\n",
    "* MSE loss.\n",
    "\n",
    "We use AdamW optimizer since it saves time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRbS8jHnqTo-"
   },
   "source": [
    "## Coding\n",
    "\n",
    "### Construct the dataset\n",
    "\n",
    "First we construct the dataset. The dataset is formatted as an array of triples of form $(x_1, x_2, y)$, interpreted as $x_1 + x_2 = y \\mod N$. We first construct the multiplication table, then split it randomly into two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t0-r7Mush7u"
   },
   "outputs": [],
   "source": [
    "N = 12\n",
    "M = 10\n",
    "alpha = 0.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIym5eM4pQPg",
    "outputId": "83339f7b-8c4a-4329-e9fd-a405d0590f20"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mod_add_table(N):\n",
    "    return (np.arange(N).reshape(-1,1) + np.arange(N).reshape(1,-1)) % N\n",
    "\n",
    "full_set = mod_add_table(N)\n",
    "print(full_set)\n",
    "\n",
    "# reshape to (N*N, 3), where each row is (x1, x2, x1+x2)\n",
    "pairs_and_sums = np.array([(x1, x2, full_set[x1, x2]) for x1 in range(N) for x2 in range(N)])\n",
    "\n",
    "np.random.shuffle(pairs_and_sums)\n",
    "train_set = pairs_and_sums[:int(alpha * len(pairs_and_sums))]\n",
    "test_set = pairs_and_sums[int(alpha * len(pairs_and_sums)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saZ_NHf2poNy",
    "outputId": "22439e9b-d42b-4f27-b938-6224ddfb9088"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mod_add_table(N):\n",
    "    return (np.arange(N).reshape(-1,1) + np.arange(N).reshape(1,-1)) % N\n",
    "\n",
    "full_set = mod_add_table(N)\n",
    "print(full_set)\n",
    "\n",
    "# reshape to (N*N, 3), where each row is (x1, x2, x1+x2)\n",
    "pairs_and_sums = np.array([(x1, x2, full_set[x1, x2]) for x1 in range(N) for x2 in range(N)])\n",
    "\n",
    "np.random.shuffle(pairs_and_sums)\n",
    "train_set = pairs_and_sums[:int(alpha * len(pairs_and_sums))]\n",
    "test_set = pairs_and_sums[int(alpha * len(pairs_and_sums)):]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot_encode(dataset, N):\n",
    "    inputs = dataset[:, :2]\n",
    "    targets = dataset[:, 2]\n",
    "\n",
    "    # Convert inputs and targets to one-hot vectors\n",
    "    inputs_one_hot = F.one_hot(inputs, num_classes=N).float()\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=N).float()\n",
    "\n",
    "    # Reshape inputs from [dataset_size, 2, N] to [dataset_size, 2*N]\n",
    "    inputs_one_hot = inputs_one_hot.view(-1, 2*N)\n",
    "    return inputs_one_hot, targets_one_hot\n",
    "\n",
    "train_inputs_one_hot, train_targets_one_hot = one_hot_encode(torch.from_numpy(train_set).long(), N)\n",
    "test_inputs_one_hot, test_targets_one_hot = one_hot_encode(torch.from_numpy(test_set).long(), N)\n",
    "\n",
    "for i in range(3):\n",
    "    print(train_set[i])\n",
    "    print(train_inputs_one_hot[i,:N])\n",
    "    print(train_inputs_one_hot[i,N:])\n",
    "    print(train_targets_one_hot[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dN1Fk1sJzS1c"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(N, alpha):\n",
    "    full_set = (np.arange(N).reshape(-1,1) + np.arange(N).reshape(1,-1)) % N\n",
    "    pairs_and_sums = np.array([(x1, x2, full_set[x1, x2]) for x1 in range(N) for x2 in range(N)])\n",
    "\n",
    "    np.random.shuffle(pairs_and_sums)\n",
    "    train_set = pairs_and_sums[:int(alpha * len(pairs_and_sums))]\n",
    "    test_set = pairs_and_sums[int(alpha * len(pairs_and_sums)):]\n",
    "\n",
    "    def one_hot_encode(dataset, N):\n",
    "        inputs = dataset[:, :2]\n",
    "        targets = dataset[:, 2]\n",
    "\n",
    "        # Convert inputs and targets to one-hot vectors\n",
    "        inputs_one_hot = F.one_hot(inputs, num_classes=N).float()\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=N).float()\n",
    "\n",
    "        # Reshape inputs from [dataset_size, 2, N] to [dataset_size, 2*N]\n",
    "        inputs_one_hot = inputs_one_hot.view(-1, 2*N)\n",
    "\n",
    "        return inputs_one_hot, targets_one_hot\n",
    "\n",
    "    train_inputs_one_hot, train_targets_one_hot = one_hot_encode(torch.from_numpy(train_set).long(), N)\n",
    "    test_inputs_one_hot, test_targets_one_hot = one_hot_encode(torch.from_numpy(test_set).long(), N)\n",
    "\n",
    "    return train_inputs_one_hot, train_targets_one_hot, test_inputs_one_hot, test_targets_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFotzxaazZB-"
   },
   "source": [
    "### Defining the neural network\n",
    "\n",
    "The network is defined in PyTorch and uses 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvnMNjtkQuHf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GrokNet(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super(GrokNet, self).__init__()\n",
    "        self.N = torch.tensor(N, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "        self.W1 = nn.Parameter(torch.randn(2*N, M))\n",
    "        self.W2 = nn.Parameter(torch.randn(M, N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.square(x @ self.W1  / torch.sqrt(self.M))\n",
    "        y = (z @ self.W2) / self.N\n",
    "        return y\n",
    "\n",
    "    def hidden(self, x):\n",
    "        with torch.no_grad():\n",
    "            return torch.square(x @ self.W1  / torch.sqrt(self.M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJ7WrSei2YCA",
    "outputId": "5f55be7a-249e-425f-8e33-ad291444e939"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert torch.cuda.is_available(), \"Please turn on GPU accelerator. On Google Colab, it is Runtime -> Change runtime type -> GPU\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "N, M = 40, 120\n",
    "alpha = 0.4\n",
    "\n",
    "train_inputs_one_hot, train_targets_one_hot, test_inputs_one_hot, test_targets_one_hot = generate_dataset(N, alpha)\n",
    "model = GrokNet(N=N, M=M)\n",
    "\n",
    "# Move to GPU\n",
    "model = model.to(device)\n",
    "train_inputs_one_hot = train_inputs_one_hot.to(device)\n",
    "train_targets_one_hot = train_targets_one_hot.to(device)\n",
    "test_inputs_one_hot = test_inputs_one_hot.to(device)\n",
    "test_targets_one_hot = test_targets_one_hot.to(device)\n",
    "\n",
    "# optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "epochs = 20000\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "train_accuracies = np.zeros(epochs)\n",
    "test_accuracies = np.zeros(epochs)\n",
    "\n",
    "# Training Loop\n",
    "progress_bar = trange(epochs)\n",
    "for epoch in progress_bar:\n",
    "    # Forward pass\n",
    "    outputs = model(train_inputs_one_hot)\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, train_targets_one_hot)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store loss\n",
    "    train_losses[epoch] = loss.item()\n",
    "\n",
    "    train_acc = (outputs.argmax(dim=1) == train_targets_one_hot.argmax(dim=1)).float().mean().item()\n",
    "    train_accuracies[epoch] = train_acc\n",
    "\n",
    "    # Evaluate on test set\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_inputs_one_hot)\n",
    "        test_loss = criterion(test_outputs, test_targets_one_hot)\n",
    "        test_losses[epoch] = test_loss.item()\n",
    "        test_acc = (test_outputs.argmax(dim=1) == test_targets_one_hot.argmax(dim=1)).float().mean().item()\n",
    "        test_accuracies[epoch] = test_acc\n",
    "\n",
    "    progress_bar.set_postfix({\"train accuracy\": f\"{train_acc:.4f}\", \"test accuracy\": f\"{test_acc:.4f}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "iwPb0U8m3NDR",
    "outputId": "18ad0e82-efd6-454b-a5db-c866dd61bc08"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplot_mosaic([['Accuracy'], ['Loss']], figsize=(10, 10))\n",
    "\n",
    "ax = axes['Accuracy']\n",
    "ax.plot(train_accuracies, label=\"Train accuracy\")\n",
    "ax.plot(test_accuracies, label=\"Test accuracy\")\n",
    "ax.set(title=f'Accuracy curves', xlabel='Epochs', ylabel='Accuracy')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = axes['Loss']\n",
    "ax.plot(train_losses, label=\"Train loss\")\n",
    "ax.plot(test_losses, label=\"Test loss\")\n",
    "ax.set(title=f'Learning curves', xlabel='Epochs', ylabel='MSE loss')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.suptitle(f\"N = {N}, M = {M}, α = {alpha}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezsiVNHuSjj4"
   },
   "source": [
    "## Observations\n",
    "\n",
    "* The testset accuracy curve *decreases* as trainset accuracy increases to perfection.\n",
    "* The testset accuracy curves rises only after trainset accuracy is perfect. First slowly, then rapidly (\"grokking\"). This can be quite puzzling, since if the network has really achieved perfection on the training set, then there is nothing left to learn, and so it shouldn't be able to improve any further -- and yet it does improve.\n",
    "* Perfect accuracy on trainset is reached at epoch 10x that of trainset.\n",
    "* The learning curves shows something smoother, but also something interesting: the train loss decreases monotonically, but the test loss rises, then decreases.\n",
    "* For a while, test loss rose *while* test accuracy increased!\n",
    "\n",
    "Some lessons:\n",
    "\n",
    "* Grokking might look less dramatic when plotted not by argmax-accuracy, but by MSE.\n",
    "  * See for example [[2201.02177] Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177). One wonders what they would have found if they had plotted MSE losses instead of accuracies?\n",
    "  * [[2301.05217] Progress measures for grokking via mechanistic interpretability](https://arxiv.org/abs/2301.05217) does plot train and test loss, and in this paper, the grokking appears in the loss curves as well. This seems harder to understand using our small model (they used a Transformer).\n",
    "* Train loss can decrease while test loss increase, but this trend can also be reversed. The shape of learning curves is quite complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEXJYoKcnaD8"
   },
   "source": [
    "\n",
    "## Simple interpretations\n",
    "\n",
    "Let's try to interpret what the neural network has learned. Since our network is so small, we are able to actually plot every single one of its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "zJivoqBlN-fe",
    "outputId": "e248b88c-cc59-45da-ff11-a9c5bd022746"
   },
   "outputs": [],
   "source": [
    "def visualize_network(W1, W2, title):\n",
    "    # Find minimal value for the separator\n",
    "    min_val = min(W1.min(), W2.min())\n",
    "\n",
    "    # Create a \"black\" separator\n",
    "    separator = np.full((1, W1.shape[1]), min_val)\n",
    "\n",
    "    # Stack everything together\n",
    "    W = np.concatenate((W1, separator, W2), axis=0)\n",
    "\n",
    "    # Now you can use plt.imshow to visualize the stacked matrices\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    cax = plt.imshow(W, cmap='viridis')\n",
    "    plt.colorbar(cax)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "W1 = model.W1.detach().cpu().numpy()\n",
    "W2 = model.W2.T.detach().cpu().numpy()\n",
    "visualize_network(W1, W2, 'W1 and W2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI4GEBFBSdv5"
   },
   "source": [
    "We notice some strange bands that look strongly suggestive of sine waves. Let's try to order them, left to right, according to how fast they are oscillating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "YjWem0fxQrJh",
    "outputId": "2c1360af-df1b-4e3d-92e3-d6a2e968c588"
   },
   "outputs": [],
   "source": [
    "def visualize_network_std(W1, W2):\n",
    "    # Compute the standard deviation along each column in W1\n",
    "    std_devs = np.std(W1, axis=0)\n",
    "\n",
    "    # Get the order of indices that would sort the standard deviations\n",
    "    order = np.argsort(std_devs)\n",
    "\n",
    "    # Reorder the columns of W1 and rows of W2 using these indices\n",
    "    W1_ordered = W1[:, order]\n",
    "    W2_ordered = W2[:, order]\n",
    "\n",
    "    visualize_network(W1_ordered, W2_ordered, 'W1 and W2, ordered by std')\n",
    "\n",
    "visualize_network_std(W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "2xmVmnaZQuQO",
    "outputId": "895ac69f-4c25-48cc-bd59-80b0b4b3463a"
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "\n",
    "def get_fft_order(W1, W2):\n",
    "    # Compute power spectrum for each column in W1 (split to two) and W2\n",
    "    power_spectrums = np.abs(fft(W1[:N,:], axis=0))**2\n",
    "    power_spectrums += np.abs(fft(W1[N:,:], axis=0))**2\n",
    "    power_spectrums += np.abs(fft(W2, axis=0))**2\n",
    "\n",
    "    # Find the peak frequency for each column\n",
    "    peak_freqs = np.argmax(power_spectrums, axis=0)\n",
    "\n",
    "    # Get the order of indices that would sort the peak frequencies\n",
    "    order = np.argsort(peak_freqs)\n",
    "    return order\n",
    "\n",
    "def visualize_network_fft(W1, W2):\n",
    "    order = get_fft_order(W1, W2)\n",
    "    W1_ordered = W1[:, order]\n",
    "    W2_ordered = W2[:, order]\n",
    "\n",
    "    visualize_network(W1_ordered, W2_ordered, 'W1 and W2, ordered by peak frequency')\n",
    "\n",
    "visualize_network_fft(W1, W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAAjgDW7nR93"
   },
   "source": [
    "When performing experiments, it is of vital importance to check the null hypothesis. While we seem to see some very suggestive pictures, we should make sure it's not an artifact of our experimental technique.\n",
    "\n",
    "So, let's generate some random networks of the same architecture, but untrained. We should expect them to look like noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HB53c3wZtVjt",
    "outputId": "fa466887-5bcb-44b2-b1fb-4be225a3442a"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "l__37oAHZUwF",
    "outputId": "96ce5fb6-e325-4bc4-9e83-3485fa5390b4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def pre_activation_mapping(N, k, W1):\n",
    "    activations = np.zeros((N, N))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            one_hot_i = F.one_hot(torch.tensor(i), num_classes=N).float().view(1,-1)\n",
    "            one_hot_j = F.one_hot(torch.tensor(j), num_classes=N).float().view(1,-1)\n",
    "            combined = torch.cat([one_hot_i, one_hot_j], dim=1)\n",
    "            z = combined @ W1 / np.sqrt(model.M)\n",
    "            activations[i, j] = z[0, k].item()\n",
    "    return activations\n",
    "\n",
    "activations = pre_activation_mapping(40, 10, W1)\n",
    "plt.imshow(activations, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aqu_LugzaXPy"
   },
   "outputs": [],
   "source": [
    "def generate_activation_plot(N, W1, order):\n",
    "    margin = 3\n",
    "    N1, N2 = 10, 10\n",
    "    full_array = np.full(((N + margin) * N1, (N + margin) * N2), -0.6)\n",
    "\n",
    "    for n in trange(N1 * N2):\n",
    "        n1, n2 = n // N2, n % N2\n",
    "        k = order[N1 * n1 + n2]\n",
    "        activations = pre_activation_mapping(N, k, W1)\n",
    "        full_array[n1 * (N + margin): n1 * (N + margin) + N, n2 * (N + margin): n2 * (N + margin) + N] = activations\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(full_array, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T7PiTz0FwAhA",
    "outputId": "987a9e30-2890-45b5-dd1e-a69ffe5cd880"
   },
   "outputs": [],
   "source": [
    "order = get_fft_order(W1, W2)\n",
    "generate_activation_plot(40, W1, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rStGP--iwYHS",
    "outputId": "564b5a65-b34c-43c6-e66c-19f5eda4b1a8"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbIPGdjoNh-9"
   },
   "source": [
    "We see that the network has learned some sine waves. It seems to be a robust fact that networks trained to do modular arithmetics, with one-hot encoding, learns to use trigonometry for this task. (the use of one-hot encoding seems very relevant, as [noted here](https://www.lesswrong.com/posts/tdENX8dzdro8PXAzP/short-remark-on-the-subjective-mathematical-naturalness-of))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkY9uCPCkWnH"
   },
   "source": [
    "## Extensions\n",
    "\n",
    "Try this activity with other configurations and report what you found. Suggestions:\n",
    "* Modular multiplication.\n",
    "* Random operation (as a null hypothesis).\n",
    "* Different activation functions (sine, ReLU).\n",
    "* Different accelerators (SGD, Adam, etc)\n",
    "* Two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9Y-Mv1cnnPj",
    "outputId": "d80cbbc1-58f5-4f91-ece4-5f82e37d7d1b"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "9U877n2czRMd",
    "outputId": "b58ca0e9-c205-4520-c772-799ad3247eca"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "# No need to fill in all the TODO blocks. Those are just here\n",
    "# because the automatic code-deleter can only remove codes from one cell\n",
    "# at a time, and I have code spread over several cells.\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cIj3-cEbzttT",
    "outputId": "909c4403-dbde-4774-9689-ec91ed13bc5c"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yNgGeE6uz3a9",
    "outputId": "291025e1-6d3e-482a-a63d-c01f86c4c7af"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: your answer here\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcAhBnrDVj9B"
   },
   "source": [
    "## Some other quotes\n",
    "\n",
    "[[2301.02679] Grokking modular arithmetic](https://arxiv.org/abs/2301.02679)\n",
    "\n",
    "> In particular, random feature models such as infinitely-wide neural networks (in the NTK regime) do not exhibit grokking, at least on the tasks that involve modular functions.\n",
    ">\n",
    "> In our minimal setup, the simplest explanation for grokking is that once training loss reached a certain value, the only way to further minimize it is to start learning the right features.\n",
    "\n",
    "Geirhos, Robert, et al. \"Shortcut learning in deep neural networks.\" Nature Machine Intelligence 2.11 (2020): 665-673.\n",
    "\n",
    "> many of deep learning’s failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike.\n",
    "\n",
    "See particularly Figure 3.\n",
    "\n",
    "![](taxonomy_decision_rules.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ot-a3ojU5rE"
   },
   "source": [
    "[The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (Richard Sutton, March 13, 2019)\n",
    "\n",
    "> One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning.\n",
    "\n",
    "[The Scaling Hypothesis · Gwern.net](https://gwern.net/scaling-hypothesis#why-does-pretraining-work)\n",
    "\n",
    "> Early on in training, a model learns the crudest levels: that some letters like ‘e’ are more frequent than others like ‘z’, that every 5 characters or so there is a space, and so on. It goes from predicted uniformly-distributed bytes to what looks like Base-60 encoding—alphanumeric gibberish. As crude as this may be, it’s enough to make quite a bit of absolute progress: a random predictor needs 8 bits to ‘predict’ a byte/character, but just by at least matching letter and space frequencies, it can almost halve its error to around 5 bits...\n",
    ">\n",
    "> ... a sample will state that someone is “alive” and then 10 sentences later, use the word “dead”, or it will digress into an irrelevant argument instead of the expected next argument, or someone will do something physically improbable, or it may just continue for a while without seeming to get anywhere.All of these errors are far less than <0.02 bits per character; we are now talking not hundredths of bits per characters but less than ten-thousandths.The pretraining thesis argues that this can go even further: we can compare this performance directly with humans doing the same objective task, who can achieve closer to 0.7 bits per character. What is in that missing >0.4?\n",
    ">\n",
    "> The last bits are deepest. The implication here is that the final few bits are the most valuable bits, which require the most of what we think of as intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-W9rQpdVVUt3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
