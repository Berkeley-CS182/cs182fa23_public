{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "If you are working on this assignment using Google Colab, please execute the codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount your Google Drive\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set up mount symlink\n",
    "\n",
    "DRIVE_PATH = '/content/gdrive/My\\ Drive/cs182hw3_sp23'\n",
    "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
    "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
    "  %mkdir $DRIVE_PATH\n",
    "\n",
    "## the space in `My Drive` causes some issues,\n",
    "## make a symlink to avoid this\n",
    "SYM_PATH = '/content/cs182hw3'\n",
    "if not os.path.exists(SYM_PATH):\n",
    "  !ln -s $DRIVE_PATH $SYM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "\n",
    "!pip install numpy==1.21.6 imageio==2.9.0 matplotlib==3.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clone homework repo\n",
    "\n",
    "%cd $SYM_PATH\n",
    "if not os.path.exists(\"cs182hw3\"):\n",
    "  !git clone https://github.com/Berkeley-CS182/cs182hw3.git\n",
    "%cd cs182hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configure Jupyter Notebook\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Convolutional Neural Networks using PyTorch\n",
    "\n",
    "In this notebook we will put everything together you've learned: affine layers, relu layers, conv layers, max-pooling, (spatial) batch norm, and dropout, and train CNNs on CIFAR-100.\n",
    "\n",
    "However, our implementation of these modules in NumPy are quite inefficient---especially convolutional layers. Therefore, we use PyTorch with GPU in this coding assignment.\n",
    "\n",
    "Make sure you have access to GPUs when running this notebook. On Google Colab, you can switch to a GPU runtime by clicking \"Runtime\" - \"Change Runtime Type\" - \"GPU\" in the menu on the top of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"submission_logs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "# make sure GPU is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data",
    "\n",
    "In this cell, we load and visualize the CIFAR100 dataset. Note that we apply data augmentation (random horizontal flip) to the training dataset:",
    "\n",
    "```python",
    "transforms.RandomHorizontalFlip()",
    "```",
    "\n",
    "Data augmentation is a popular technique in machine learning and computer vision that involves generating additional training data to improve the performance of a model. One common form of data augmentation for image data is random horizontal flipping, which involves flipping an image horizontally with a 50% chance during training. This technique is often used to increase the variability of the training data and to help the model generalize better to new, unseen images. By randomly flipping images, the model is exposed to a wider range of orientations and can better learn to recognize features that are invariant to horizontal flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),   # convert image to PyTorch Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        # normalize to [-1.0, 1.0] (originally [0.0, 1.0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.RandomHorizontalFlip()   # data augmentation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR100(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "valid_test_data = datasets.CIFAR100(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=valid_test_transform,\n",
    ")\n",
    "\n",
    "# split original test data to valid data and test data\n",
    "valid_data = list(valid_test_data)[::2]\n",
    "test_data = list(valid_test_data)[1::2]\n",
    "\n",
    "classes = [\n",
    "    \"apple\",\n",
    "    \"aquarium_fish\",\n",
    "    \"baby\",\n",
    "    \"bear\",\n",
    "    \"beaver\",\n",
    "    \"bed\",\n",
    "    \"bee\",\n",
    "    \"beetle\",\n",
    "    \"bicycle\",\n",
    "    \"bottle\",\n",
    "    \"bowl\",\n",
    "    \"boy\",\n",
    "    \"bridge\",\n",
    "    \"bus\",\n",
    "    \"butterfly\",\n",
    "    \"camel\",\n",
    "    \"can\",\n",
    "    \"castle\",\n",
    "    \"caterpillar\",\n",
    "    \"cattle\",\n",
    "    \"chair\",\n",
    "    \"chimpanzee\",\n",
    "    \"clock\",\n",
    "    \"cloud\",\n",
    "    \"cockroach\",\n",
    "    \"couch\",\n",
    "    \"cra\",\n",
    "    \"crocodile\",\n",
    "    \"cup\",\n",
    "    \"dinosaur\",\n",
    "    \"dolphin\",\n",
    "    \"elephant\",\n",
    "    \"flatfish\",\n",
    "    \"forest\",\n",
    "    \"fox\",\n",
    "    \"girl\",\n",
    "    \"hamster\",\n",
    "    \"house\",\n",
    "    \"kangaroo\",\n",
    "    \"keyboard\",\n",
    "    \"lamp\",\n",
    "    \"lawn_mower\",\n",
    "    \"leopard\",\n",
    "    \"lion\",\n",
    "    \"lizard\",\n",
    "    \"lobster\",\n",
    "    \"man\",\n",
    "    \"maple_tree\",\n",
    "    \"motorcycle\",\n",
    "    \"mountain\",\n",
    "    \"mouse\",\n",
    "    \"mushroom\",\n",
    "    \"oak_tree\",\n",
    "    \"orange\",\n",
    "    \"orchid\",\n",
    "    \"otter\",\n",
    "    \"palm_tree\",\n",
    "    \"pear\",\n",
    "    \"pickup_truck\",\n",
    "    \"pine_tree\",\n",
    "    \"plain\",\n",
    "    \"plate\",\n",
    "    \"poppy\",\n",
    "    \"porcupine\",\n",
    "    \"possum\",\n",
    "    \"rabbit\",\n",
    "    \"raccoon\",\n",
    "    \"ray\",\n",
    "    \"road\",\n",
    "    \"rocket\",\n",
    "    \"rose\",\n",
    "    \"sea\",\n",
    "    \"seal\",\n",
    "    \"shark\",\n",
    "    \"shrew\",\n",
    "    \"skunk\",\n",
    "    \"skyscraper\",\n",
    "    \"snail\",\n",
    "    \"snake\",\n",
    "    \"spider\",\n",
    "    \"squirrel\",\n",
    "    \"streetcar\",\n",
    "    \"sunflower\",\n",
    "    \"sweet_pepper\",\n",
    "    \"table\",\n",
    "    \"tank\",\n",
    "    \"telephone\",\n",
    "    \"television\",\n",
    "    \"tiger\",\n",
    "    \"tractor\",\n",
    "    \"train\",\n",
    "    \"trout\",\n",
    "    \"tulip\",\n",
    "    \"turtle\",\n",
    "    \"wardrobe\",\n",
    "    \"whale\",\n",
    "    \"willow_tree\",\n",
    "    \"wolf\",\n",
    "    \"woman\",\n",
    "    \"worm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "valid_dataloader = utils.data.DataLoader(valid_data, batch_size=5)\n",
    "\n",
    "for X, y in valid_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of 5 images in the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(valid_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print('   '.join(f'{classes[labels[j]]:5s}' for j in range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Architecture\n",
    "\n",
    "**Complete the code in `dl_pytorch/model.py`** to finish the implementation of a convolutional neural network with batch normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_pytorch.model import NeuralNetwork\n",
    "\n",
    "model = NeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "assert len(model.state_dict()) == 10\n",
    "assert model.conv1.weight.shape == torch.Size([16, 3, 3, 3])\n",
    "assert model.conv1.bias.shape == torch.Size([16])\n",
    "assert model.conv2.weight.shape == torch.Size([32, 16, 3, 3])\n",
    "assert model.conv2.bias.shape == torch.Size([32])\n",
    "assert model.conv3.weight.shape == torch.Size([64, 32, 3, 3])\n",
    "assert model.conv3.bias.shape == torch.Size([64])\n",
    "assert model.fc1.weight.shape == torch.Size([256, 1024])\n",
    "assert model.fc1.bias.shape == torch.Size([256])\n",
    "assert model.fc2.weight.shape == torch.Size([100, 256])\n",
    "assert model.fc2.bias.shape == torch.Size([100])\n",
    "assert model(torch.randn(9, 3, 32, 32)).shape == torch.Size([9, 100])\n",
    "\n",
    "model = NeuralNetwork(do_batchnorm=True, p_dropout=0.1)\n",
    "assert len(model.state_dict()) == 25\n",
    "assert model.bn1.weight.shape == model.bn1.bias.shape == torch.Size([16])\n",
    "assert model.bn2.weight.shape == model.bn2.bias.shape == torch.Size([32])\n",
    "assert model.bn3.weight.shape == model.bn3.bias.shape == torch.Size([64])\n",
    "assert model(torch.randn(11, 3, 32, 32)).shape == torch.Size([11, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "\n",
    "Complete the code cells below to train your neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: complete the following code for backpropagation and gradient\n",
    "        #  update of a single step.\n",
    "        # Hint: 3 lines\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Evaluation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(params, optim_type, lr, momentum, lr_decay, l2_reg):\n",
    "    if optim_type == \"sgd\":\n",
    "        optimizer = optim.SGD(params, lr=lr, momentum=0.0, weight_decay=l2_reg)\n",
    "    elif optim_type == \"sgd_momentum\":\n",
    "        optimizer = optim.SGD(params, lr=lr, momentum=momentum,\n",
    "                              weight_decay=l2_reg)\n",
    "    elif optim_type == \"adam\":\n",
    "        optimizer = optim.AdamW(params, lr=lr, betas=(momentum, 0.999),\n",
    "                                weight_decay=l2_reg)\n",
    "    else:\n",
    "        raise ValueError(optim_type)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, lr_decay)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network. It should achieve at least 35% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(hp, nn_cls, save_prefix):\n",
    "    print(\"Hyperparameters:\", hp)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = nn_cls(do_batchnorm=hp.do_batchnorm, p_dropout=hp.p_dropout).cuda()\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = utils.data.DataLoader(\n",
    "        training_data, batch_size=hp.batch_size)\n",
    "    valid_dataloader = utils.data.DataLoader(\n",
    "        valid_data, batch_size=hp.batch_size)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer, scheduler = get_optimizer(\n",
    "        model.parameters(), hp.optim_type, hp.lr, hp.momentum, hp.lr_decay,\n",
    "        hp.l2_reg)\n",
    "    \n",
    "    for t in range(hp.epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test(valid_dataloader, model, loss_fn)\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    print(f\"Saving the model to submission_logs/{save_prefix}.pt\")\n",
    "    torch.save(model.state_dict(), f\"submission_logs/{save_prefix}.pt\")\n",
    "    return model\n",
    "\n",
    "def eval_on_test(hp, model, save_prefix):\n",
    "    train_dataloader = utils.data.DataLoader(\n",
    "        training_data, batch_size=hp.batch_size)\n",
    "    test_dataloader = utils.data.DataLoader(\n",
    "        test_data, batch_size=hp.batch_size)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(\"Evaluating on the test set\")\n",
    "    test_acc = test(test_dataloader, model, loss_fn)\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Parameter count: {}\".format(n_params))\n",
    "    n_steps = len(train_dataloader) * hp.epochs\n",
    "    print(\"Training steps: {}\".format(n_steps))\n",
    "    with open(f\"submission_logs/{save_prefix}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"test_acc\": test_acc,\n",
    "            \"hparams\": hp.__dict__,\n",
    "            \"n_params\": n_params,\n",
    "            \"n_steps\": n_steps\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_pytorch.hparams import HP as hp_base\n",
    "\n",
    "model = run_training(hp_base, NeuralNetwork, \"model\")\n",
    "eval_on_test(hp_base, model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network with batch norm and dropout. It should achieve at least 38% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_pytorch.hparams_bn_drop import HP as hp_bn_drop\n",
    "\n",
    "model = run_training(hp_bn_drop, NeuralNetwork, \"model_bn_drop\")\n",
    "eval_on_test(hp_bn_drop, model, \"model_bn_drop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design your own neural network\n",
    "\n",
    "\n",
    "It's time to showcase your deep learning skills! In this assignment, you will be designing your own neural network using PyTorch. Your task is to **implement your neural network design in the files `dl_pytorch/my_model.py` and `dl_pytorch/hparams_my_model.py`**. The goal is to achieve a test accuracy of **44%** or higher.\n",
    "\n",
    "To ensure reproducibility and to maintain the focus of the assignment, please adhere to the following rules:\n",
    "\n",
    "1. Do not modify the code in the Jupyter Notebook cell or other cells that this cell depends on. It means that you cannot change data processing, the training loop, and the random seed. The emphasis of this assignment is on the model architecture and hyperparameter tuning.\n",
    "\n",
    "1. The number of model parameters must not exceed `1,000,000`.\n",
    "\n",
    "1. The total number of training steps should be no more than `20,000`.\n",
    "\n",
    "1. The maximum number of training epochs is `10`.\n",
    "\n",
    "1. Please refrain from using any pre-trained models or other downloaded assets.\n",
    "\n",
    "Your test accuracy will be displayed on the Gradescope leaderboard. Please note that your rank on the leaderboard does not affect your grade. In order to receive full credit for this part of the assignment, you only need to abide by the rules outlined above and achieve a minimum test accuracy of 44%. Your grade will be scaled linearly, with a score of 0 for a test accuracy of 38% and full credit for a test accuracy of 44% or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_pytorch.my_model import MyNeuralNetwork\n",
    "from dl_pytorch.hparams_my_model import HP as hp_my_model\n",
    "\n",
    "model = run_training(hp_my_model, MyNeuralNetwork, \"model_my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are ready to eval on test set, run this cell\n",
    "# WARNING: In real-world applications, it is a bad practice to evaluate\n",
    "#          frequently on the test set because the model will then perform poorly\n",
    "#          on new, unseen data even if it achieves a high test accuracy.\n",
    "eval_on_test(hp_my_model, model, \"model_my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question:\n",
    "\n",
    "**Briefly describe your neural network design and the procedure of hyperparameter tuning.** Please include the answer of this question in your written assignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect your submissions\n",
    "\n",
    "The following command will collect your solutions generated by both notebooks.\n",
    "\n",
    "On Colab, after running the following cell, you can download your submissions from the `Files` tab, which can be opened by clicking the file icon on the left hand side of the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f cs182hw3_submission.zip\n",
    "!zip -r cs182hw3_submission.zip . -x \"*.git*\" \"*deeplearning/datasets*\" \"data*\" \"*.ipynb_checkpoints*\" \"*README.md\" \".env/*\" \"*.pyc\" \"*deeplearning/build/*\" \"*__pycache__/*\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
