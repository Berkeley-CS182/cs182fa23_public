{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Giacfc7eHrUJ"
   },
   "source": [
    "# Visualizing features from local linearization of neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufJzj4IOHrUO"
   },
   "outputs": [],
   "source": [
    "!pip install ipympl torchviz\n",
    "!pip install torch==1.13 --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# restart your runtime after this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKsd5p0jHrUP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from ipywidgets import fixed, interactive, widgets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ACCCVC-HrUQ"
   },
   "outputs": [],
   "source": [
    "# enable matplotlib widgets;\n",
    "\n",
    "# on Google Colab\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzhYAZbiHrUQ"
   },
   "outputs": [],
   "source": [
    "def to_torch(x):\n",
    "    return torch.from_numpy(x).float()\n",
    "\n",
    "\n",
    "def to_numpy(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def plot_data(X, y, X_test, y_test):\n",
    "    clip_bound = 2.5\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-clip_bound, clip_bound)\n",
    "    plt.scatter(X[:, 0], y, c='darkorange', s=40.0, label='training data points')\n",
    "    plt.plot(X_test, y_test, '--', color='royalblue', linewidth=2.0, label='Ground truth')\n",
    "\n",
    "\n",
    "def plot_relu(bias, slope):\n",
    "    plt.scatter([-bias / slope], 0, c='darkgrey', s=40.0)\n",
    "    if slope > 0 and bias < 0:\n",
    "        plt.plot([0, -bias / slope, 1], [0, 0, slope * (1 - bias)], ':')\n",
    "    elif slope < 0 and bias > 0:\n",
    "        plt.plot([0, -bias / slope, 1], [-bias * slope, 0, 0], ':')\n",
    "\n",
    "\n",
    "def plot_relus(params):\n",
    "    slopes = to_numpy(params[0]).ravel()\n",
    "    biases = to_numpy(params[1])\n",
    "    for relu in range(biases.size):\n",
    "        plot_relu(biases[relu], slopes[relu])\n",
    "\n",
    "\n",
    "def plot_function(X_test, net):\n",
    "    y_pred = net(to_torch(X_test))\n",
    "    plt.plot(X_test, to_numpy(y_pred), '-', color='forestgreen', label='prediction')\n",
    "\n",
    "\n",
    "def plot_update(X, y, X_test, y_test, net, state=None):\n",
    "    if state is not None:\n",
    "        net.load_state_dict(state)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plot_relus(list(net.parameters()))\n",
    "    plot_function(X_test, net)\n",
    "    plot_data(X, y, X_test, y_test)\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "def train_network(X, y, X_test, y_test, net, optim, n_steps, save_every, initial_weights=None, verbose=False):\n",
    "    loss = torch.nn.MSELoss()\n",
    "    y_torch = to_torch(y.reshape(-1, 1))\n",
    "    X_torch = to_torch(X)\n",
    "    if initial_weights is not None:\n",
    "        net.load_state_dict(initial_weights)\n",
    "    history = {}\n",
    "    for s in range(n_steps):\n",
    "        subsample = np.random.choice(y.size, y.size // 5)\n",
    "        step_loss = loss(y_torch[subsample], net(X_torch[subsample, :]))\n",
    "        optim.zero_grad()\n",
    "        step_loss.backward()\n",
    "        optim.step()\n",
    "        if (s + 1) % save_every == 0 or s == 0:\n",
    "#             plot_update(X, y, X_test, y_test, net)\n",
    "            history[s + 1] = {}\n",
    "            history[s + 1]['state'] = copy.deepcopy(net.state_dict())\n",
    "            with torch.no_grad():\n",
    "                test_loss = loss(to_torch(y_test.reshape(-1, 1)), net(to_torch(X_test)))\n",
    "            history[s + 1]['train_error'] = to_numpy(step_loss).item()\n",
    "            history[s + 1]['test_error'] = to_numpy(test_loss).item()\n",
    "            if verbose:\n",
    "                print(\"SGD Iteration %d\" % (s + 1))\n",
    "                print(\"\\tTrain Loss: %.3f\" % to_numpy(step_loss).item())\n",
    "                print(\"\\tTest Loss: %.3f\" % to_numpy(test_loss).item())\n",
    "            else:\n",
    "                # Print update every 10th save point\n",
    "                if (s + 1) % (save_every * 10) == 0:\n",
    "                    print(\"SGD Iteration %d\" % (s + 1))\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_test_train_errors(history):\n",
    "    sample_points = np.array(list(history.keys()))\n",
    "    etrain = [history[s]['train_error'] for s in history]\n",
    "    etest = [history[s]['test_error'] for s in history]\n",
    "    plt.plot(sample_points / 1e3, etrain, label='Train Error')\n",
    "    plt.plot(sample_points / 1e3, etest, label='Test Error')\n",
    "    plt.xlabel(\"Iterations (1000's)\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "def make_iter_slider(iters):\n",
    "    # print(iters)\n",
    "    return widgets.SelectionSlider(\n",
    "        options=iters,\n",
    "        value=1,\n",
    "        description='SGD Iterations: ',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "\n",
    "def history_interactive(history, idx, X, y, X_test, y_test, net):\n",
    "    plot_update(X, y, X_test, y_test, net, state=history[idx]['state'])\n",
    "    plt.show()\n",
    "    print(\"Train Error: %.3f\" % history[idx]['train_error'])\n",
    "    print(\"Test Error: %.3f\" % history[idx]['test_error'])\n",
    "\n",
    "\n",
    "def make_history_interactive(history, X, y, X_test, y_test, net):\n",
    "    sample_points = list(history.keys())\n",
    "    return interactive(history_interactive,\n",
    "                       history=fixed(history),\n",
    "                       idx=make_iter_slider(sample_points),\n",
    "                       X=fixed(X),\n",
    "                       y=fixed(y),\n",
    "                       X_test=fixed(X_test),\n",
    "                       y_test=fixed(y_test),\n",
    "                       net=fixed(net))\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8InbMWunHrUV"
   },
   "source": [
    "# Generate Training and Test Data\n",
    "\n",
    "We are using piecewise linear function. Our training data has added noise $y = f(x) + \\epsilon,\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The test data is noise free.\n",
    "\n",
    "_Once you have gone through the discussion once you may wish to adjust the number of training samples and noise variance to see how gradient descent behaves under the new conditions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgiG1--NHrUV"
   },
   "outputs": [],
   "source": [
    "f_type = 'piecewise_linear'\n",
    "\n",
    "def f_true(X, f_type):\n",
    "    if f_type == 'sin(20x)':\n",
    "        return np.sin(20 * X[:,0])\n",
    "    else:\n",
    "        TenX = 10 * X[:,0]\n",
    "        _ = 12345\n",
    "        return (TenX - np.floor(TenX)) * np.sin(_ * np.ceil(TenX)) - (TenX - np.ceil(TenX)) * np.sin(_ * np.floor(TenX))\n",
    "\n",
    "n_features = 1\n",
    "n_samples = 200\n",
    "sigma = 0.1\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Generate train data\n",
    "X = np.sort(rng.rand(n_samples, n_features), axis=0)\n",
    "y = f_true(X, f_type) + rng.randn(n_samples) * sigma\n",
    "\n",
    "# Generate NOISELESS test data\n",
    "X_test = np.concatenate([X.copy(), np.expand_dims(np.linspace(0., 1., 1000), axis=1)])\n",
    "X_test = np.sort(X_test, axis=0)\n",
    "y_test = f_true(X_test, f_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THtHT2T4HrUX"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSyroCMtHrUY"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVUXmkd3HrUZ"
   },
   "source": [
    "# Define the Neural Networks\n",
    "\n",
    "We will learn the piecewise linear target function using a simple 1-hidden layer neural network with ReLU non-linearity, defined by\n",
    "$$ \\hat{y} = \\mathbf{W}^{(2)} \\Phi \\left( \\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)} \\right) + \\mathbf{b}^{(2)} $$\n",
    "where $\\Phi(x) = ReLU(x)$ and superscripts refer to indices, not the power operator.\n",
    "\n",
    "We will also create two SGD optimizers to allow us to choose whether to train all parameters or only the linear output layer's parameters. Note that we use separate learning rates for the two version of training. There is too much variance in the gradients when training all layers to use a large learning rate, so we have to decrease it.\n",
    "\n",
    "We will modify the default initialization of the biases so that the ReLU elbows are all inside the region we are interested in.\n",
    "\n",
    "We create several versions of this network with varying widths to explore how hidden layer width impacts learning performance.\n",
    "\n",
    "_Once you have gone through the discussion once you may wish to train networks with even larger widths to see how they behave under the three different training paradigms in this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5gY3Ou3HrUZ"
   },
   "outputs": [],
   "source": [
    "# Don't rerun this cell after training or you will lose all your work\n",
    "nets_by_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqAZu5-WHrUa"
   },
   "outputs": [],
   "source": [
    "widths = [10, 20, 40]\n",
    "for width in widths:\n",
    "    # Define a 1-hidden layer ReLU nonlinearity network\n",
    "    net = nn.Sequential(nn.Linear(1, width),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(width, 1))\n",
    "    loss = nn.MSELoss()\n",
    "    # Get trainable parameters\n",
    "    weights_all = list(net.parameters())\n",
    "    # Get the output weights alone\n",
    "    weights_out = weights_all[2:]\n",
    "    # Adjust initial biases so elbows are in [0,1]\n",
    "    elbows = np.sort(np.random.rand(width))\n",
    "    new_biases = -elbows * to_numpy(weights_all[0]).ravel()\n",
    "    weights_all[1].data = to_torch(new_biases)\n",
    "    # Create SGD optimizers for outputs alone and for all weights\n",
    "    lr_out = 0.2\n",
    "    lr_all = 0.02\n",
    "    opt_all = torch.optim.SGD(params=weights_all, lr=lr_all)\n",
    "    opt_out = torch.optim.SGD(params=weights_out, lr=lr_out)\n",
    "    # Save initial state for comparisons\n",
    "    initial_weights = copy.deepcopy(net.state_dict())\n",
    "    # print(\"Initial Weights\", initial_weights)\n",
    "    nets_by_size[width] = {'net': net, 'opt_all': opt_all,\n",
    "                           'opt_out': opt_out, 'init': initial_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXk00JfvHrUb"
   },
   "source": [
    "# Train the neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVMdbT03HrUb"
   },
   "outputs": [],
   "source": [
    "n_steps = 150000\n",
    "save_every = 1000\n",
    "t0 = time.time()\n",
    "for w in widths:\n",
    "    print(\"-\"*40)\n",
    "    print(\"Width\", w)\n",
    "    new_net = nn.Sequential(nn.Linear(1, w),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(w, 1))\n",
    "    new_net.load_state_dict(nets_by_size[w]['net'].state_dict().copy())\n",
    "    opt_all = torch.optim.SGD(params=new_net.parameters(), lr=lr_all)\n",
    "    initial_weights = nets_by_size[w]['init']\n",
    "    history_all = train_network(X, y, X_test, y_test,\n",
    "                            new_net, optim=opt_all,\n",
    "                            n_steps=n_steps, save_every=save_every,\n",
    "                            initial_weights=initial_weights,\n",
    "                            verbose=False)\n",
    "    nets_by_size[w]['trained_net'] = new_net\n",
    "    nets_by_size[w]['hist_all'] = history_all\n",
    "    print(\"Width\", w)\n",
    "    plot_test_train_errors(history_all)\n",
    "t1 = time.time()\n",
    "print(\"-\"*40)\n",
    "print(\"Trained all layers in %.1f minutes\" % ((t1 - t0) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5u1kSTSHrUc"
   },
   "source": [
    "# (a) Visualize Gradients\n",
    "\n",
    "Visualize the features corresponding to\n",
    "$\\frac{\\partial}{\\partial w_i^{(1)}} y(x)$\n",
    "and\n",
    "$\\frac{\\partial}{\\partial b_i^{(1)}} y(x)$\n",
    "where\n",
    "$w^{(1)}_i$\n",
    "are\n",
    "the first hidden layer's weights and the\n",
    "$b^{(1)}_i$\n",
    "are the first hidden layer's biases. These derivatives should be evaluated at\n",
    "at least both the random initialization and the final trained\n",
    "network. When visualizing these features, plot them as a function\n",
    "of the scalar input $x$, the same way that the notebook plots the\n",
    "constituent \"elbow\" features that are the outputs of the\n",
    "penultimate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw9_omspHrUc"
   },
   "outputs": [],
   "source": [
    "def backward_and_plot_grad(X, model, vis_name='all', title='', legend=False):\n",
    "    \"\"\"\n",
    "    Run backpropagation on `model` using `X` as the input\n",
    "    to compute the gradient w.r.t. parameters of `y`,\n",
    "    and then visualize collected gradients according to `vis_name`\n",
    "    \"\"\"\n",
    "    width = model[0].out_features  # the width is the number of hidden units.\n",
    "    gradients = np.zeros((width, X.shape[0]))\n",
    "    num_pts = 0\n",
    "    gradient_collect, vis_collect = { }, { }\n",
    "    for x in X:\n",
    "        y = model(to_torch(x))\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Complete the following part to run backpropagation. (2 lines)\n",
    "        # Hint: Remember to set grad to zero before backpropagation\n",
    "        ########################################################################\n",
    "        pass\n",
    "        ########################################################################\n",
    "\n",
    "        # collect gradients from `p.grad.data`\n",
    "        for n, p in model.named_parameters():\n",
    "            for w_idx, w_grad in enumerate( p.grad.data.reshape(-1) ):\n",
    "                if f'{n}.{w_idx}' not in gradient_collect:\n",
    "                    gradient_collect[ f'{n}.{w_idx}' ] = {'x':[], 'y': []}\n",
    "                if vis_name == 'all' or vis_name == n:\n",
    "                    if f'{n}.{w_idx}' not in vis_collect:\n",
    "                        vis_collect[f'{n}.{w_idx}'] = True\n",
    "                gradient_collect[ f'{n}.{w_idx}' ]['y'].append( w_grad.item() )\n",
    "                gradient_collect[ f'{n}.{w_idx}' ]['x'].append( x )\n",
    "\n",
    "    for w_n in vis_collect:\n",
    "        # we assume that X is sorted, so we use line plot\n",
    "        plt.plot( X, gradient_collect[w_n]['y'], label=w_n )\n",
    "\n",
    "    plt.xlabel('Data Points (X)')\n",
    "    plt.ylabel(f'Gradient for {vis_name} of {width}-width Net')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for width in nets_by_size:\n",
    "    backward_and_plot_grad(X, nets_by_size[width]['net'], '0.weight', 'Random Init')\n",
    "    backward_and_plot_grad(X, nets_by_size[width]['trained_net'], '0.weight', 'Trained')\n",
    "    backward_and_plot_grad(X, nets_by_size[width]['net'], '0.bias', 'Random Init')\n",
    "    backward_and_plot_grad(X, nets_by_size[width]['trained_net'], '0.bias', 'Trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faJqnWTtHrUd"
   },
   "source": [
    "# (b) SVD for feature matrix\n",
    "\n",
    "During training, we can imagine that we have a generalized\n",
    "linear model with a feature matrix corresponding to the linearized\n",
    "features corresponding to each learnable parameter. We know from\n",
    "our analysis of gradient descent, that the singular values and\n",
    "singular vectors corresponding to this feature matrix are\n",
    "important.\n",
    "\n",
    "Use the SVD of this feature matrix to plot both the singular values and visualize the “principle\n",
    "features” that correspond to the d-dimensional singular vectors multiplied by all the features\n",
    "corresponding to the parameters\n",
    "\n",
    "(HINT: Remember that the feature matrix whose SVD you are\n",
    "taking has $n$ rows where each row corresponds to one training\n",
    "point and $d$ columns where each column corresponds to each of\n",
    "the learnable features. Meanwhile, you are going to be\n",
    "plotting/visualizing the \"principle features\" as functions of\n",
    "$x$ even at places where you don't have training points.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kytqv9qqHrUf"
   },
   "outputs": [],
   "source": [
    "def compute_svd_plot_features(X, y, X_test, y_test, model):\n",
    "    width = model[0].out_features  # the width is the number of hidden units.\n",
    "    gradients = np.zeros((width, X.shape[0]))\n",
    "    num_pts = 0\n",
    "    gradient_collect, vis_collect = { }, { }\n",
    "    for x in X:\n",
    "        y = model(to_torch(x))\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Complete the following part to run backpropagation. (2 lines)\n",
    "        # Hint: The same as part (a)\n",
    "        ########################################################################\n",
    "        pass\n",
    "        ########################################################################\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            for w_idx, w_grad in enumerate( p.grad.view(-1).data ):\n",
    "                if f'{n}.{w_idx}' not in gradient_collect:\n",
    "                    gradient_collect[ f'{n}.{w_idx}' ] = {'x':[], 'y': []}\n",
    "                gradient_collect[ f'{n}.{w_idx}' ]['y'].append( w_grad.item() )\n",
    "                gradient_collect[ f'{n}.{w_idx}' ]['x'].append( x )\n",
    "\n",
    "    feature_matrix = []\n",
    "    for w_n in gradient_collect:\n",
    "        feature_matrix.append( gradient_collect[w_n]['y'] )\n",
    "    feature_matrix = np.array( feature_matrix ).T\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Complete the following part to SVD-decompose the feature matrix.\n",
    "    #       (1 line)\n",
    "    # Hint: the shape of u, s, vh should be [n, d], [d], and [d, d]\n",
    "    #       respectively\n",
    "    ############################################################################\n",
    "    u, s, vh = ?\n",
    "    ############################################################################\n",
    "\n",
    "    plt.scatter(np.arange(s.shape[0]), s, c='darkorange', s=40.0, label='singular values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Construct more training matrix\n",
    "    ############################################################################\n",
    "    # TODO: Complete the following part to compute the pricipal feature\n",
    "    #       (1 line)\n",
    "    ############################################################################\n",
    "    princple_feature = ?\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    for w_idx in range(feature_matrix.shape[1]):\n",
    "        plt.plot( X, princple_feature.T[w_idx] )\n",
    "\n",
    "    plt.xlabel('Data Points (X)')\n",
    "    plt.ylabel(f'Princeple Feature of {width}-width Net')\n",
    "    plt.show()\n",
    "\n",
    "for w in widths:\n",
    "    net = nets_by_size[w]['net']\n",
    "    print(\"Width\", w)\n",
    "    compute_svd_plot_features(X, y, X_test, y_test, net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIIS_WUwHrUg"
   },
   "source": [
    "# (c) Two-layer Network\n",
    "\n",
    "Augment the jupyter notebook to add a second hidden\n",
    "layer of the same size as the first hidden layer, fully connected\n",
    "to the first hidden layer.\n",
    "\n",
    "Allow the visualization of the features corresponding\n",
    "to the parameters in both hidden layers, as well as the\n",
    "\"principle features\" and the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBE5XB24HrUg"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# TODO: Write your code here\n",
    "############################################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
