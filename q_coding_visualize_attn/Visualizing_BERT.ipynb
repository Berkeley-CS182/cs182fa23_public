{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0NwPumBsi03",
    "outputId": "6aa379dc-54e7-48b6-8244-435b04412ae1"
   },
   "outputs": [],
   "source": [
    "!pip install bertviz\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GJOJ7LetFJM"
   },
   "outputs": [],
   "source": [
    "from bertviz.transformers_neuron_view import GPT2Model, GPT2Tokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel, BertTokenizer\n",
    "from bertviz import model_view\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1gan-2rkHwt"
   },
   "source": [
    "## BERT outgrew Sesame Street\n",
    "\n",
    "We have seen how attention works mathematically, but now, let's explore how it works in practice. For this part of the question, we will be visualizing how this simple mechanism allows the powerful large language models of today to function. We will be using an open source visualization tool called [BertViz](https://github.com/jessevig/bertviz).\n",
    "\n",
    "**Recall some of the powerful large language models we have studied so far:**\n",
    "\n",
    "\n",
    "*   The GPT language model is a statistical language model that is autoregressive in nature, and it uses a deep neural network (specifically a transformer decoder) to predict the next word in a sequence. It is trained on a large corpus of text and can be used to generate text that sounds like it was written by a human. (This description was written by GPT-3. How meta!)\n",
    "*   BERT is a transfomer encoder that has been pre-trained with two tasks, which allow it to learn better representations for downstream tasks. First, it learns word-level associations by trying to fill in tokens that have been randomly masked with a 15% probability. Second, it learns sentence-level associations by trying to identify which sentences go first, given a randomly shuffled passage. This base model is then fine-tuned for different tasks, such as question-answering and text-infill. You will be fine-tuning your own BERT model in a later question (`Coding Question: Summarization (Part II)`). This model is bidirectional in nature as it can process text in both directions, from left-to-right and from right-to-left.\n",
    "\n",
    "Fun fact: Back in the 2018, language models had very interesting names based on muppets. Well, there's a ELMo, BERT, and there's an ERNIE (and this name has been claimed twice!).\n",
    "\n",
    "## About BertViz\n",
    "\n",
    "Attention gives us some insight into how these language models form representations about the tokens they interact with, and BertViz is an interactive tool that allows us to visualize attention effectively. We will be using BertViz's `model view` to see how words that are being updated (in the left column of the plots you will generate below) are connected to the words being attended to (in the right column of the plots you will generate below).\n",
    "\n",
    "The lines in the plots represent the attention connections: when the attention score is close to 1, the line color is strong, and when the attention score is close to 0, the line is faint. You can also see the queries, keys, and values that resulted in those attention scores by hovering over the tokens as explained below.\n",
    "\n",
    "Please refer to the BertViz github page linked above if you are interested in learning more!\n",
    "\n",
    "**Note:** Attention visualization only gives us a window into how the model is learning. However, understanding these large language models and the connections they make is actually really nuanced and complex. In fact, it is the subject of ongoing research.\n",
    "\n",
    "BertViz Usage (from their github page):\n",
    "- Hover over any of the tokens on the left side of the visualization to see what tokens are being paid attention to at that moment.\n",
    "- Then, click on the + icon that is revealed when hovering. This will reveal the query vectors, key vectors, and intermediate computations for the attention weights (blue=positive, orange=negative).\n",
    "- Once in the expanded view, hover over any other token on the left to see the associated attention computations.\n",
    "- Click on the Layer or Head drop-downs to change the model layer or head (zero-indexed).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e42WouVMJ6Ar"
   },
   "source": [
    "## a) Attention in GPT-2\n",
    "\n",
    "We will first be using BertViz in order to visualize how attention works within GPT. For the purposes of this homework, we will be loading pre-trained models from Hugging Face as training takes an extremely long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6Op0HM0NlQj",
    "outputId": "fc158ee4-94cc-44da-bc62-4a54fca0a35d"
   },
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "model_version = 'gpt2'\n",
    "model = GPT2Model.from_pretrained(model_version)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxF1-QH8J7Vk"
   },
   "source": [
    "Let's see what the model pays attention to when the sentence structure is simple and basic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "KXEArvnPfTFH",
    "outputId": "1a9e5196-74ec-4690-ca88-9b1c734f60e5"
   },
   "outputs": [],
   "source": [
    "text = \"The dog ran\"\n",
    "show(model, model_type, tokenizer, text, display_mode='dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uCpL-TLJ_W3"
   },
   "source": [
    "Let's try a more complicated sentence structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "HNh67X8mJ_Au",
    "outputId": "a1040f23-f9c3-4a9a-d538-744bc3ae6375"
   },
   "outputs": [],
   "source": [
    "text = \"The dog sitting in the car\"\n",
    "show(model, model_type, tokenizer, text, display_mode='dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYyyqtNNKUWV"
   },
   "source": [
    "What happens when the model is tasked with keeping track of information from the past? Oftentimes, we need to look at the broader context that can span sentences or paragraphs to know the correct tense of a verb to use, which names to fill in where, etc. This is where the transformers excel--at keeping track of history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "PZePFv-9KUAu",
    "outputId": "6b76375d-42bb-46bd-b205-46a6a8085715"
   },
   "outputs": [],
   "source": [
    "text = \"Jaime Salazar is running for election. Despite his party affiliation, Mr.\"\n",
    "show(model, model_type, tokenizer, text, display_mode='dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOMn-STrKSo-"
   },
   "source": [
    "**Answer the following questions in your writeup:**\n",
    "\n",
    "\n",
    "1.   What similarities and differences do you notice in the visualizations between the examples in part (a)? Explore the queries, keys, and values to identify any interesting patterns associated with the attention mechanism.\n",
    "2. How does attention differ between the different layers of the GPT model? Do you notice that the tokens are attending to different tokens as we go through the layers of the network?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtcbtSqEMpNn"
   },
   "source": [
    "## b) BERT pays attention\n",
    "\n",
    "Let's now use BertViz to see how attention works within the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTOYpEzDNm84",
    "outputId": "1892e5e8-acb6-48e8-f07f-c3f7dfa7c211"
   },
   "outputs": [],
   "source": [
    "model_type = 'bert'\n",
    "model_version = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "model = BertModel.from_pretrained(model_version)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqgE7qNW5kzd"
   },
   "source": [
    "First, let's try a simple set of sentences where sentence b follows sentence a sequentially. Notice how we have a pronoun reference to the \"party\" mentioned in sentence a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "VWYbTVowGRzA",
    "outputId": "9b895846-b705-443a-b40f-84e078da1caf"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"I wanted to have a party\"\n",
    "sentence_b = \"I bought a cake for it.\"\n",
    "show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='dark', layer=2, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I6axgPPOaFE"
   },
   "source": [
    "**Answer the following questions in your writeup:**\n",
    "\n",
    "\n",
    "3.   Look at different layers of the BERT model in the visualizations of part (b) and identify different patterns associated with the attention mechanism. Explore the queries, keys, and values to further inform your answer. For instance, do you notice that any particular type of tokens are attended to at a given timestep?\n",
    "4. Do you spot any differences between how attention works in GPT vs. BERT? Think about how the model architectures are different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkbCo6Bt8Sxr"
   },
   "source": [
    "Let's understand what BERT does when it sees two sentences where words are used in multiple different ways. For instance, the word \"play\" has a couple of different meanings. So, what words will the model attend on, and what differences will we notice in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "0-fvdzPh8QbU",
    "outputId": "4b36845a-d08e-4a8e-e2ae-f1e8456312b4"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"T was going to play at the park.\"\n",
    "show(model, model_type, tokenizer, sentence_a, display_mode='dark', layer=2, head=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "00kRGGkW9zNa",
    "outputId": "dc39ec5b-98fa-4005-c89e-61f107d31bc1"
   },
   "outputs": [],
   "source": [
    "sentence_b = \"I was going to a play in the park\"\n",
    "show(model, model_type, tokenizer, sentence_b, display_mode='dark', layer=2, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQTaLtsH97uN"
   },
   "source": [
    "**Answer the following question in your writeup:**\n",
    "\n",
    "5. Look through the different layers of the two BERT networks above associated with sentence a and sentence b, and take a look at the queries, keys, and values associated with the different tokens. Do you notice any differences in the embeddings learned for the two sentences that are essentially identical in structure but different in meaning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxN2oJ-yPNCJ"
   },
   "source": [
    "BERT is able to take care of input given from left-to-right and from right-to-left. Let's see what happens if we pass in a sentence backwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "nv9pY-eXtoOi",
    "outputId": "272007c1-88e8-4b4c-f0df-fee1d616dd26"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"party a have to wanted I\"\n",
    "sentence_b = \"I bought a cake for it\"\n",
    "show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='dark', layer=2, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgDn_hAPP3jv"
   },
   "source": [
    "BERT was also pre-trained to identify which sentences come first sequentially. What happens if we pass in sentences in reverse order sequentially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "dNAAhm-FP3O5",
    "outputId": "c94a906f-89b2-47c0-fd50-46fbfa7ddf2e"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"I bought a cake for it\"\n",
    "sentence_b = \"I went to a party at my neighbor's house\"\n",
    "show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='dark', layer=2, head=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRGUo_sGPqYQ"
   },
   "source": [
    "**Answer the following question in your writeup:**\n",
    "\n",
    "6.   Do you notice BERT's bidirectionality in play?\n",
    "7.   Do you think pre-training the BERT helped it learn better representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUztDGoLQY0X"
   },
   "source": [
    "## c) BERT has multiple heads!\n",
    "\n",
    "Recall that BERT uses multiple attention heads in practice.Let's visualize BERT's multiple attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "c7417afe947e48dd917ce7419687d342",
      "7b74ef3d0f404e7cae9730765f875cf7",
      "0c0b57a8f0204bdd8762c95ba89e5939",
      "e3bccc7f75dd4e6ab6d1f513eabf36fa",
      "e543bf11aae04623b7e417b338a41407"
     ]
    },
    "id": "v7_Z1dhLPZId",
    "outputId": "2b208bff-8826-4b39-e836-2d288959250d"
   },
   "outputs": [],
   "source": [
    "model_version = 'bert-base-uncased'\n",
    "model = AutoModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EufmzhP9RO0X",
    "outputId": "e77098df-1430-48f9-d82e-c63bedae0391"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"I wanted to have a party\"\n",
    "sentence_b = \"I like Thanksgiving dinner\"\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "token_type_ids = inputs['token_type_ids'] # token type id is 0 for Sentence A and 1 for Sentence B\n",
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "sentence_b_start = token_type_ids[0].tolist().index(1) # Sentence B starts at first index of token type id 1\n",
    "token_ids = input_ids[0].tolist() # Batch index 0\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "model_view(attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJXyIoFeRmpT"
   },
   "source": [
    "**Answer the following questions in your writeup:**\n",
    "\n",
    "\n",
    "8.   Do you notice different features being learned throughout the different attention heads of BERT? Why do you think this might be?\n",
    "9.   Can you identify any of the different features that the different attention heads are focusing on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdneQx-hRPs2"
   },
   "source": [
    "These were just some small examples, but we encourage you to play around with this visualization tool and these pre-trained models on your own! There are some other cool models that are accessible through [Hugging Face](https://huggingface.co/models). If you come across anything interesting, please mention it in your writeup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMW9VgvNaj13"
   },
   "source": [
    "## d) Visualizing untrained attention weights\n",
    "\n",
    "So far, we've been looking at the learned attention heads of the BERT model, trained on billions of tokens. Now, let's see how the attention heads behave without their weights.\n",
    "The code block below reinitializes most of the network weights. Re-run some prior cells to observe the difference.\n",
    "\n",
    "**Answer the following questions in your writeup:**\n",
    "\n",
    "10.  What differences do you notice in the attention patterns between the randomly initialized and trained BERT models?\n",
    "11. Run the final cell in the notebook. What are some words or tokens that you would expect strong attention between? What might you guess about the gradients of this attention head for those words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et6MEn2vaj13"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    try:\n",
    "        m.reset_parameters()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "model = BertModel.from_pretrained(model_version)\n",
    "model = model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "oBovhSDYaj14",
    "outputId": "d469258a-496e-40f2-ce21-7eecb7a0f149"
   },
   "outputs": [],
   "source": [
    "sentence_a = \"I am happy\"\n",
    "sentence_b = \"I am not sad\"\n",
    "show(model, model_type, tokenizer, sentence_a, sentence_b, display_mode='dark', layer=2, head=0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a0edca3c5a4484aab9abb582af82b4c4c618fc29416989b754d4fc12a285930"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
