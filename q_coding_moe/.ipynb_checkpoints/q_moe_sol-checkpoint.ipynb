{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7731df1c",
   "metadata": {
    "id": "7731df1c"
   },
   "source": [
    "# Mixture of experts\n",
    "\n",
    "## Before deep learning\n",
    "\n",
    "[Mixture of Experts](https://en.wikipedia.org/wiki/Mixture_of_experts) is an old technique dating back to 1991. Back in the old days before deep learning, it was mainly studied as a generalized version of *mixture models* in statistics. For example, if you have a statistical distribution with two humps, you can model it as the weighted sum of two gaussians. That would be a mixture of gaussians.\n",
    "\n",
    "[![](https://upload.wikimedia.org/wikipedia/commons/7/71/Gaussian-mixture-example.svg)](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model)\n",
    "\n",
    "A mixture of experts is then a simple generalization, and training a mixture of experts, back in the old days, was mostly thought of as statistical inference. The main problem was simply modelling complex data with a larger family of statistical distribution. Their main worry was that the experts would overfit.\n",
    "\n",
    "They had little data (enough to fit onto a floppy disk), and each expert was usually just a gaussian distibution or a logistic classifier (any more complex and they wouldn't know how to calculate the integrals and derivatives). Consequently, what they ended up trying to solve was to fit a few thousand datapoints using tens of very simple experts.\n",
    "\n",
    "It is a general fact of classical machine learning that they were very worried about overfitting, and it is reasonable back then to worry, since they had such small datasets (MNIST was in 1994). This, combined with their inability to hand-design learning algorithms for complex machine learning architectures and the slowness of pure gradient descent, meant that machine learning algorithms back then were simple ones fitted onto small datasets.\n",
    "\n",
    "The overall effect is:\n",
    "* getting training data: expensive (you have to do it yourself)\n",
    "* designing the algorithm: expensive (cheaper if you have graduate students)\n",
    "* training compute: moderate to high (though a few pioneers have bravely pushed to the \"very expensive\" regime, and failed[^norvig2021])\n",
    "* inference compute: very cheap (since that you wouldn't be able to train anything large)\n",
    "\n",
    "[^norvig2021]: [Peter Norvig – Singularity Is in the Eye of the Beholder | gradient-dissent – Weights & Biases](https://wandb.ai/wandb_fc/gradient-dissent/reports/Peter-Norvig-Google-s-Director-of-Research-Singularity-is-in-the-eye-of-the-beholder--Vmlldzo2MTYwNjk?galleryTag=gradient-dissent)\n",
    "\n",
    ">  I certainly remember Geoff Hinton came to Berkeley when I was a grad student in 1981, I think, when he talked about these neural nets. And we fellow grad students thought that was so cool. So we said, \"Let's go back into the lab and implement it.\n",
    ">\n",
    "> And of course, there was absolutely nothing you could download, so we had to build it all from scratch. And we got it to do XOR, and then we got it to do something a little bit more complicated. And it was exciting. And then we gave it the first real problem, and it ran overnight, and it didn't converge, and we let it run one more day, and it still didn't converge. And then we gave up, and we went back to our sort of knowledge-based systems approach. But if we had the computing power of today, it probably would have converged after five seconds.\n",
    "\n",
    "This should be compared to the very different situation with deep learning:\n",
    "\n",
    "* getting training data: cheap (just download it online)\n",
    "* designing the algorithm: cheap (make a standard network, add a few decorations, then use backprop with Adam optimizer)\n",
    "* training compute: as expensive as you want\n",
    "* inference compute: as expensive as you want\n",
    "\n",
    "## After deep learning\n",
    "\n",
    "While classical statistics and machine learning was mainly constrained by how many partial derivatives and integrals the statistician can calculate confidently on paper[^1], deep learning is mainly constrained by memory and compute budget.\n",
    "\n",
    "[^1]: If you want a taste of the old days, look at the formulas inside \"Hierarchical mixtures of experts and the EM algorithm\" (Jordan, Michael I., and Robert A. Jacobs, 1994)\n",
    "\n",
    "So when the deep learning era came circa 2012, people immediately started  looking into how to perform **conditional computing**: save computing cost by only calling a small portion of the model. The idea is that you would have different portions of the model be specialized for different forms of input, and for each input, the model would first cheaply find out which expert should handle it, then call upon only the few specialized experts to handle this particular input.\n",
    "\n",
    "Deep learning came with AlexNet (2012), and the first paper on applying MoE to deep learning was \"Learning Factored Representations in a Deep Mixture of Experts\" (2013). Things really started heating up though with sparsely-gated MoE (2017).\n",
    "\n",
    "## Why MoE for deep learning?\n",
    "\n",
    "Generally, one uses a MoE on the frontier, because:\n",
    "\n",
    "* You really need to push the metric up by a few points.\n",
    "* You can't train a dense model larger than the frontier model, because it simply fails to converge, or the hyperparameter settings for the small models don't work for the larger one (and you can't just run a grid search to find it because it costs a million dollars to do a single run).\n",
    "* You can train around 10 copies of the frontier model, because while you don't have the money to do grid search beyond the current frontier, you have the money to train 10 at the frontier.\n",
    "* You can't infer a dense model larger than the frontier one, because one dense model $N$ times as wide would cost you $N^2$ amount of storage *and* compute, while if you just train $N$ experts, each with roughly the same architecture as the dense model, it would cost you about $N$ amount of storage *and* about $2$ amount of compute (if only 2 experts are called per query).\n",
    "* Indeed, if there are too many parameters, then it can't even be fit onto a good GPU and must be split across GPUs, and then the GPU-GPU communication becomes a serious problem (the \"von Neumann bottleneck\").\n",
    "\n",
    "[![](https://cs61.seas.harvard.edu/site/img/storage-hierarchy.png)](https://cs61.seas.harvard.edu/site/2018/Storage2/)\n",
    "\n",
    "All of which are satified by Microsoft, Google, etc. This explains the \"rumored\" (all but certain at this point) that GPT-4 is a MoE made by multiple GPT-3-like models.\n",
    "\n",
    "A quick scan of the recent literature shows this:\n",
    "\n",
    "* Outrageously Large Neural Networks (Google, 2017): *We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.*\n",
    "* Switch transformers (Google, 2020): *Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters, respectively.*\n",
    "* Scaling Vision with Sparse Mixture of Experts (Google again, 2021): *we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.*\n",
    "\n",
    "The \"Outrageously Large Neural Networks\" paper is not the first paper on MoE in the deep learning era, but it is the most important one. Also notice that it was applied to between \"stacked LSTM layers\", because it was published before Transformers, back when neural language models meant stacks of LSTM. Nowadays, of course, MoE usually means MoE layers within Transformers, because only with Transformers do people regularly train models beyond 10 billion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af47989",
   "metadata": {
    "id": "4af47989"
   },
   "source": [
    "## Classical MoE\n",
    "\n",
    "The classical version of MoE is fairly simple: You start with a few machine learning models and then you ensemble them together.\n",
    "\n",
    "In this example, we train a binary classifier for points in $\\mathbb R^2$. The distribution is constructed so that points are more likely to be class 1 in the upper-right quadrant, and more likely to be class 0 in the other 3 quadrants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b63a20",
   "metadata": {
    "id": "01b63a20"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the dataset\n",
    "def generate_dataset(num_samples, sharpness=5):\n",
    "    X = np.random.randn(num_samples, 2)\n",
    "    p = np.minimum(np.exp(sharpness * np.minimum(X[:, 0], X[:, 1])), 1)\n",
    "    y = np.random.binomial(1, p, size=num_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4572ee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "f4572ee3",
    "outputId": "b999812d-584d-4b47-a153-3b3c15065e28"
   },
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "num_samples = 1000\n",
    "X, y = generate_dataset(num_samples)\n",
    "\n",
    "# Split the dataset manually\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * num_samples)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Plot the dataset\n",
    "def plot_dataset(X, y, ax):\n",
    "    ax.scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], label='Class 0', marker='o', c='blue')\n",
    "    ax.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], label='Class 1', marker='x', c='red')\n",
    "    ax.set_title('Dataset Scatter Plot')\n",
    "    return ax\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_dataset(X_test, y_test, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EgO5UAuFpz1w",
   "metadata": {
    "id": "EgO5UAuFpz1w"
   },
   "source": [
    "In our MoE model, each expert is just a linear logistic classifier:\n",
    "\n",
    "$$Pr_\\theta(y = 0 | x) = \\frac{1}{1 + e^{\\theta_0^T x + \\theta_1}}$$\n",
    "\n",
    "The weights are found by linear-softmax:\n",
    "\n",
    "$$Pr(y=0 | x ) = \\sum_i w(x)_i Pr_{\\theta^{(i)}}(y = 0 | x)$$\n",
    "\n",
    "with\n",
    "\n",
    "$$w(x) = \\mathrm{softmax}(Ax)$$\n",
    "\n",
    "for a learned matrix $A$.\n",
    "\n",
    "**What should the MoE learn? How many experts should it use?**\n",
    "\n",
    "########################################################################\n",
    "\n",
    "TODO: your answer here\n",
    "\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SiyB-keTdqhi",
   "metadata": {
    "id": "SiyB-keTdqhi"
   },
   "source": [
    "Suppose now we have a different binary dataset, with $y = 0$ inside the square $[-1, +1]^2$, and $y=1$ outside the square.\n",
    "\n",
    "**Can we still use a MoE with linear logistic experts? If so, what is the weight matrix like, and how many experts do we need?**\n",
    "\n",
    "########################################################################\n",
    "\n",
    "TODO: your answer here\n",
    "\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEzRGFBYsjaZ",
   "metadata": {
    "id": "lEzRGFBYsjaZ"
   },
   "source": [
    "### Training a tiny MoE\n",
    "\n",
    "Let's train a small MoE with just 6 experts. \n",
    "\n",
    "In fact, we can compare the model's performance with the Bayes-optimal predictor. We know the Bayes-optimal predictor, since it is just the dataset distribution, which we designed ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b20ae2",
   "metadata": {
    "id": "e2b20ae2"
   },
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, output_dim):\n",
    "        super(MoE, self).__init__()\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_experts)])\n",
    "        self.weighting = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            weights = torch.softmax(self.weighting(x) + mask, dim=1)\n",
    "        else:\n",
    "            weights = torch.softmax(self.weighting(x), dim=1)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        output = torch.stack(expert_outputs, dim=2)  # Stack expert outputs along dimension 2\n",
    "        weighted_output = torch.sum(output * weights.unsqueeze(1), dim=2)\n",
    "        return weighted_output\n",
    "\n",
    "    def sparse_forward(self, x, sparsity):\n",
    "        norms = self.weighting.weight.norm(dim=1)\n",
    "        topk_indices = torch.topk(norms, sparsity).indices\n",
    "        mask = torch.full_like(norms, -1000.0)\n",
    "        mask[topk_indices] = 0\n",
    "        return self.forward(x, mask=mask)\n",
    "\n",
    "    def sparsely_gated_forward(self, x, sparsity):\n",
    "        weighted_output = None\n",
    "        ########################################################################\n",
    "        # TODO: implement sparsely-gated inference\n",
    "        # Hint: look at how `sparse_forward` is implemented.\n",
    "        ########################################################################\n",
    "        ########################################################################\n",
    "        return weighted_output\n",
    "\n",
    "class BayesOptimal(nn.Module):\n",
    "    def __init__(self, sharpness=5):\n",
    "        super(BayesOptimal, self).__init__()\n",
    "        self.sharpness = sharpness\n",
    "\n",
    "    def forward(self, X):\n",
    "        exp_term = torch.exp(self.sharpness * torch.minimum(X[:, 0], X[:, 1]))\n",
    "        ps = torch.minimum(exp_term, torch.ones_like(exp_term))\n",
    "        y = torch.ones_like(X)\n",
    "        y[:,0] -= ps\n",
    "        y[:,1] = ps\n",
    "        y += 1e-6\n",
    "        return y.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bb270",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e41bb270",
    "outputId": "1e386575-8ef6-46ff-eedd-817f5e628c53"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_samples = 10000\n",
    "input_dim = 2\n",
    "num_experts = 6\n",
    "output_dim = 2\n",
    "learning_rate = 0.05\n",
    "num_epochs = 2000\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = generate_dataset(num_samples)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(split_ratio * num_samples)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Create and initialize the MoE model\n",
    "model = MoE(input_dim, num_experts, output_dim)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "ref_model = BayesOptimal()\n",
    "ref_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = ref_model.forward(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    print(f'Bayes optimal loss: {loss.item():.4f}')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model.forward(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    ref_outputs = ref_model.forward(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    _, ref_predicted = torch.max(ref_outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    ref_accuracy = (ref_predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Bayes optimal Accuracy: {ref_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEA53LQdbPnz",
   "metadata": {
    "id": "lEA53LQdbPnz"
   },
   "source": [
    "## Sparsifying MoE\n",
    "\n",
    "Given a MoE, there are two ways to use it. One can use it as-is, but then every expert must be consulted on every query, defeating the main purpose of MoE in the age of large models: conditional computing.\n",
    "\n",
    "We will try out two ways to sparsify a trained MoE. In the first MoE paper (1991), they inspected the weights (the matrix $A$ in our notation), and found that some experts would never be called on any input. Then they just removed those experts. This can be understood as sparsification \"at compile time\".\n",
    "\n",
    "Here, we implement sparsification \"at compile time\" by ranking the rows of $A$ according to their L2-norm, find the top-k rows of them, then mask out all the other experts.\n",
    "\n",
    "This implementation is given to you as the method `sparse_forward` in `class MoE`.\n",
    "\n",
    "In the **sparsely-gated MoE**, the sparsification is done \"at runtime\". That is:\n",
    "\n",
    "$$w(x) = \\mathrm{softmax}(\\mathrm{top}_k(Ax))$$\n",
    "\n",
    "where $\\mathrm{top}_k(v)$ preserves the top-k entries of $v$, but set all other entries to $-\\infty$.\n",
    "\n",
    "The implementation is not given to you. You should **go back and implement** the method `sparsely_gated_forward` in `class MoE`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e0b67",
   "metadata": {},
   "source": [
    "After you have implemented `sparsely_gated_forward`, answer this question:\n",
    "\n",
    "**Assume our MoE is well-trained. What sparsity should it use for \"at compile time\"? What sparsity should it use for \"at run time\"?**\n",
    "\n",
    "########################################################################\n",
    "\n",
    "TODO: your answer here\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "Let's see how well your prediction holds. Run the next two cell blocks and look at the resulting images. If they don't look like what you expected, then double check your implementation of `sparsely_gated_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e123655",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "6e123655",
    "outputId": "fdbe9256-2165-4ad8-afe2-108a7244fbf4"
   },
   "outputs": [],
   "source": [
    "sparsity_values = range(1, 7)\n",
    "\n",
    "# Create a 2x3 grid of subfigures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Loop through the sparsity values and create individual heatmaps in each subfigure\n",
    "for i, sparsity in enumerate(sparsity_values):\n",
    "    # Define a mesh grid for visualization\n",
    "    x_min, x_max = -4, 4\n",
    "    y_min, y_max = -4, 4\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Flatten the mesh grid and make predictions using the MoE model\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_points = torch.FloatTensor(grid_points)\n",
    "    with torch.no_grad():\n",
    "        Z = model.sparse_forward(grid_points, sparsity).numpy()\n",
    "    Z = Z[:, 1].reshape(xx.shape)  # We are interested in the probability of class 1\n",
    "\n",
    "    # Plot the heatmap in the current subfigure\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axs[row, col]\n",
    "\n",
    "    # Plot the dataset points (assuming you have a function plot_dataset for this)\n",
    "    plot_dataset(X_test, y_test, ax)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    heatmap = ax.contourf(xx, yy, Z, levels=20, cmap=plt.cm.RdBu_r, alpha=0.8)\n",
    "\n",
    "    # Set the title and axis limits for the current subfigure\n",
    "    ax.set_title(f'Sparsity {sparsity}')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "fig.suptitle(\"Prediction of sparsified MoE, at various levels of sparsity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef6320",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "id": "b7ef6320",
    "outputId": "66bd03c5-0f4e-45b1-8654-e1526c15b503"
   },
   "outputs": [],
   "source": [
    "sparsity_values = range(1, 7)\n",
    "\n",
    "# Create a 2x3 grid of subfigures\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Loop through the sparsity values and create individual heatmaps in each subfigure\n",
    "for i, sparsity in enumerate(sparsity_values):\n",
    "    # Define a mesh grid for visualization\n",
    "    x_min, x_max = -4, 4\n",
    "    y_min, y_max = -4, 4\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Flatten the mesh grid and make predictions using the MoE model\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_points = torch.FloatTensor(grid_points)\n",
    "    with torch.no_grad():\n",
    "        Z = model.sparsely_gated_forward(grid_points, sparsity).numpy()\n",
    "    Z = Z[:, 1].reshape(xx.shape)  # We are interested in the probability of class 1\n",
    "\n",
    "    # Plot the heatmap in the current subfigure\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axs[row, col]\n",
    "\n",
    "    # Plot the dataset points (assuming you have a function plot_dataset for this)\n",
    "    plot_dataset(X_test, y_test, ax)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    heatmap = ax.contourf(xx, yy, Z, levels=20, cmap=plt.cm.RdBu_r, alpha=0.8)\n",
    "\n",
    "    # Set the title and axis limits for the current subfigure\n",
    "    ax.set_title(f'Sparsity {sparsity}')\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "\n",
    "fig.suptitle(\"Prediction of sparsely-gated MoE, at various levels of sparsity\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9893YbppvLgm",
   "metadata": {
    "id": "9893YbppvLgm"
   },
   "source": [
    "## Training a sparsely-gated MoE directly\n",
    "\n",
    "I also tried training a sparsely-gated MoE directly using backprop, but the result is much less stable and achieved significantly lower accuracy. My guess is that the sparsely-gated MoE has \"sharp edges\" in loss landscape.\n",
    "\n",
    "Indeed, the review paper [ST-MoE: Designing stable and transferable sparse expert models (2022)\n",
    "](https://arxiv.org/pdf/2202.08906.pdf) says\n",
    "\n",
    "> Sparse models often suffer from training instabilities worse than those observed in standard densely-activated Transformers.\n",
    "\n",
    "This is unsettled, a current field of ML engineering research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb46a3f",
   "metadata": {
    "id": "4bb46a3f"
   },
   "source": [
    "## Load balancing\n",
    "\n",
    "Vanilla MoE tend to have issues of load balancing: some experts are consulted often, while other experts rarely or not at all.\n",
    "\n",
    "For example, in the very first paper on MoE \\footnote{Jacobs, Robert A., et al. \"Adaptive mixtures of local experts.\" Neural computation 3.1 (1991): 79-87.}, they trained 6 experts to recognize phonemes from 6 Japanese speakers. They found that the trained MoE would give close to zero weighting on all but 3 experts:\n",
    "\n",
    "> Only experts 4, 5, and 6 are active in the final mixture. This solution is typical - in all simulations with mixtures of 4 or 8 experts all but 2 or 3 experts had mixing proportions that were effectively 0 for all cases.\n",
    "\n",
    "This might not have been a serious problem in the past, when neural networks were seen as merely a form of high-dimensional statistical model learnable by any one of the typical statistical algorithms (maximal likelihood, Bayesian inference, expectation maximization...). Nowadays, MoE are used because you need to throw more compute at the problem, but cannot train a larger dense model. In this case, it would defeat the purpose of MoE if a substantial portion of experts end up neglected.\n",
    "\n",
    "It is no coincidence, then, that the sparsely-gated MoE layer (2017) paper specifically used two auxiliary loss functions to encourage the experts to have equal \"weight\" over time. This is simplified to just one auxiliary loss function in the Switch Transformers paper (2020).\n",
    "\n",
    "Specifically, consider the sparsely-gated MoE with $k=1$ -- where just the top-ranked expert is consulted every time. Let $n$ be the number of experts, and consider a batch of queries $\\{x_1, x_2, ..., x_T\\}$, then the auxiliary loss of the batch is\n",
    "\n",
    "$$\n",
    "L := n \\sum_{i=1}^n f_i P_i\n",
    "$$\n",
    "where $f_i=\\frac{1}{T} \\#(\\text{queries sent to expert $i$})$ is the fraction of time where expert $i$ is ranked highest, and $P_i=\\frac{1}{T} \\sum_{j=1}^T w_i\\left(x_j\\right)$ is the fraction of weight on expert $i$.\n",
    "\n",
    "In the original paper, they claimed that we can obtain the minimal auxiliary loss $L$ at the limit where every expert has equal weight $1 / n$ on all samples, and every expert is ranked the highest equally often.\n",
    "\n",
    "**Assuming this is so, what is the minimal loss?**\n",
    "\n",
    "########################################################################\n",
    "\n",
    "TODO: your answer here\n",
    "\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58a9b5",
   "metadata": {},
   "source": [
    "Unfortunately, the original paper is wrong!\n",
    "\n",
    "**When there are many experts, and large batch, construct a way to let $L$ approach $1/2$.** It is not difficult to show that $1/2$ is in fact the lower bound, but you are not required to show this.\n",
    "\n",
    "Hint: Run the next code blocks, with various settings of $T, n$ and look at its outputs. Can you find a pattern? Notice that you should not use $T, n$ larger than $10$ as it would take too much time to run.\n",
    "\n",
    "########################################################################\n",
    "\n",
    "TODO: your answer here\n",
    "\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f805bfbb",
   "metadata": {},
   "source": [
    "I tried to show that this thing actually converges to the global minimum by direct minimization, but strangely, direct optimization with SciPy optimize always fails to converge to ~1/2. It even fails to converge to ~1. Indeed, often it just moves around the initial point a bit then immediately gives up and stops. My suspicion is that the loss landscape is too jagged.\n",
    "\n",
    "If you however use a global optimization method like `dual_annealing`, then the minimimum can be found. I am unsure of the implications for using the auxiliary loss in a real MoE system, but seeing that Google has been training those huge models since 2017, I would guess that it really does work as a load balancing loss.\n",
    "\n",
    "Too bad that I don't have a thousand TPU-days to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870f95d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "870f95d3",
    "outputId": "79211c16-135b-48cd-b552-e7a6d49b8469"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "import scipy\n",
    "\n",
    "# Define the dimensions of the matrix\n",
    "T = 10\n",
    "n = 10\n",
    "\n",
    "\n",
    "def helper_function(x):\n",
    "    x_matrix = x.reshape((T, n))\n",
    "\n",
    "    w = softmax(x_matrix, axis=1)\n",
    "    max_indices = np.argmax(w, axis=1)\n",
    "    f = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        f[i] = np.mean(max_indices == i)\n",
    "    P = np.mean(w, axis=0)\n",
    "    return f, P\n",
    "\n",
    "def objective_function(x):\n",
    "    f, P = helper_function(x)\n",
    "    L = n * np.sum(f * P)\n",
    "    return L\n",
    "\n",
    "# Initialize x with random values\n",
    "x_initial = np.array([[10, -1],\n",
    "                      [0, 1],\n",
    "                      [0, 1],\n",
    "                      [0, 1]])\n",
    "x_initial = np.random.randint(0, 10, size=(T, n))\n",
    "\n",
    "print(-objective_function(x_initial))\n",
    "# Minimize the objective function L\n",
    "result = scipy.optimize.dual_annealing(objective_function,\n",
    "                                      bounds=[(-10, 10)]*(n*T))\n",
    "\n",
    "# Extract the optimized x matrix\n",
    "x_optimized = result.x.reshape((T, n))\n",
    "\n",
    "# Print the optimized x matrix and the minimized L value\n",
    "print(\"Optimized x:\")\n",
    "print(x_optimized)\n",
    "print(\"Minimized L value:\", -result.fun)\n",
    "print(softmax(x_optimized, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b4265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "3b6b4265",
    "outputId": "95f90795-50e5-4cce-f8d5-6e63fe36f2e6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(softmax(x_optimized, axis=1) * T, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "f, P = helper_function(x_optimized)\n",
    "for i in range(len(f)):\n",
    "    print(f\"expert {i} is consulted {T * f[i]:.0f} times, with weight {T * P[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
