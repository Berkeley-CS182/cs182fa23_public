{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAWsAt6NpWvX"
   },
   "source": [
    "# Meta-Learning on 1-D Functions (Coding Part)\n",
    "\n",
    "In this coding assignment, you will implement MAML for both regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgBbElVfpWva"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZIab9adpWvb",
    "outputId": "b33cc39d-bd4f-49ea-9e55-288bb3c794f9"
   },
   "outputs": [],
   "source": [
    "#@title Install Packages\n",
    "\n",
    "!pip install higher\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1gY95bhpWvc"
   },
   "outputs": [],
   "source": [
    "#@title Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import higher\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZHBr8wQpWvc"
   },
   "outputs": [],
   "source": [
    "#@title Utility Functions\n",
    "\n",
    "###############################################\n",
    "#### ndarray/Tensor manipulation functions ####\n",
    "def to_numpy(x):\n",
    "    return x.detach().numpy()\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    return torch.tensor(x)\n",
    "\n",
    "\n",
    "def print_dict_initialization(params_dict, dict_name):\n",
    "    for key in params_dict:\n",
    "        print(str(key) + '=' + str(dict_name) + \"['\" + str(key) + '\"]')\n",
    "\n",
    "\n",
    "def my_sign_tensor(x):\n",
    "    y = torch.sign(x)\n",
    "    y[y == 0] = 1\n",
    "    return y.int()\n",
    "\n",
    "\n",
    "def my_sign_numpy(x):\n",
    "    y = np.sign(x)\n",
    "    y[y == 0] = 1\n",
    "    return y.astype('int')\n",
    "\n",
    "\n",
    "# Use this version of my_sign\n",
    "def my_sign(x):\n",
    "    if torch.is_tensor(x):\n",
    "        return my_sign_tensor(x)\n",
    "    return my_sign_numpy(x)\n",
    "\n",
    "\n",
    "###################################\n",
    "#### Data generation functions ####\n",
    "def add_label_noise(y, noise_prob):\n",
    "    y = y * np.random.choice([-1, 1], p=[noise_prob, 1 - noise_prob], size=y.shape)\n",
    "    return y\n",
    "\n",
    "\n",
    "def featurize_fourier(x, d, normalize=False):\n",
    "    assert (d - 1) % 2 == 0, \"d must be odd\"\n",
    "    max_r = int((d - 1) / 2)\n",
    "    n = len(x)\n",
    "    A = np.zeros((n, d))\n",
    "    A[:, 0] = 1\n",
    "    for d_ in range(1, max_r + 1):\n",
    "        A[:, 2 * (d_ - 1) + 1] = np.sin(d_ * x * np.pi)\n",
    "        A[:, 2 * (d_ - 1) + 2] = np.cos(d_ * x * np.pi)\n",
    "    if normalize:\n",
    "        A[:, 0] *= (1 / np.sqrt(2))\n",
    "        A *= np.sqrt(2)\n",
    "    return A\n",
    "\n",
    "\n",
    "def featurize(x, d, phi_type, normalize=True):\n",
    "    function_map = {\n",
    "        # 'polynomial':featurize_vandermonde,\n",
    "        'fourier': featurize_fourier}\n",
    "    return function_map[phi_type](x, d, normalize)\n",
    "\n",
    "\n",
    "def generate_x(n, x_type, x_low=-1, x_high=1):\n",
    "    if x_type == 'grid':\n",
    "        x = np.linspace(x_low, x_high, n, endpoint=False).astype(np.float64)\n",
    "    elif x_type == 'uniform_random':\n",
    "        x = np.sort(np.random.uniform(x_low, x_high, n).astype(np.float64))\n",
    "        # Note that for making it easy for plotting we sort the randomly sampled x in ascending order\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return x\n",
    "\n",
    "\n",
    "def generate_y(features, k_idx, k_val):\n",
    "    # y as linear combination of features\n",
    "    return np.sum(features[:, k_idx] * k_val, 1)\n",
    "\n",
    "\n",
    "##########################################\n",
    "#### Closed-form regression functions ####\n",
    "def solve_ls(phi, y, weights=None):\n",
    "    d = phi.shape[1]\n",
    "    if weights is None:\n",
    "        weights = np.ones(d)\n",
    "    phi_weighted = weights * phi\n",
    "    LR = LinearRegression(fit_intercept=False)\n",
    "    LR.fit(phi_weighted, y)\n",
    "    coeffs_weighted = LR.coef_\n",
    "    alpha = coeffs_weighted * weights\n",
    "    loss = np.mean((y - phi @ alpha.T)**2)\n",
    "\n",
    "    return alpha.T, loss\n",
    "\n",
    "\n",
    "def solve_ridge(phi, y, lambda_ridge, weights=None):\n",
    "    d = phi.shape[1]\n",
    "    if weights is None:\n",
    "        weights = np.ones(d)\n",
    "    phi_weighted = weights * phi\n",
    "\n",
    "    Rdg = Ridge(fit_intercept=False, alpha=lambda_ridge)\n",
    "    Rdg.fit(phi_weighted, y)\n",
    "    coeffs_weighted = Rdg.coef_\n",
    "    alpha = coeffs_weighted * weights\n",
    "    loss = np.mean((y - phi @ alpha.T)**2) + lambda_ridge * np.sum((coeffs_weighted)**2)\n",
    "    return alpha, loss\n",
    "\n",
    "\n",
    "############################################\n",
    "#### sklearn logistic regression solver ####\n",
    "def solve_logistic(phi, z, weights=None):\n",
    "    # print(z)\n",
    "    # raise ValueError\n",
    "    d = phi.shape[1]\n",
    "    if weights is None:\n",
    "        weights = np.ones(d)\n",
    "    phi_weighted = weights * phi\n",
    "    clf = LogisticRegression(tol=1e-4, verbose=False, solver='lbfgs', random_state=0, fit_intercept=False, C=1e6).fit(phi_weighted, z)\n",
    "\n",
    "    coeffs_weighted = clf.coef_\n",
    "    alpha = coeffs_weighted * weights\n",
    "\n",
    "    z_pred = my_sign(phi @ alpha.T)[:, 0]\n",
    "    loss = np.mean(z != z_pred)\n",
    "    # print(loss)\n",
    "    return alpha.T, loss\n",
    "\n",
    "\n",
    "#######################################\n",
    "#### Model class for use with MAML ####\n",
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.feature_weights = torch.nn.Parameter(torch.ones(d).double())\n",
    "        self.coeffs = torch.nn.Parameter(torch.zeros(d).double())\n",
    "\n",
    "    def forward(self, F):\n",
    "        return (F * self.feature_weights) @ self.coeffs\n",
    "\n",
    "\n",
    "##############################\n",
    "#### ipywidget generators ####\n",
    "def generate_int_widget(desc, min_, val, max_, step=1):\n",
    "    return widgets.IntSlider(\n",
    "        value=val,\n",
    "        min=min_,\n",
    "        max=max_,\n",
    "        step=step,\n",
    "        description=desc,\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d')\n",
    "\n",
    "\n",
    "def generate_float_widget(desc, min_, val, max_, step):\n",
    "    return widgets.FloatSlider(\n",
    "        value=val,\n",
    "        min=min_,\n",
    "        max=max_,\n",
    "        step=step,\n",
    "        description=desc,\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='.1f',\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_floatlog_widget(desc, min_, step, val, max_, base=10):\n",
    "    return widgets.FloatLogSlider(\n",
    "        value=val,\n",
    "        base=base,\n",
    "        min=min_,  # max exponent of base\n",
    "        max=max_,  # min exponent of base\n",
    "        step=step,  # exponent step\n",
    "        description=desc\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h62MNftwpWve"
   },
   "outputs": [],
   "source": [
    "#@title Visualization Utilities\n",
    "\n",
    "def visualize_prediction_classification(data_dict, feature_weights, n_train_post):\n",
    "    train_data_dict = data_dict['train'][n_train_post]\n",
    "    test_data_dict  = data_dict['test']\n",
    "\n",
    "\n",
    "    x_train_post = train_data_dict['x']\n",
    "    features_post = train_data_dict['features']\n",
    "    y_post = train_data_dict['y'][0]\n",
    "\n",
    "    z_post = my_sign(y_post)\n",
    "    w_post, loss = solve_logistic(features_post, z_post, feature_weights)\n",
    "\n",
    "    w_post /= np.linalg.norm(w_post)\n",
    "    y_post_pred = features_post@w_post\n",
    "    z_post_pred = my_sign(y_post_pred)\n",
    "\n",
    "    x_test_post = test_data_dict['x']\n",
    "    features_test_post = test_data_dict['features']\n",
    "    y_test_post = test_data_dict['y'][0]\n",
    "    y_test_post_pred = features_test_post@w_post\n",
    "\n",
    "\n",
    "    #print(y_test_post_pred.shape, y_test_post.shape)\n",
    "    #print(np.mean((np.squeeze(my_sign(y_test_post))!= np.squeeze(my_sign(y_test_post_pred)))))\n",
    "    # print(((my_sign(y_test_post)!= my_sign(y_test_post_pred)).astype('float')).shape)\n",
    "    # print(((my_sign(y_test_post)!= my_sign(y_test_post_pred)).astype('float')))\n",
    "    # plt.plot(my_sign(y_test_post) - my_sign(y_test_post_pred))\n",
    "    # plt.show()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = [12,5])\n",
    "    ax1.scatter(x_train_post, z_post, marker = 'o', s = 100, color = 'red', alpha = 0.5, label = 'Training points true')\n",
    "    ax1.scatter(x_train_post, z_post_pred, marker='x', s = 100, color = 'green', alpha = 0.5, label = 'Training points predictions')\n",
    "    ax1.plot(x_test_post, y_test_post_pred, '-', color = 'blue', alpha = 0.4,  label = 'Predicted function')\n",
    "\n",
    "    z_test_post_pred = my_sign(y_test_post_pred)\n",
    "    ax1.scatter(x_test_post, z_test_post_pred, s=10, color='green', label = 'Predicted Sign Labels', alpha = 0.8)\n",
    "    z_test_post = my_sign(y_test_post)\n",
    "    ax1.scatter(x_test_post, z_test_post, s = 10, color='orange', label = 'True Sign Labels', alpha = 0.8)\n",
    "\n",
    "    ax1.plot(x_test_post, y_test_post, '-', color = 'brown', label = 'True function', alpha = 0.4)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_title('n_train_post =' +str(n_train_post))\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(np.abs(feature_weights), 'o-')\n",
    "    ax2.set_title('Feature weights')\n",
    "    ax2.set_xlabel('Feature #')\n",
    "    ax2.set_ylabel('abs(feature_weight)')\n",
    "#     ax2.set_yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def  visualize_test_loss_reg(iteration, n_train_inner, n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss,\n",
    "        init_n_train_post_range=None,init_avg_test_loss=None, init_top_10_loss=None,init_bot_10_loss=None,\\\n",
    "            oracle_n_train_post_range=None,oracle_avg_test_loss=None, oracle_top_10_loss=None, oracle_bot_10_loss=None, zero_avg_loss = None, zero_top_10_loss = None, zero_bot_10_loss = None,  wrong_n_train_post_range=None, wrong_avg_test_loss=None, wrong_top_10_loss=None, wrong_bot_10_loss=None, noise_std = None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = [12,5])\n",
    "\n",
    "\n",
    "    ax1.fill_between(n_train_post_range, bot_10_loss, top_10_loss, alpha = 0.5)\n",
    "\n",
    "    if iteration is not None:\n",
    "        ax1.plot(n_train_post_range, avg_test_loss, 'o-', label = 'Iteration: ' + str(iteration))\n",
    "    else:\n",
    "        ax1.plot(n_train_post_range, avg_test_loss, 'o-', label = 'Meta-learned')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel('Test mse')\n",
    "    ax1.set_xlabel('n_train_post')\n",
    "    ax1.set_title('Test mse vs n_train_post')\n",
    "\n",
    "    if init_avg_test_loss is not None:\n",
    "        ax1.fill_between(init_n_train_post_range, init_bot_10_loss, init_top_10_loss, alpha = 0.5, color = 'orange')\n",
    "        ax1.plot(init_n_train_post_range, init_avg_test_loss, 'o-', c = 'orange', label = 'Init')\n",
    "\n",
    "\n",
    "    if oracle_avg_test_loss is not None:\n",
    "        ax1.fill_between(oracle_n_train_post_range,oracle_bot_10_loss, oracle_top_10_loss, alpha = 0.5, color = 'green')\n",
    "        ax1.plot(oracle_n_train_post_range, oracle_avg_test_loss, 'o-', c = 'green', label = 'Oracle')\n",
    "    if wrong_avg_test_loss is not None:\n",
    "        ax1.fill_between(wrong_n_train_post_range,wrong_bot_10_loss, wrong_top_10_loss, alpha = 0.5, color = 'red')\n",
    "        ax1.plot(wrong_n_train_post_range, wrong_avg_test_loss, 'o-', c = 'red', label = 'Wrong weights')\n",
    "\n",
    "    if zero_avg_loss is not None:\n",
    "        ax1.fill_between(n_train_post_range,zero_bot_10_loss*np.ones(len(n_train_post_range)), zero_top_10_loss*np.ones(len(n_train_post_range)), alpha = 0.5, color = 'yellow')\n",
    "\n",
    "        ax1.plot(n_train_post_range, np.ones(len(n_train_post_range))*zero_avg_loss, '-', c = 'yellow', label = 'Zero')\n",
    "\n",
    "\n",
    "\n",
    "    if noise_std is not None:\n",
    "        ax1.plot(n_train_post_range, np.ones_like(n_train_post_range)*(noise_std**2), '--', c = 'black', label = 'Noise variance')\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    idx = np.where(n_train_post_range <= 4*n_train_inner)[0]\n",
    "    cn_train_post_range = n_train_post_range[idx]\n",
    "    cavg_test_loss = avg_test_loss[idx]\n",
    "    ctop_10_loss = top_10_loss[idx]\n",
    "    cbot_10_loss = bot_10_loss[idx]\n",
    "\n",
    "\n",
    "    ax2.fill_between(cn_train_post_range, cbot_10_loss, ctop_10_loss, alpha = 0.5)\n",
    "    if iteration is not None:\n",
    "        ax2.plot(cn_train_post_range, cavg_test_loss, 'o-', label = 'Iteration: ' + str(iteration))\n",
    "    else:\n",
    "        ax2.plot(cn_train_post_range, cavg_test_loss, 'o-', label = 'Meta-learned')\n",
    "\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Test mse')\n",
    "    ax2.set_xlabel('n_train_post')\n",
    "    ax2.set_title('Test mse vs n_train_post (zoomed)')\n",
    "\n",
    "    if init_avg_test_loss is not None:\n",
    "        idx = np.where(init_n_train_post_range <= 4*n_train_inner)[0]\n",
    "        cinit_n_train_post_range = init_n_train_post_range[idx]\n",
    "        cinit_avg_test_loss = init_avg_test_loss[idx]\n",
    "        cinit_top_10_loss = init_top_10_loss[idx]\n",
    "        cinit_bot_10_loss = init_bot_10_loss[idx]\n",
    "\n",
    "        ax2.fill_between(cinit_n_train_post_range, cinit_bot_10_loss, cinit_top_10_loss, alpha = 0.5, color = 'orange')\n",
    "        ax2.plot(cinit_n_train_post_range, cinit_avg_test_loss, 'o-', c = 'orange', label = 'Init')\n",
    "\n",
    "    if oracle_avg_test_loss is not None:\n",
    "        idx = np.where(oracle_n_train_post_range <= 4*n_train_inner)[0]\n",
    "        coracle_n_train_post_range = oracle_n_train_post_range[idx]\n",
    "        coracle_avg_test_loss = oracle_avg_test_loss[idx]\n",
    "        coracle_top_10_loss = oracle_top_10_loss[idx]\n",
    "        coracle_bot_10_loss = oracle_bot_10_loss[idx]\n",
    "        ax2.fill_between(coracle_n_train_post_range,coracle_bot_10_loss, coracle_top_10_loss, alpha = 0.5, color = 'green')\n",
    "        ax2.plot(coracle_n_train_post_range, coracle_avg_test_loss, 'o-', c = 'green', label = 'Oracle')\n",
    "\n",
    "    if wrong_avg_test_loss is not None:\n",
    "        idx = np.where(wrong_n_train_post_range <= 4*n_train_inner)[0]\n",
    "        cwrong_n_train_post_range = wrong_n_train_post_range[idx]\n",
    "        cwrong_avg_test_loss = wrong_avg_test_loss[idx]\n",
    "        cwrong_top_10_loss = wrong_top_10_loss[idx]\n",
    "        cwrong_bot_10_loss = wrong_bot_10_loss[idx]\n",
    "        ax2.fill_between(cwrong_n_train_post_range,cwrong_bot_10_loss, cwrong_top_10_loss, alpha = 0.5, color = 'red')\n",
    "        ax2.plot(cwrong_n_train_post_range, cwrong_avg_test_loss, 'o-', c = 'red', label = 'Wrong weights')\n",
    "\n",
    "\n",
    "    if zero_avg_loss is not None:\n",
    "        ax2.fill_between(cn_train_post_range,zero_bot_10_loss*np.ones(len(cn_train_post_range)), zero_top_10_loss*np.ones(len(cn_train_post_range)), alpha = 0.5, color = 'yellow')\n",
    "\n",
    "        ax2.plot(cn_train_post_range, np.ones(len(cn_train_post_range))*zero_avg_loss, '-', c = 'yellow', label = 'Zero')\n",
    "\n",
    "    if noise_std is not None:\n",
    "        ax2.plot(cn_train_post_range, np.ones_like(cn_train_post_range)*(noise_std**2), '--', label = 'Noise variance', c = 'black')\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_prediction_reg(data_dict, feature_weights, n_train_post):\n",
    "    train_data_dict = data_dict['train'][n_train_post]\n",
    "    test_data_dict  = data_dict['test']\n",
    "\n",
    "\n",
    "    x_train_post = train_data_dict['x']\n",
    "    features_post = train_data_dict['features']\n",
    "    y_post = train_data_dict['y'][0]\n",
    "\n",
    "    w_post, loss = solve_ls(features_post, y_post, feature_weights)\n",
    "\n",
    "    y_post_pred = features_post@w_post\n",
    "\n",
    "    x_test_post = test_data_dict['x']\n",
    "    features_test_post = test_data_dict['features']\n",
    "    y_test_post = test_data_dict['y'][0]\n",
    "    y_test_post_pred = features_test_post@w_post\n",
    "\n",
    "\n",
    "\n",
    "    #For plotting purposes add x_train to x_test\n",
    "\n",
    "    x_test_post = np.concatenate([x_train_post, x_test_post])\n",
    "    y_test_post = np.concatenate([y_post, y_test_post])\n",
    "    y_test_post_pred = np.concatenate([y_post_pred, y_test_post_pred])\n",
    "\n",
    "    idx = np.argsort(x_test_post)\n",
    "    x_test_post = x_test_post[idx]\n",
    "    y_test_post = y_test_post[idx]\n",
    "    y_test_post_pred = y_test_post_pred[idx]\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = [12,5])\n",
    "    ax1.scatter(x_train_post, y_post, marker = 'o', color = 'red', alpha = 0.5, label = 'Training points true')\n",
    "    ax1.scatter(x_train_post, y_post_pred, marker='x', color = 'green', alpha = 0.5, label = 'Training points predictions')\n",
    "\n",
    "\n",
    "\n",
    "    ax1.plot(x_test_post, y_test_post_pred, label = 'Predicted function')\n",
    "    ax1.plot(x_test_post, y_test_post, label = 'True function')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_title('n_train_post =' +str(n_train_post))\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(np.abs(feature_weights), 'o-')\n",
    "    ax2.set_title('Feature weights')\n",
    "    ax2.set_xlabel('Feature #')\n",
    "    ax2.set_ylabel('abs(feature_weight)')\n",
    "#     ax2.set_yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_test_loss_classification(iteration, n_train_inner, n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss,\n",
    "        init_n_train_post_range=None,init_avg_test_loss=None, init_top_10_loss=None, init_bot_10_loss=None,\\\n",
    "            oracle_n_train_post_range=None,oracle_avg_test_loss=None, oracle_top_10_loss=None, oracle_bot_10_loss=None, zero_avg_loss = None, zero_top_10_loss = None, zero_bot_10_loss = None, noise_prob = None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = [12,5])\n",
    "\n",
    "\n",
    "    ax1.fill_between(n_train_post_range, bot_10_loss, top_10_loss, alpha = 0.5)\n",
    "    ax1.plot(n_train_post_range, avg_test_loss, 'o-', label = 'Iteration: ' + str(iteration))\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylabel('Test classification error')\n",
    "    ax1.set_xlabel('n_train_post')\n",
    "    ax1.set_title('Test classification error vs n_train_post')\n",
    "\n",
    "    if init_avg_test_loss is not None:\n",
    "        ax1.fill_between(init_n_train_post_range, init_bot_10_loss, init_top_10_loss, alpha = 0.5, color = 'orange')\n",
    "        ax1.plot(init_n_train_post_range, init_avg_test_loss, 'o-', c = 'orange', label = 'Init')\n",
    "\n",
    "\n",
    "\n",
    "    if oracle_avg_test_loss is not None:\n",
    "        ax1.fill_between(oracle_n_train_post_range,oracle_bot_10_loss, oracle_top_10_loss, alpha = 0.5, color = 'green')\n",
    "        ax1.plot(oracle_n_train_post_range, oracle_avg_test_loss, 'o-', c = 'green', label = 'Oracle')\n",
    "\n",
    "\n",
    "    if zero_avg_loss is not None:\n",
    "        ax1.fill_between(n_train_post_range,zero_bot_10_loss*np.ones(len(n_train_post_range)), zero_top_10_loss*np.ones(len(n_train_post_range)), alpha = 0.5, color = 'yellow')\n",
    "\n",
    "        ax1.plot(n_train_post_range, np.ones(len(n_train_post_range))*zero_avg_loss, '-', c = 'yellow', label = 'Zero')\n",
    "\n",
    "    if noise_prob is not None and noise_prob != 0:\n",
    "        ax1.plot(n_train_post_range, np.ones_like(n_train_post_range)*(noise_prob), '--', c = 'black', label = 'Noise variance')\n",
    "\n",
    "    ax1.legend()\n",
    "    idx = np.where(n_train_post_range <= 2*n_train_inner)[0]\n",
    "    cn_train_post_range = n_train_post_range[idx]\n",
    "    cavg_test_loss = avg_test_loss[idx]\n",
    "    ctop_10_loss = top_10_loss[idx]\n",
    "    cbot_10_loss = bot_10_loss[idx]\n",
    "\n",
    "\n",
    "    ax2.fill_between(cn_train_post_range, cbot_10_loss, ctop_10_loss, alpha = 0.5)\n",
    "    ax2.plot(cn_train_post_range, cavg_test_loss, 'o-', label = 'Iteration: ' + str(iteration))\n",
    "\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylabel('Test classification error')\n",
    "    ax2.set_xlabel('n_train_post')\n",
    "    ax2.set_title('Test mse vs n_train_post (zoomed)')\n",
    "\n",
    "    if init_avg_test_loss is not None:\n",
    "        idx = np.where(init_n_train_post_range <= 2*n_train_inner)[0]\n",
    "        cinit_n_train_post_range = init_n_train_post_range[idx]\n",
    "        cinit_avg_test_loss = init_avg_test_loss[idx]\n",
    "        cinit_top_10_loss = init_top_10_loss[idx]\n",
    "        cinit_bot_10_loss = init_bot_10_loss[idx]\n",
    "\n",
    "        ax2.fill_between(cinit_n_train_post_range, cinit_bot_10_loss, cinit_top_10_loss, alpha = 0.5, color = 'orange')\n",
    "        ax2.plot(cinit_n_train_post_range, cinit_avg_test_loss, 'o-', c = 'orange', label = 'Init')\n",
    "\n",
    "    if oracle_avg_test_loss is not None:\n",
    "        idx = np.where(oracle_n_train_post_range <= 2*n_train_inner)[0]\n",
    "        coracle_n_train_post_range = oracle_n_train_post_range[idx]\n",
    "        coracle_avg_test_loss = oracle_avg_test_loss[idx]\n",
    "        coracle_top_10_loss = oracle_top_10_loss[idx]\n",
    "        coracle_bot_10_loss = oracle_bot_10_loss[idx]\n",
    "        ax2.fill_between(coracle_n_train_post_range,coracle_bot_10_loss, coracle_top_10_loss, alpha = 0.5, color = 'green')\n",
    "        ax2.plot(coracle_n_train_post_range, coracle_avg_test_loss, 'o-', c = 'green', label = 'Oracle')\n",
    "\n",
    "\n",
    "    if zero_avg_loss is not None:\n",
    "        ax2.fill_between(cn_train_post_range,zero_bot_10_loss*np.ones(len(cn_train_post_range)), zero_top_10_loss*np.ones(len(cn_train_post_range)), alpha = 0.5, color = 'yellow')\n",
    "\n",
    "        ax2.plot(cn_train_post_range, np.ones(len(cn_train_post_range))*zero_avg_loss, '-', c = 'yellow', label = 'Zero')\n",
    "\n",
    "    if noise_prob is not None and noise_prob != 0:\n",
    "        ax2.plot(cn_train_post_range, np.ones_like(cn_train_post_range)*(noise_prob), '--', label = 'Noise probability', c = 'black')\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u9JsTHjpWvg"
   },
   "source": [
    "## MAML for Regression with Closed-Form Min-Norm Solution\n",
    "\n",
    "In this homework, we aim to learn a suitable set of feature weights for regression tasks originating from a distribution $D_T$. This distribution is defined using several entries in the `params_dict`. By default, the true feature indices, represented by `k_idx`, are set to ${5,6,7,8,9,10,11}$. The true coefficients for these features are generated as i.i.d samples from a uniform distribution $U[-1, 1]$, and then normalized to have a total length of 1. The total number of features is determined by the parameter `d`.\n",
    "\n",
    "For the inner training loop, the $x$ sample spacing is controlled by the `x_type` parameter. The meta-update, however, always utilizes uniformly random spaced samples to prevent differentiation issues with aliased features. In this exercise, we will only use uniformly random samples, which is a logical choice as it matches the meta-update requirement, and there is no specific reason to have different spacing for the inner training loop.\n",
    "\n",
    "An essential distinction between the original MAML paper and our implementation in this notebook is that *we employ the closed-form min-norm solution for regression* rather than gradient descent. Luckily, PyTorch allows us to backpropagate gradients through matrix inversion, enabling us to update the feature weights using the min-norm least squares solution instead of gradient descent steps. In later sections, we will utilize gradient descent for the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_Q_qLOZpWvg"
   },
   "outputs": [],
   "source": [
    "def closed_form_ls(F_t, y_t):\n",
    "    # Returns min norm least squares solution\n",
    "    w_t = F_t.T @ torch.inverse(F_t @ F_t.T) @ y_t\n",
    "    return w_t\n",
    "\n",
    "\n",
    "def meta_update_reg(w_t, feature_weights_t, n_train_meta, phi_type, d, k_idx, k_val, noise_std):\n",
    "    x_meta = generate_x(n_train_meta, 'uniform_random')\n",
    "    features_meta = featurize(x_meta, phi_type=phi_type, d=d, normalize=True)\n",
    "    features_meta_t = torch.tensor(features_meta)\n",
    "    y_meta = generate_y(features_meta, k_idx, k_val)\n",
    "    y_meta += np.random.normal(0, noise_std, y_meta.shape)\n",
    "\n",
    "    y_meta_t = torch.tensor(y_meta)\n",
    "    y_meta_pred_t = features_meta_t @ w_t\n",
    "\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    loss = criterion(y_meta_t, y_meta_pred_t)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_post_data_reg(x_type, phi_type, k_idx, num_tasks_test, d, n_train_post_range, n_test, noise_std):\n",
    "    k_val_test = np.random.uniform(-1, 1, size=(len(k_idx), num_tasks_test))\n",
    "    k_val_test /= np.linalg.norm(k_val_test, axis=0)\n",
    "\n",
    "    data_dict = {'train': {}, 'test': {}}\n",
    "\n",
    "    x_test = generate_x(n_test, 'uniform_random')\n",
    "    features_test = featurize(x_test, phi_type=phi_type, d=d, normalize=True)\n",
    "    data_dict['test']['features'] = features_test\n",
    "\n",
    "    data_dict['test']['x'] = x_test\n",
    "    data_dict['test']['y'] = []\n",
    "    for i in range(num_tasks_test):\n",
    "        k_val_post = k_val_test[:, i]\n",
    "        y_test = generate_y(features_test, k_idx, k_val_post)\n",
    "        data_dict['test']['y'].append(y_test)\n",
    "    for n_train_post in n_train_post_range:\n",
    "        data_dict['train'][n_train_post] = {}\n",
    "\n",
    "        x_train_post = generate_x(n_train_post, x_type)\n",
    "        features_post = featurize(x_train_post, phi_type=phi_type, d=d, normalize=True)\n",
    "\n",
    "        data_dict['train'][n_train_post]['x'] = x_train_post\n",
    "        data_dict['train'][n_train_post]['features'] = features_post\n",
    "\n",
    "        data_dict['train'][n_train_post]['y'] = []\n",
    "        for i in range(num_tasks_test):\n",
    "            k_val_post = k_val_test[:, i]\n",
    "            y_post = generate_y(features_post, k_idx, k_val_post)\n",
    "            y_post += np.random.normal(0, noise_std, y_post.shape)\n",
    "            data_dict['train'][n_train_post]['y'].append(y_post)\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def test_oracle_reg(data_dict, x_type, feature_weights, min_n_train_post=0):\n",
    "    # plt.plot(feature_weights, 'o-')\n",
    "    # plt.show()\n",
    "    # print(feature_weights)\n",
    "    k_idx = np.where(feature_weights != 0)[0]\n",
    "    feature_weights = None\n",
    "    test_loss_matrix = []\n",
    "\n",
    "    n_train_post_range = np.sort(np.array(list(data_dict['train'].keys())))\n",
    "    n_train_post_range = n_train_post_range[n_train_post_range >= min_n_train_post]\n",
    "\n",
    "    test_data_dict = data_dict['test']\n",
    "    features_test_post = test_data_dict['features']\n",
    "\n",
    "    for n_train_post in n_train_post_range:\n",
    "        # print(\"n\", n_train_post)\n",
    "        train_data_dict = data_dict['train'][n_train_post]\n",
    "\n",
    "        features_post = train_data_dict['features'][:, k_idx]\n",
    "        cfeatures_test_post = features_test_post[:, k_idx]\n",
    "\n",
    "        feature_norms = np.linalg.norm(features_post, axis=0)\n",
    "        r_idx = np.where(feature_norms > 1e-6)[0]\n",
    "\n",
    "        features_post = features_post[:, r_idx]\n",
    "        cfeatures_test_post = cfeatures_test_post[:, r_idx]\n",
    "\n",
    "        test_loss_array = []\n",
    "\n",
    "        for i in range(len(train_data_dict['y'])):\n",
    "            y_post = train_data_dict['y'][i]\n",
    "            # print(features_post.shape, y_post.shape)\n",
    "            if x_type == 'grid':\n",
    "                # Use ridge with small reguralizer to avoid crazy effects of poor conditioning\n",
    "                w_post, loss = solve_ridge(features_post, y_post, lambda_ridge=1e-12, weights=feature_weights)\n",
    "            else:\n",
    "                w_post, loss = solve_ls(features_post, y_post, weights=feature_weights)\n",
    "\n",
    "            y_post_pred = features_post @ w_post\n",
    "            y_test_post = test_data_dict['y'][i]\n",
    "            y_test_post_pred = cfeatures_test_post @ w_post\n",
    "\n",
    "            # Compute the regression loss\n",
    "            test_loss = np.mean((y_test_post - y_test_post_pred)**2)\n",
    "            test_loss_array.append(test_loss)\n",
    "\n",
    "        test_loss_matrix.append(test_loss_array)\n",
    "\n",
    "    test_loss_matrix = np.array(test_loss_matrix).T\n",
    "    avg_test_loss = np.mean(test_loss_matrix, 0)\n",
    "\n",
    "    top_10_loss = np.percentile(test_loss_matrix, 90, axis=0)\n",
    "    bot_10_loss = np.percentile(test_loss_matrix, 10, axis=0)\n",
    "\n",
    "    return n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss\n",
    "\n",
    "\n",
    "def test_reg(data_dict, feature_weights, min_n_train_post=0):\n",
    "    # plt.plot(feature_weights, 'o-')\n",
    "    # plt.show()\n",
    "    # print(feature_weights)\n",
    "    test_loss_matrix = []\n",
    "\n",
    "    n_train_post_range = np.sort(np.array(list(data_dict['train'].keys())))\n",
    "    n_train_post_range = n_train_post_range[n_train_post_range >= min_n_train_post]\n",
    "\n",
    "    test_data_dict = data_dict['test']\n",
    "    features_test_post = test_data_dict['features']\n",
    "\n",
    "    for n_train_post in n_train_post_range:\n",
    "        # print(\"n\", n_train_post)\n",
    "        train_data_dict = data_dict['train'][n_train_post]\n",
    "\n",
    "        features_post = train_data_dict['features']\n",
    "\n",
    "        test_loss_array = []\n",
    "        for i in range(len(train_data_dict['y'])):\n",
    "            y_post = train_data_dict['y'][i]\n",
    "            # print(features_post.shape, y_post.shape)\n",
    "            w_post, loss = solve_ls(features_post, y_post, feature_weights)\n",
    "            y_post_pred = features_post @ w_post\n",
    "\n",
    "            y_test_post = test_data_dict['y'][i]\n",
    "            y_test_post_pred = features_test_post @ w_post\n",
    "\n",
    "            # Compute the regression loss\n",
    "            test_loss = np.mean((y_test_post - y_test_post_pred)**2)\n",
    "            test_loss_array.append(test_loss)\n",
    "\n",
    "        test_loss_matrix.append(test_loss_array)\n",
    "\n",
    "    test_loss_matrix = np.array(test_loss_matrix).T\n",
    "    avg_test_loss = np.mean(test_loss_matrix, 0)\n",
    "\n",
    "    top_10_loss = np.percentile(test_loss_matrix, 90, axis=0)\n",
    "    bot_10_loss = np.percentile(test_loss_matrix, 10, axis=0)\n",
    "\n",
    "    return n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss\n",
    "\n",
    "\n",
    "def test_zero_reg(data_dict):\n",
    "    test_data_dict = data_dict['test']\n",
    "    ys = test_data_dict['y']\n",
    "\n",
    "    test_loss_array = []\n",
    "    for i in range(len(ys)):\n",
    "        y = ys[i]\n",
    "        # Compute the regression loss\n",
    "        test_loss = np.mean(y**2)\n",
    "        test_loss_array.append(test_loss)\n",
    "\n",
    "    avg_test_loss = np.mean(test_loss_array)\n",
    "    top_10_loss = np.percentile(test_loss_array, 90)\n",
    "    bot_10_loss = np.percentile(test_loss_array, 10)\n",
    "\n",
    "    return avg_test_loss, top_10_loss, bot_10_loss\n",
    "\n",
    "\n",
    "def meta_learning_reg_closed_form(params_dict):\n",
    "    seed = params_dict[\"seed\"]\n",
    "    n_train_inner = params_dict[\"n_train_inner\"]\n",
    "    n_train_meta = params_dict[\"n_train_meta\"]\n",
    "    # n_train_post = params_dict[\"n_train_post\"]\n",
    "    n_test_post = params_dict[\"n_test_post\"]\n",
    "    x_type = params_dict[\"x_type\"]\n",
    "    d = params_dict[\"d\"]\n",
    "    phi_type = params_dict[\"phi_type\"]\n",
    "    k_idx = params_dict[\"k_idx\"]\n",
    "    optimizer_type = params_dict[\"optimizer_type\"]\n",
    "    stepsize_meta = params_dict[\"stepsize_meta\"]\n",
    "    num_inner_tasks = params_dict[\"num_inner_tasks\"]\n",
    "    num_tasks_test = params_dict[\"num_tasks_test\"]\n",
    "    num_stats = params_dict[\"num_stats\"]\n",
    "    num_iterations = params_dict[\"num_iterations\"]\n",
    "    noise_std = params_dict.get('noise_std', 0)\n",
    "    num_n_train_post_range = params_dict['num_n_train_post_range']\n",
    "\n",
    "    # Set seed:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    stats_every = num_iterations // num_stats\n",
    "    init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = None, None, None, None\n",
    "\n",
    "    # Initialize meta parameter: weights on the d features\n",
    "    feature_weights_t = torch.tensor(np.ones(d), requires_grad=True)\n",
    "\n",
    "    # Define meta parameter optimizer\n",
    "    if optimizer_type == 'SGD':\n",
    "        opt_meta = torch.optim.SGD([feature_weights_t], lr=stepsize_meta)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        opt_meta = torch.optim.Adam([feature_weights_t], lr=stepsize_meta)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Meta training loop\n",
    "    # Get post train and test data\n",
    "    n_train_post_range = np.logspace(np.log10(1), np.log10(3 * d), num_n_train_post_range).astype('int')\n",
    "\n",
    "\n",
    "    must_have_points = [n_train_inner, len(k_idx) - 1, len(k_idx), len(k_idx) + 1]\n",
    "\n",
    "    for point in must_have_points:\n",
    "        if point not in n_train_post_range:\n",
    "            n_train_post_range = np.hstack([n_train_post_range, point])\n",
    "\n",
    "    n_train_post_range = np.sort(n_train_post_range)\n",
    "    n_train_post_range = np.unique(n_train_post_range)\n",
    "    # print(n_train_post_range)\n",
    "    data_dict = get_post_data_reg(x_type, phi_type, k_idx, num_tasks_test, d, n_train_post_range, n_test_post, noise_std)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        opt_meta.zero_grad()\n",
    "\n",
    "        # Get x and features\n",
    "        x = generate_x(n_train_inner, x_type)\n",
    "        features = featurize(x, phi_type=phi_type, d=d, normalize=True)\n",
    "        weighted_features_t = to_tensor(features) * feature_weights_t\n",
    "\n",
    "        # Loop over inner tasks\n",
    "        for t in range(num_inner_tasks):\n",
    "            # Get random coefficients\n",
    "            k_val = np.random.uniform(-1, 1, size=len(k_idx))\n",
    "            k_val /= np.linalg.norm(k_val)\n",
    "\n",
    "            # Generate y\n",
    "            y = generate_y(features, k_idx, k_val)\n",
    "            y += np.random.normal(0, noise_std, y.shape)\n",
    "            y_t = torch.tensor(y)\n",
    "\n",
    "            # Get closed form solution for w_t as a function of feature_weights_t\n",
    "            w_t = closed_form_ls(weighted_features_t, y_t)\n",
    "            w_t = w_t * feature_weights_t  # Reweight the coefficients so that we can multiply with unweighted features to get prediction\n",
    "\n",
    "            # Meta update\n",
    "            meta_loss = meta_update_reg(w_t, feature_weights_t, n_train_meta, phi_type, d, k_idx, k_val, noise_std)\n",
    "            meta_loss.backward(retain_graph=True)\n",
    "\n",
    "        if i == 0:\n",
    "            # n_train_post_range = np.logspace(0, np.log10(3 * d), 40).astype('int')\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i)\n",
    "            # #Oracle stats\n",
    "            oracle_feature_weights = np.zeros(d)\n",
    "            oracle_feature_weights[k_idx] = 1\n",
    "            oracle_n_train_post_range, oracle_avg_test_loss, oracle_top_10_loss, oracle_bot_10_loss = test_oracle_reg(\n",
    "                data_dict, feature_weights=oracle_feature_weights, x_type=x_type)\n",
    "\n",
    "            # plt.plot(oracle_n_train_post_range, oracle_avg_test_loss)\n",
    "            # plt.show()\n",
    "            zero_avg_loss, zero_top_10_loss, zero_bot_10_loss = test_zero_reg(data_dict)\n",
    "\n",
    "            feature_weights = to_numpy(feature_weights_t)\n",
    "            init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = test_reg(data_dict, feature_weights)\n",
    "\n",
    "            visualize_test_loss_reg(\n",
    "                0, n_train_inner, init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss,\n",
    "                oracle_n_train_post_range=oracle_n_train_post_range, oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss, oracle_bot_10_loss=oracle_bot_10_loss, zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss, zero_bot_10_loss=zero_bot_10_loss, noise_std=noise_std)\n",
    "\n",
    "            visualize_prediction_reg(data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        #Stats\n",
    "        if (i + 1) % stats_every == 0 or i == num_iterations - 1:\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i + 1)\n",
    "            feature_weights = to_numpy(feature_weights_t)\n",
    "\n",
    "            n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss = test_reg(data_dict, feature_weights)\n",
    "            visualize_test_loss_reg(\n",
    "                i, n_train_inner, n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss, init_n_train_post_range, init_avg_test_loss,\n",
    "                init_top_10_loss, init_bot_10_loss, oracle_n_train_post_range=oracle_n_train_post_range, oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss, oracle_bot_10_loss=oracle_bot_10_loss, zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss, zero_bot_10_loss=zero_bot_10_loss, noise_std=noise_std)\n",
    "            visualize_prediction_reg(data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        opt_meta.step()  # Finally update meta weights after loop through all tasks\n",
    "\n",
    "    return to_numpy(feature_weights_t), data_dict\n",
    "\n",
    "\n",
    "def meta_learning_reg_sgd(params_dict):\n",
    "    seed = params_dict[\"seed\"]\n",
    "    n_train_inner = params_dict[\"n_train_inner\"]\n",
    "    n_train_meta = params_dict[\"n_train_meta\"]\n",
    "#     n_train_post = params_dict[\"n_train_post\"]\n",
    "    n_test_post = params_dict[\"n_test_post\"]\n",
    "    x_type = params_dict[\"x_type\"]\n",
    "    d = params_dict[\"d\"]\n",
    "    phi_type = params_dict[\"phi_type\"]\n",
    "    k_idx = params_dict[\"k_idx\"]\n",
    "    optimizer_type = params_dict[\"optimizer_type\"]\n",
    "    stepsize_meta = params_dict[\"stepsize_meta\"]\n",
    "    num_inner_tasks = params_dict[\"num_inner_tasks\"]\n",
    "    num_tasks_test = params_dict[\"num_tasks_test\"]\n",
    "    num_stats = params_dict[\"num_stats\"]\n",
    "    num_iterations = params_dict[\"num_iterations\"]\n",
    "    noise_std = params_dict.get('noise_std', 0)\n",
    "    num_n_train_post_range = params_dict['num_n_train_post_range']\n",
    "\n",
    "    stepsize_inner = params_dict[\"stepsize_inner\"]\n",
    "    num_gd_steps = params_dict['num_gd_steps']\n",
    "\n",
    "    # Set seed:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    stats_every = num_iterations // num_stats\n",
    "    init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = None, None, None, None\n",
    "\n",
    "    # Define meta parameter optimizer\n",
    "    dm = DummyModel(d)\n",
    "\n",
    "    # Define meta parameter optimizer\n",
    "    if optimizer_type == 'SGD':\n",
    "        opt_meta = torch.optim.SGD([dm.feature_weights], lr=stepsize_meta)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        opt_meta = torch.optim.Adam([dm.feature_weights], lr=stepsize_meta)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Meta training loop\n",
    "    # Get post train and test data\n",
    "    n_train_post_range = np.logspace(np.log10(1), np.log10(3 * d), num_n_train_post_range).astype('int')\n",
    "\n",
    "    must_have_points = [n_train_inner, len(k_idx) - 1, len(k_idx), len(k_idx) + 1]\n",
    "\n",
    "    for point in must_have_points:\n",
    "        if point not in n_train_post_range:\n",
    "            n_train_post_range = np.hstack([n_train_post_range, point])\n",
    "\n",
    "    n_train_post_range = np.sort(n_train_post_range)\n",
    "    n_train_post_range = np.unique(n_train_post_range)\n",
    "    data_dict = get_post_data_reg(x_type, phi_type, k_idx, num_tasks_test, d, n_train_post_range, n_test_post, noise_std)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        opt_meta.zero_grad()\n",
    "\n",
    "        # Get x and features\n",
    "        x = generate_x(n_train_inner, x_type)\n",
    "        features = featurize(x, phi_type=phi_type, d=d, normalize=True)\n",
    "        features_t = to_tensor(features)\n",
    "\n",
    "        # Loop over inner tasks\n",
    "        for t in range(num_inner_tasks):\n",
    "            # Get random coefficients\n",
    "            k_val = np.random.uniform(-1, 1, size=len(k_idx))\n",
    "            k_val /= np.linalg.norm(k_val)\n",
    "\n",
    "            dm.coeffs = torch.nn.Parameter(torch.zeros(d).double())\n",
    "\n",
    "            # Generate y\n",
    "            y = generate_y(features, k_idx, k_val)\n",
    "            y += np.random.normal(0, noise_std, y.shape)\n",
    "            y_t = torch.tensor(y)\n",
    "\n",
    "            opt_inner = torch.optim.SGD([dm.coeffs], lr=stepsize_inner)\n",
    "            with higher.innerloop_ctx(dm, opt_inner, copy_initial_weights=False, track_higher_grads=True) as (mod, opt):\n",
    "\n",
    "                for j in range(num_gd_steps):\n",
    "                    y_pred = mod(features_t)\n",
    "                    loss = torch.mean((y_pred - y_t)**2)\n",
    "#                     opt_inner.zero_grad()\n",
    "#                     loss.backward()\n",
    "                    opt.step(loss)\n",
    "\n",
    "                # Meta update\n",
    "                meta_loss = meta_update_reg(mod.coeffs * mod.feature_weights, mod.feature_weights, n_train_meta, phi_type, d, k_idx, k_val, noise_std)\n",
    "                meta_loss.backward(retain_graph=True)\n",
    "\n",
    "        if i == 0:\n",
    "            # n_train_post_range = np.logspace(0, np.log10(3 * d), 40).astype('int')\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i)\n",
    "            # #Oracle stats\n",
    "            oracle_feature_weights = np.zeros(d)\n",
    "            oracle_feature_weights[k_idx] = 1\n",
    "            oracle_n_train_post_range, oracle_avg_test_loss, oracle_top_10_loss, oracle_bot_10_loss = test_oracle_reg(\n",
    "                data_dict, feature_weights=oracle_feature_weights, x_type=x_type)\n",
    "\n",
    "            # plt.plot(oracle_n_train_post_range, oracle_avg_test_loss)\n",
    "            # plt.show()\n",
    "            zero_avg_loss, zero_top_10_loss, zero_bot_10_loss = test_zero_reg(data_dict)\n",
    "\n",
    "            feature_weights = to_numpy(dm.feature_weights)\n",
    "            init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = test_reg(data_dict, feature_weights)\n",
    "\n",
    "            visualize_test_loss_reg(\n",
    "                0, n_train_inner, init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss,\n",
    "                oracle_n_train_post_range=oracle_n_train_post_range, oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss, oracle_bot_10_loss=oracle_bot_10_loss, zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss, zero_bot_10_loss=zero_bot_10_loss, noise_std=noise_std)\n",
    "\n",
    "            visualize_prediction_reg(data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        # Stats\n",
    "        if (i + 1) % stats_every == 0 or i == num_iterations - 1:\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i + 1)\n",
    "            feature_weights = to_numpy(dm.feature_weights)\n",
    "\n",
    "            n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss = test_reg(data_dict, feature_weights)\n",
    "            visualize_test_loss_reg(\n",
    "                i, n_train_inner, n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss, init_n_train_post_range, init_avg_test_loss,\n",
    "                init_top_10_loss, init_bot_10_loss, oracle_n_train_post_range=oracle_n_train_post_range, oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss, oracle_bot_10_loss=oracle_bot_10_loss, zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss, zero_bot_10_loss=zero_bot_10_loss, noise_std=noise_std)\n",
    "            visualize_prediction_reg(data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        opt_meta.step()  # Finally update meta weights after loop through all tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2x0UZ4PpWvh"
   },
   "outputs": [],
   "source": [
    "def get_params_dict_reg():\n",
    "\n",
    "    params_dict = {\n",
    "        #Parameters\n",
    "    'seed':7,\n",
    "    'n_train_inner':32, #Number of training samples for training inner task\n",
    "    'n_train_meta':64, #Number of training samples for updating the meta parameter\n",
    "\n",
    "    'n_train_post':32, #Number of training samples used after meta training has been done to learn the weights\n",
    "    'n_test_post':1000, #Number of samples used for plotting and evaluating test performance after meta training has been done\n",
    "\n",
    "    # 'x_type':'uniform_random', #sampling time for inner and post training tasks\n",
    "    # 'x_type':'grid', #sampling time for training tasks\n",
    "\n",
    "    'd':501, #Number of features\n",
    "    'phi_type':'fourier', #Feature type\n",
    "     'noise_std':1e-1, #standard deviation of awgn noise added during training\n",
    "    'optimizer_type':'SGD', #Can be either SGD or Adam\n",
    "    'k_idx':np.arange(5,11), #Frequency range present in tasks during meta training\n",
    "    'stepsize_meta':1e-2, #Stepsize used for meta updates\n",
    "\n",
    "     'num_inner_tasks':5, #Number of inner tasks for each meta update\n",
    "    'num_tasks_test':10, #Number of tasks to test on\n",
    "    'num_stats': 10, #Determines how often we collect stats\n",
    "\n",
    "    'num_iterations':100, #Iterations for training meta parameter\n",
    "        'num_n_train_post_range':40, #How many points do we use to generative test loss vs n_train_post curve\n",
    "\n",
    "    }\n",
    "    return params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE8AkDH-pWvi"
   },
   "source": [
    "### Training Points on a Grid\n",
    "\n",
    "In this subsection, we use grid-spaced datapoints in the inner loop of training, which is the same spacing rule you explored in the theory parts of this problem. Meanwhile, the meta-update and test data spacings will continue to use uniformly random samples. As a result, during the inner training loop, the features within each alias group will be identical, but each feature will be unique during the meta-update and when computing the test error.\n",
    "\n",
    "You should observe that the feature weights exhibit behavior similar to the limits you derived in the analytical parts, where the true features are favored (i.e., they have higher weights). However, there may be a noticeable difference in how certain other feature weights behave.\n",
    "\n",
    "**Run the next two cells then answer questions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eXAfpC3UpWvi",
    "outputId": "de103f15-a4a3-4664-b922-e16be1ccfc51"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Evenly spaced training points\n",
    "x_type = 'grid'\n",
    "params_dict = get_params_dict_reg()\n",
    "cparams_dict = params_dict.copy()\n",
    "cparams_dict['x_type'] = x_type\n",
    "_ = meta_learning_reg_closed_form(cparams_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDEmOr7UpWvi"
   },
   "source": [
    "For each logged iteration, we generate visualizations, consisting of two rows with two subfigures each, totaling four subfigures.\n",
    "\n",
    "- First Row:\n",
    "\n",
    "  - The left figure presents the test Mean Squared Error (MSE) loss with respect to the number of datapoints used for linear regression after meta-training, plotted on a log scale.\n",
    "    - The green curve represents the oracle test loss using only the features present in the true signal.\n",
    "    - The blue curve displays the test loss using the feature weights learned through meta-training.\n",
    "    - The orange curve marks the initial location (iteration-0) of the blue curve.\n",
    "    - For each curve, the solid line corresponds to the average test loss over 10 tasks, and the shaded band indicates the range between the 10th and 90th percentile.\n",
    "    - The yellow line shows a baseline case where we predict zero for each datapoint.\n",
    "    - The dashed line represents the noise variance used when generating the data.\n",
    "\n",
    "  - The right figure is a zoomed-in version of the left figure, focusing on specific details.\n",
    "\n",
    "- Second Row:\n",
    "\n",
    "  - The left figure compares the true function (orange) to the predicted function (blue) for one particular task.\n",
    "    - The red dots represent the training points.\n",
    "    - The green crosses indicate the predictions on these training points. Note that these coincide since we are in the overparameterized regime and can interpolate the training data.\n",
    "\n",
    "  - The right figure illustrates the learned feature weights as meta-training progresses. Observe how all 500 features were initially equally weighted with a value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFAAvUzbpWvi"
   },
   "source": [
    "#### Question\n",
    "\n",
    "Considering the plot of regression test loss versus `n_train_post`, **how does the performance of the meta-learned feature weights compare to the case where all feature weights are set to 1?** Additionally, **how does their performance compare to the oracle**, which performs regression using only the features present in the data? Can you **explain the reason for the downward spike observed at `n_train_post = 32`?** Include the answer in your written submission of the written assignent.\n",
    "\n",
    "#### Question\n",
    "\n",
    "By examining the changes in feature weights over time during meta-learning, **can you justify the observed improvement in performance?** Specifically, can you **explain why certain feature weights are driven towards zero**? Include the answer in your written submission of the written assignent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dstcwSthpWvi"
   },
   "source": [
    "## MAML for Regression with Gradient Descent\n",
    "\n",
    "In the previous sections, we demonstrated how to employ the closed-form min-norm least squares solution for training the meta-learning parameter (feature weights) in MAML. However, for many problems, we may not have access to closed-form solutions for the tasks we aim to solve. In such cases, we need to rely on iterative methods like gradient descent.\n",
    "\n",
    "For the regression task, we can perform gradient descent on the squared loss. However, it is crucial to preserve gradients with respect to the feature weights when calculating the coefficients during inner training. PyTorch enables us to achieve this using the `higher` library.\n",
    "\n",
    "Please note that in these experiments, we employ gradient descent in the inner loop for `num_gd_steps`. However, when testing our performance, we utilize the closed-form expression for the min-norm least squares solution. This is because, during the final performance evaluation, we must either execute enough iterations of gradient descent to approach the closed-form solution or directly use the closed-form solution. Interestingly, we will observe that even a single gradient descent step towards the solution during meta-training helps us learn the feature weights effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEXtKR1dpWvi"
   },
   "outputs": [],
   "source": [
    "def get_params_dict_reg_sgd():\n",
    "    params_dict = {\n",
    "        #Parameters\n",
    "    'seed':7,\n",
    "    'n_train_inner':32, #Number of training samples for training inner task\n",
    "    'n_train_meta':64, #Number of training samples for updating the meta parameter\n",
    "\n",
    "    'n_train_post':32, #Number of training samples used after meta training has been done to learn the weights\n",
    "    'n_test_post':1000, #Number of samples used for plotting and evaluating test performance after meta training has been done\n",
    "\n",
    "    'x_type':'uniform_random', #sampling time for inner and post training tasks\n",
    "    # 'x_type':'grid', #sampling time for training tasks\n",
    "\n",
    "    'd':501, #Number of features\n",
    "    'phi_type':'fourier', #Feature type\n",
    "     'noise_std':1e-1, #standard deviation of awgn noise added during training\n",
    "    'optimizer_type':'SGD', #Optimizer type for meta updates Can be either SGD or Adam\n",
    "    'k_idx':np.arange(5,11), #Frequency range present in tasks during meta training\n",
    "    'stepsize_meta':1e-2, #Stepsize used for meta updates\n",
    "\n",
    "     'num_inner_tasks':5, #Number of inner tasks for each meta update\n",
    "    'num_tasks_test':10, #Number of tasks to test on\n",
    "    'num_stats': 10, #Determines how often we collect stats\n",
    "\n",
    "    'num_iterations':100, #Iterations for training meta parameter\n",
    "        'num_n_train_post_range':40, #How many points do we use to generative test loss vs n_train_post curve\n",
    "\n",
    "\n",
    "    'stepsize_inner':1e-2, #Stepsize for GD update in inner tasks,\n",
    "    'num_gd_steps':5, #Number of GD steps in inner task to move towards min norm ls solution\n",
    "\n",
    "    }\n",
    "    return params_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gOvywyFnpWvj",
    "outputId": "8a9d1ab8-942c-48d1-8f61-2d7bb5a22a5d"
   },
   "outputs": [],
   "source": [
    "params_dict = get_params_dict_reg_sgd()\n",
    "num_gd_steps = 5\n",
    "cparams_dict = params_dict.copy()\n",
    "cparams_dict['num_gd_steps'] = num_gd_steps\n",
    "\n",
    "meta_learning_reg_sgd(cparams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-JrHgtFTpWvj",
    "outputId": "05f64296-0bd3-4954-9357-90f938232083"
   },
   "outputs": [],
   "source": [
    "params_dict = get_params_dict_reg_sgd()\n",
    "num_gd_steps = 1\n",
    "cparams_dict = params_dict.copy()\n",
    "cparams_dict['num_gd_steps'] = num_gd_steps\n",
    "\n",
    "meta_learning_reg_sgd(cparams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDrRMl_9pWvj"
   },
   "source": [
    "#### Question\n",
    "\n",
    "**With `num_gd_steps = 5`, does meta-learning contribute to improved performance during test time? Furthermore, if we change this to `num_gd_steps = 1`, does meta-learning continue to function effectively?** Include the answer in your written submission of the written assignent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7LGn_aKpWvj"
   },
   "source": [
    "## MAML for Classification\n",
    "\n",
    "\n",
    "Suppose we want to learn an effective set of feature weights for performing classification tasks. We will use the same setup as the regression problem, but now our training data consists of pairs $(x_i, z_i)$, where $z_i = \\text{sgn}(f(x_i))$. The function $f$ represents the underlying true function that is sampled from a distribution $D_T$ as before.\n",
    "\n",
    "Given a set of feature weights and their corresponding weighted features $\\phi_w$, we solve the logistic regression problem to learn a set of coefficients $\\alpha$. Using these coefficients, we assign a label to a test point $x_{\\text{test}}$ as $\\hat{z}_{test} = sgn(\\langle \\alpha, \\phi_w(x_{test}) \\rangle)$. The test loss of interest is the classification error $\\mathbb{E}[\\hat{z}_{test} \\neq z_{test}]$, where the expectation is taken over the randomness in the test point. Since the test loss is not differentiable, we use the logistic loss during training (logistic regression).\n",
    "\n",
    "Next, we describe the process of learning the coefficients $\\alpha \\in \\mathbb{R}^d$. We have training data pairs $(x_i, z_i)$, where $x_i$ represents a data point, and $z_i$ denotes its label from the set ${+1, -1}$. For each point $x_i$, we have a set of weighted features $\\phi_w(x_i)$.\n",
    "\n",
    "Recall that a standard logistic function is defined as $$g(t) = \\frac{1}{e^{-t}+1}$$ (refer to: https://en.wikipedia.org/wiki/Logistic_function), which possesses the useful property of $g(-t) = 1 - g(t)$. Consequently, we can use it to model the binary probability: given a value $t$ that might belong to two classes with labels $+1$ and $-1$, we can model the probability of $t$ being part of class $+1$ as $p_1(t) = g(t)$, and conveniently obtain $1 - g(t) = p_{-1}(t)$. In our classification problem setup, this $t = z_i\\langle \\alpha, \\phi_w(x_i) \\rangle$. Thus, $g(t)$ can be interpreted as the probability of \"correct classification\", because for a correct prediction, $t = z_i\\langle \\alpha, x_i \\rangle$ will always be positive, and its probability $g(t)$ will be appropriately limited to 1.\n",
    "\n",
    "Therefore, to maximize the total log probability of predicting the correct label on all datapoints we find the $\\alpha$ that maximizes\n",
    "$$\\Sigma_i \\log{ \\frac{1}{e^{-z_i\\langle \\alpha, \\phi_w(x_i) \\rangle}+1} }.$$ Flipping the sign, this is equivalent to finding the $\\alpha$ that  minimizes the loss\n",
    "$$ - \\Sigma_i \\log{ \\frac{1}{e^{-z_i\\langle \\alpha, \\phi_w(x_i) \\rangle}+1} },$$ as listed here: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "One way to find this $\\alpha$ is by using gradient descent similar to what we did in the regression setting. In the regression setting we used the squared loss but in the classification setting we use the logistic loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JOWnoRgpWvj"
   },
   "source": [
    "**Complete the following code** and make sure it run without any errors though the meta learning is being done on regression tasks and test losses are being computed on the regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP6Qom-3pWvj"
   },
   "outputs": [],
   "source": [
    "def get_post_data_classification(x_type, phi_type, k_idx, num_tasks_test, d, n_train_post_range, n_test, noise_prob):\n",
    "\n",
    "    k_val_test = np.random.uniform(-1, 1, size=(len(k_idx), num_tasks_test))\n",
    "    k_val_test /= np.linalg.norm(k_val_test, axis=0)\n",
    "\n",
    "    data_dict = {'train': {}, 'test': {}}\n",
    "\n",
    "    # features_post_complete = featurize(x_train_post_complete, phi_type=phi_type,d=d, normalize = True)\n",
    "    x_test = generate_x(n_test, 'uniform_random')\n",
    "    features_test = featurize(x_test, phi_type=phi_type, d=d, normalize=True)\n",
    "    data_dict['test']['features'] = features_test\n",
    "\n",
    "    data_dict['test']['x'] = x_test\n",
    "    data_dict['test']['y'] = []\n",
    "    for i in range(num_tasks_test):\n",
    "        k_val_post = k_val_test[:, i]\n",
    "        y_test = generate_y(features_test, k_idx, k_val_post)\n",
    "        data_dict['test']['y'].append(y_test)\n",
    "    for n_train_post in n_train_post_range:\n",
    "        data_dict['train'][n_train_post] = {}\n",
    "\n",
    "        x_train_post = generate_x(n_train_post, x_type)\n",
    "        features_post = featurize(x_train_post, phi_type=phi_type, d=d, normalize=True)\n",
    "\n",
    "        data_dict['train'][n_train_post]['x'] = x_train_post\n",
    "        data_dict['train'][n_train_post]['features'] = features_post\n",
    "        data_dict['train'][n_train_post]['y'] = []\n",
    "\n",
    "        for i in range(num_tasks_test):\n",
    "            k_val_post = k_val_test[:, i]\n",
    "            y_post = generate_y(features_post, k_idx, k_val_post)\n",
    "            y_post = add_label_noise(y_post, noise_prob)\n",
    "            # y_post += np.random.normal(0, noise_std, y_post.shape)\n",
    "            data_dict['train'][n_train_post]['y'].append(y_post)\n",
    "    return data_dict\n",
    "\n",
    "def meta_update_classification(w_t, feature_weights_t, n_train_meta, phi_type, d, k_idx, k_val, noise_prob):\n",
    "    x_meta = generate_x(n_train_meta, 'uniform_random')\n",
    "    features_meta = featurize(x_meta, phi_type=phi_type, d=d, normalize=True)\n",
    "    features_meta_t = torch.tensor(features_meta)\n",
    "    y_meta = generate_y(features_meta, k_idx, k_val)\n",
    "    y_meta = add_label_noise(y_meta, noise_prob)\n",
    "    # y_meta += np.random.normal(0, noise_prob, y_meta.shape)\n",
    "\n",
    "    y_meta_t = torch.tensor(y_meta)\n",
    "    y_meta_pred_t = features_meta_t @ w_t\n",
    "\n",
    "    ############################################################################\n",
    "    # TODO: Complete the following code\n",
    "    #\n",
    "    # Hint: In regression, we have written:\n",
    "    #   criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "    #   loss = criterion( y_meta_t, y_meta_pred_t)\n",
    "    # Now for classification, we need a different loss, defined in the text block.\n",
    "    # Also, use my_sign for the sign function, to avoid some stupid compatibility issues.\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def meta_learning_classification(params_dict):\n",
    "    seed = params_dict[\"seed\"]\n",
    "    n_train_inner = params_dict[\"n_train_inner\"]\n",
    "    n_train_meta = params_dict[\"n_train_meta\"]\n",
    "    # n_train_post = params_dict[\"n_train_post\"]\n",
    "    n_test_post = params_dict[\"n_test_post\"]\n",
    "    x_type = params_dict[\"x_type\"]\n",
    "    d = params_dict[\"d\"]\n",
    "    phi_type = params_dict[\"phi_type\"]\n",
    "    k_idx = params_dict[\"k_idx\"]\n",
    "    optimizer_type = params_dict[\"optimizer_type\"]\n",
    "    stepsize_meta = params_dict[\"stepsize_meta\"]\n",
    "    num_inner_tasks = params_dict[\"num_inner_tasks\"]\n",
    "    num_tasks_test = params_dict[\"num_tasks_test\"]\n",
    "    num_stats = params_dict[\"num_stats\"]\n",
    "    num_iterations = params_dict[\"num_iterations\"]\n",
    "    noise_prob = params_dict['noise_prob']\n",
    "    num_n_train_post_range = params_dict['num_n_train_post_range']\n",
    "\n",
    "    stepsize_inner = params_dict[\"stepsize_inner\"]\n",
    "    num_gd_steps = params_dict['num_gd_steps']\n",
    "\n",
    "    # Set seed:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Parameters\n",
    "    stats_every = num_iterations // num_stats\n",
    "    init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = None, None, None, None\n",
    "\n",
    "    dm = DummyModel(d)\n",
    "    # Define meta parameter optimizer\n",
    "    if optimizer_type == 'SGD':\n",
    "        opt_meta = torch.optim.SGD([dm.feature_weights], lr=stepsize_meta)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        opt_meta = torch.optim.Adam([dm.feature_weights], lr=stepsize_meta)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # Meta training loop\n",
    "    # Get post train and test data\n",
    "    n_train_post_range = np.logspace(np.log10(24), np.log10(3 * d), num_n_train_post_range).astype('int')\n",
    "\n",
    "    # must_have_points  = [n_train_inner, len(k_idx)-1, len(k_idx), len(k_idx)+1]\n",
    "    must_have_points = [n_train_inner]\n",
    "    for point in must_have_points:\n",
    "        if point not in n_train_post_range:\n",
    "            n_train_post_range = np.hstack([n_train_post_range, point])\n",
    "\n",
    "    n_train_post_range = np.sort(n_train_post_range)\n",
    "    n_train_post_range = np.unique(n_train_post_range)\n",
    "    data_dict = get_post_data_classification(\n",
    "        x_type, phi_type, k_idx, num_tasks_test, d, n_train_post_range, n_test_post, noise_prob)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        opt_meta.zero_grad()\n",
    "\n",
    "        # Get x and features\n",
    "        x = generate_x(n_train_inner, x_type)\n",
    "        features = featurize(x, phi_type=phi_type, d=d, normalize=True)\n",
    "        features_t = to_tensor(features)\n",
    "\n",
    "        # Loop over inner tasks\n",
    "        for t in range(num_inner_tasks):\n",
    "            # Get random coefficients\n",
    "            k_val = np.random.uniform(-1, 1, size=len(k_idx))\n",
    "            k_val /= np.linalg.norm(k_val)\n",
    "            dm.coeffs = torch.nn.Parameter(torch.zeros(d).double())\n",
    "\n",
    "            # Generate y\n",
    "            y = generate_y(features, k_idx, k_val)\n",
    "            # y += np.random.normal(0, noise_std, y.shape)\n",
    "\n",
    "            y = add_label_noise(y, noise_prob)\n",
    "            y_t = torch.tensor(y)\n",
    "\n",
    "            opt_inner = torch.optim.SGD([dm.coeffs], lr=stepsize_inner, weight_decay=1e-5)\n",
    "            with higher.innerloop_ctx(dm, opt_inner, copy_initial_weights=False, track_higher_grads=True) as (mod, opt):\n",
    "\n",
    "                for j in range(num_gd_steps):\n",
    "                    y_pred = mod(features_t)\n",
    "\n",
    "                    ############################################################\n",
    "                    # TODO: Complete the following code\n",
    "                    #\n",
    "                    # Hint: In regression, we have written:\n",
    "                    #   loss = torch.mean((y_pred - y_t)**2)\n",
    "                    # Now for classification, we need a different loss, defined in the text block.\n",
    "                    # Also, use my_sign for the sign function, to avoid some stupid compatibility issues.\n",
    "                    ############################################################\n",
    "                    ############################################################\n",
    "\n",
    "                    opt_inner.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    opt.step(loss)\n",
    "\n",
    "                # Meta update\n",
    "                meta_loss = meta_update_classification(\n",
    "                    mod.coeffs * mod.feature_weights, mod.feature_weights, n_train_meta, phi_type, d, k_idx, k_val, noise_prob)\n",
    "                meta_loss.backward(retain_graph=True)\n",
    "\n",
    "        if i == 0:\n",
    "            # n_train_post_range = np.logspace(0,np.log10(3*d), 40).astype('int')\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i)\n",
    "            # Oracle stats\n",
    "\n",
    "            # Start comment for dev\n",
    "            oracle_feature_weights = np.zeros(d)\n",
    "            oracle_feature_weights[k_idx] = 1\n",
    "            oracle_n_train_post_range, oracle_avg_test_loss, oracle_top_10_loss, oracle_bot_10_loss = test_classification_oracle(\n",
    "                data_dict, feature_weights=oracle_feature_weights, x_type=x_type)\n",
    "\n",
    "            # plt.plot(oracle_n_train_post_range, oracle_avg_test_loss)\n",
    "            # plt.show()\n",
    "\n",
    "            # oracle_n_train_post_range,oracle_avg_test_loss, oracle_top_10_loss, oracle_bot_10_loss = None, None, None, None\n",
    "\n",
    "            zero_avg_loss, zero_top_10_loss, zero_bot_10_loss = test_classification_zero(data_dict)\n",
    "            # zero_avg_loss, zero_top_10_loss, zero_bot_10_loss  = None, None, None\n",
    "\n",
    "            # End comment for dev\n",
    "\n",
    "            feature_weights = to_numpy(dm.feature_weights)\n",
    "            init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss = test_classification(\n",
    "                data_dict, feature_weights)\n",
    "\n",
    "            visualize_test_loss_classification(\n",
    "                0, n_train_inner, init_n_train_post_range, init_avg_test_loss, init_top_10_loss, init_bot_10_loss,\n",
    "                oracle_n_train_post_range=oracle_n_train_post_range, oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss, oracle_bot_10_loss=oracle_bot_10_loss, zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss, zero_bot_10_loss=zero_bot_10_loss, noise_prob=noise_prob)\n",
    "\n",
    "            visualize_prediction_classification(\n",
    "                data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        # Stats\n",
    "        if (i+1) % stats_every == 0 or i == num_iterations - 1:\n",
    "\n",
    "            print(\"-\" * 70)\n",
    "            print(\"Iteration: \", i + 1)\n",
    "\n",
    "            feature_weights = to_numpy(dm.feature_weights)\n",
    "\n",
    "            n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss = test_classification(\n",
    "                data_dict, feature_weights)\n",
    "            visualize_test_loss_classification(\n",
    "                i, n_train_inner, n_train_post_range, avg_test_loss,\n",
    "                top_10_loss, bot_10_loss, init_n_train_post_range, init_avg_test_loss,\n",
    "                init_top_10_loss, init_bot_10_loss,\n",
    "                oracle_n_train_post_range=oracle_n_train_post_range,\n",
    "                oracle_avg_test_loss=oracle_avg_test_loss,\n",
    "                oracle_top_10_loss=oracle_top_10_loss,\n",
    "                oracle_bot_10_loss=oracle_bot_10_loss,\n",
    "                zero_avg_loss=zero_avg_loss,\n",
    "                zero_top_10_loss=zero_top_10_loss,\n",
    "                zero_bot_10_loss=zero_bot_10_loss,\n",
    "                noise_prob=noise_prob)\n",
    "            visualize_prediction_classification(\n",
    "                data_dict, feature_weights, n_train_inner)\n",
    "\n",
    "        opt_meta.step()  # Finally update meta weights after loop through all tasks\n",
    "\n",
    "\n",
    "def test_classification(data_dict, feature_weights, min_n_train_post=0):\n",
    "    # plt.plot(feature_weights, 'o-')\n",
    "    # plt.show()\n",
    "    # print(feature_weights)\n",
    "    # Added for dev\n",
    "    test_loss_matrix = []\n",
    "\n",
    "    n_train_post_range = np.sort(np.array(list(data_dict['train'].keys())))\n",
    "\n",
    "    n_train_post_range = n_train_post_range[n_train_post_range >= min_n_train_post]\n",
    "\n",
    "    test_data_dict = data_dict['test']\n",
    "    features_test_post = test_data_dict['features']\n",
    "\n",
    "    # n_train_post_range = np.array([32])\n",
    "\n",
    "    for n_train_post in n_train_post_range:\n",
    "        # print(\"n\", n_train_post)\n",
    "        train_data_dict = data_dict['train'][n_train_post]\n",
    "        features_post = train_data_dict['features']\n",
    "\n",
    "        test_loss_array = []\n",
    "        for i in range(len(train_data_dict['y'])):\n",
    "\n",
    "            y_post = train_data_dict['y'][i]\n",
    "            # print(features_post.shape, y_post.shape)\n",
    "            z_post = np.squeeze(my_sign(y_post))\n",
    "\n",
    "            w_post, loss = solve_logistic(\n",
    "                features_post, z_post, feature_weights)\n",
    "            y_post_pred = features_post @ w_post\n",
    "\n",
    "            y_test_post = test_data_dict['y'][i]\n",
    "            y_test_post_pred = features_test_post@w_post\n",
    "\n",
    "            z_post_pred = np.squeeze(my_sign(y_post_pred))\n",
    "            z_test_post = np.squeeze(my_sign(y_test_post))\n",
    "            z_test_post_pred = np.squeeze(my_sign(y_test_post_pred))\n",
    "\n",
    "            # Compute the regression loss\n",
    "            test_loss = np.mean(z_test_post != z_test_post_pred)\n",
    "            test_loss_array.append(test_loss)\n",
    "\n",
    "        test_loss_matrix.append(test_loss_array)\n",
    "\n",
    "    test_loss_matrix = np.array(test_loss_matrix).T\n",
    "    avg_test_loss = np.mean(test_loss_matrix, 0)\n",
    "\n",
    "    top_10_loss = np.percentile(test_loss_matrix, 90, axis=0)\n",
    "    bot_10_loss = np.percentile(test_loss_matrix, 10, axis=0)\n",
    "\n",
    "    return n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss\n",
    "\n",
    "\n",
    "def test_classification_zero(data_dict):\n",
    "    test_data_dict = data_dict['test']\n",
    "    ys = test_data_dict['y']\n",
    "\n",
    "    test_loss_array = []\n",
    "    for i in range(len(ys)):\n",
    "        y = ys[i]\n",
    "        z = my_sign(y)\n",
    "        z_guess = np.random.randint(low=-1, high=1, size=z.shape)\n",
    "        z_guess = my_sign(z_guess)\n",
    "        test_loss = np.mean(z != z_guess)\n",
    "        # NOTE: for regression, the initial loss is np.mean(y**2), i.e. predicting zero for every point,\n",
    "        # here the equivalent would be randomly guessing signs as labels\n",
    "        test_loss_array.append(test_loss)\n",
    "\n",
    "    avg_test_loss = np.mean(test_loss_array)\n",
    "    top_10_loss = np.percentile(test_loss_array, 90)\n",
    "    bot_10_loss = np.percentile(test_loss_array, 10)\n",
    "\n",
    "    return avg_test_loss, top_10_loss, bot_10_loss\n",
    "\n",
    "\n",
    "def test_classification_oracle(data_dict, x_type, feature_weights, min_n_train_post=0):\n",
    "    # plt.plot(feature_weights, 'o-')\n",
    "    # plt.show()\n",
    "    # print(feature_weights)\n",
    "    k_idx = np.where(feature_weights != 0)[0]\n",
    "    feature_weights = None\n",
    "\n",
    "    test_loss_matrix = []\n",
    "\n",
    "    n_train_post_range = np.sort(np.array(list(data_dict['train'].keys())))\n",
    "\n",
    "    n_train_post_range = n_train_post_range[n_train_post_range >= min_n_train_post]\n",
    "\n",
    "    test_data_dict = data_dict['test']\n",
    "    features_test_post = test_data_dict['features']\n",
    "\n",
    "    for n_train_post in n_train_post_range:\n",
    "        # print(\"n\", n_train_post)\n",
    "        train_data_dict = data_dict['train'][n_train_post]\n",
    "\n",
    "        features_post = train_data_dict['features'][:, k_idx]\n",
    "        cfeatures_test_post = features_test_post[:, k_idx]\n",
    "\n",
    "        feature_norms = np.linalg.norm(features_post, axis=0)\n",
    "        r_idx = np.where(feature_norms > 1e-6)[0]\n",
    "\n",
    "        features_post = features_post[:, r_idx]\n",
    "        cfeatures_test_post = cfeatures_test_post[:, r_idx]\n",
    "\n",
    "        test_loss_array = []\n",
    "\n",
    "        for i in range(len(train_data_dict['y'])):\n",
    "            y_post = train_data_dict['y'][i]\n",
    "            z_post = np.squeeze(my_sign(y_post))\n",
    "\n",
    "            w_post, loss = solve_logistic(features_post, z_post, feature_weights)\n",
    "\n",
    "            y_post_pred = features_post @ w_post\n",
    "\n",
    "            y_test_post = test_data_dict['y'][i]\n",
    "            y_test_post_pred = cfeatures_test_post@w_post\n",
    "\n",
    "            z_test_post = np.squeeze(my_sign(y_test_post))\n",
    "            z_test_post_pred = np.squeeze(my_sign(y_test_post_pred))\n",
    "            test_loss = np.mean(z_test_post != z_test_post_pred)\n",
    "            # print(test_loss)\n",
    "            # Compute the regression loss\n",
    "            test_loss_array.append(test_loss)\n",
    "\n",
    "        test_loss_matrix.append(test_loss_array)\n",
    "\n",
    "    test_loss_matrix = np.array(test_loss_matrix).T\n",
    "    avg_test_loss = np.mean(test_loss_matrix, 0)\n",
    "\n",
    "    top_10_loss = np.percentile(test_loss_matrix, 90, axis=0)\n",
    "    bot_10_loss = np.percentile(test_loss_matrix, 10, axis=0)\n",
    "\n",
    "    return n_train_post_range, avg_test_loss, top_10_loss, bot_10_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Py8xkMMMpWvk",
    "outputId": "321f36b0-0715-46bb-8160-9706d3d9004a"
   },
   "outputs": [],
   "source": [
    "def get_params_dict_classification():\n",
    "    params_dict = {\n",
    "        #Parameters\n",
    "        'seed':7,\n",
    "        'n_train_inner':32, #Number of training samples for training inner task\n",
    "        'n_train_meta':64, #Number of training samples for updating the meta parameter\n",
    "\n",
    "        'n_train_post':32, #Number of training samples used after meta training has been done to learn the weights\n",
    "        'n_test_post':1000, #Number of samples used for plotting and evaluating test performance after meta training has been done\n",
    "\n",
    "        'x_type':'uniform_random', #sampling time for inner and post training tasks\n",
    "        # 'x_type':'grid', #sampling time for training tasks\n",
    "\n",
    "        'd':501, #Number of features\n",
    "        'phi_type':'fourier', #Feature type\n",
    "        'noise_prob':0.0, #standard deviation of awgn noise added during training\n",
    "        'optimizer_type':'SGD', #Optimizer type for meta updates Can be either SGD or Adam\n",
    "        'k_idx':np.arange(5,11), #Frequency range present in tasks during meta training\n",
    "        'stepsize_meta':1e-2, #Stepsize used for meta updates\n",
    "\n",
    "        'num_inner_tasks':5, #Number of inner tasks for each meta update\n",
    "        'num_tasks_test':10, #Number of tasks to test on\n",
    "        'num_stats': 10, #Determines how often we collect stats\n",
    "\n",
    "        'num_iterations':100, #Iterations for training meta parameter\n",
    "        'num_n_train_post_range':40, #How many points do we use to generative test loss vs n_train_post curve\n",
    "\n",
    "\n",
    "\n",
    "        'num_gd_steps':5, #Number of GD steps in inner task to move towards min norm ls solution\n",
    "\n",
    "    }\n",
    "    return params_dict\n",
    "\n",
    "params_dict = get_params_dict_classification()\n",
    "cparams_dict = params_dict.copy()\n",
    "stepsize_inner = 1e-2\n",
    "cparams_dict['stepsize_inner'] = stepsize_inner\n",
    "\n",
    "meta_learning_classification(cparams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rFG_cxdgpWvk",
    "outputId": "1c6c2bfd-572f-4c2d-ceb5-40b0d82fabb6"
   },
   "outputs": [],
   "source": [
    "params_dict = get_params_dict_classification()\n",
    "cparams_dict = params_dict.copy()\n",
    "stepsize_inner = 3e-1\n",
    "cparams_dict['stepsize_inner'] = stepsize_inner\n",
    "\n",
    "meta_learning_classification(cparams_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UKlLI2rpWvk"
   },
   "source": [
    "After making the required changes, the plots contain the following information:\n",
    "\n",
    "- First row\n",
    "  - The left plot shows the test classification error with respect to the number of data points used for logistic regression after meta-training is complete.\n",
    "    - The green curve represents the test loss from the oracle (using only features present in the true function).\n",
    "    - Blue curve shows the feature weights learned at the current meta-training iteration.\n",
    "    - The orange curve marks the performance at initialization with all ones feature weight.\n",
    "    - The yellow curve provides a baseline where we guess the same label for all points (resulting in a 0.5 classification error).\n",
    "    - In each curve, we plot the average, 10th, and 90th percentile of the classification error over 10 tasks.\n",
    "  \n",
    "  - The right plot on the first row is a zoomed-in version of the left plot.\n",
    "\n",
    "- Second row\n",
    "\n",
    "  - The left plot displays the true sign labels in orange and the predicted sign labels in green.\n",
    "    - The red dots represent the labels corresponding to the training points.\n",
    "    - The green crosses show our predictions on the training points.\n",
    "    - The brown curve illustrates the underlying true function.\n",
    "    - The purple curve presents the function given by $\\langle \\alpha, x \\rangle$ after normalizing $\\alpha$ to have a unit norm.\n",
    "\n",
    "  - The right plot on the second row illustrates the feature weights at the current iteration being meta-learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2AJWuDlpWvk"
   },
   "source": [
    "#### Question\n",
    "\n",
    "Based on the plot of classification error versus `n_train_post`, **how does the performance of the meta-learned feature weights compare to the case where all feature weights are 1? How does the performance of the meta-learned feature weights compare to the oracle** (which performs logistic regression using only the features present in the data)? Include the answer in your written submission of the written assignent.\n",
    "\n",
    "#### Question\n",
    "\n",
    "By observing the evolution of the feature weights over time as we perform meta-learning, **can you justify the improvement in performance? Specifically, can you explain why some feature weights are being driven towards zero?** Include the answer in your written submission of the written assignent."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
