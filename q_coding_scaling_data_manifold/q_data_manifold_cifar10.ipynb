{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8de507b-c3dd-4702-867e-bb262eee0a77",
   "metadata": {},
   "source": [
    "# Scaling from data manifold dimension: the case of CIFAR10\n",
    "\n",
    "This notebook continues our question about explaining scaling laws from the data manifold dimension. The purpose of this notebook is to reproduce Figure 6 from the paper \"Scaling laws from the data manifold dimension\" (Sharma, Kaplan, 2022).\n",
    "\n",
    "![](cifar_figure_6.png)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87e74f-87e8-43e8-9b06-784e930edb02",
   "metadata": {},
   "source": [
    "In words, the figure is produced by training a standard CNN with 3 convolution layers and 2 fully connected layers on CIFAR-10. The CIFAR-10 dataset is a popular benchmark, consisting of 60,000 32x32 RGB images in 10 different classes, with 6,000 images per class. \n",
    "\n",
    "While the images live in a space of dimension $32^2 \\times 3 = 3072$, (Sharma, Kaplan, 2022) reports that the CIFAR-10 images lies in a data manifold with dimension of only around 16--18.\n",
    "\n",
    "In order to run a controlled experiment, the experiments were run by varying as few parameters as possible. The training is done with the following designs:\n",
    "* The network architecture is fixed, and the network parameter count is changed by changing a single number: the number of channels in the convolutional layers.\n",
    "* Each training run lasts 50 epochs, with batch size 128.\n",
    "* Optimizer is `AdamW` with `lr=5e-4`.\n",
    "\n",
    "The experiment is run with 20 different network sizes, from 5408 to 115114.\n",
    "\n",
    "I generated all the data and logged them into tensorboard log files -- it took about 4 days of on-and-off computing (I don't have a powerful GPU). You can view them for yourself to get a feel for the shape of the dataset, by running `tensorboard --logdir=cifar10/logs`. Suffice to say that the dataset is not very clean, and accidents sometimes happen.\n",
    "\n",
    "* A few training runs terminated early thanks to GPU running out of memory, or the computer going to sleep. A few training runs got mashed together into the same logging file due to my choosing to start multiple training runs in parallel, and since they were all started at the exact same second, they all logged into the exact same file with the exact same name.\n",
    "* Some training runs only started decreasing in loss after a few epochs. A few training runs completely failed to train, with loss curves not decreasing. These delays and failures are not consistently reproducible. I have not managed to find out why, presumably due to some trivial accident.\n",
    "\n",
    "Despite these accidents, it is clean enough for us to proceed, with a little caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d2cbe-241f-40cb-ab0a-dc6503e57a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tbparse import SummaryReader\n",
    "import numpy as np\n",
    "plt.rcParams.update({'font.size': 16,\n",
    "                     'figure.titlesize':\"large\",\n",
    "                    'axes.titlesize':\"large\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ec728-b709-447d-a624-a4a55486e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    reader = SummaryReader(f\"./cifar10/logs/n_{i}\")\n",
    "    df_text = reader.text\n",
    "    model_size = df_text.loc[df_text['tag'] == 'Model size', 'value'].iloc[0]\n",
    "    \n",
    "    df_scalars = reader.scalars\n",
    "    \n",
    "    # Add \"channel\" and \"size\" columns to the scalars DataFrame\n",
    "    df_scalars['channel'] = i\n",
    "    df_scalars['size'] = model_size\n",
    "    \n",
    "    dfs.append(df_scalars)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ff29d-c41c-42bb-aef4-86b40b63309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in [\"Loss/Train\", \"Accuracy/Train\", \"Loss/Validation\", \"Accuracy/Validation\"]:\n",
    "    filtered_df = df[df['tag'] == tag]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(filtered_df['size'], filtered_df['value'], s=2)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.title(f'{tag}')\n",
    "    plt.xlabel('Network size')\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e5d9e-cc30-4b40-b3e5-325a389af5e7",
   "metadata": {},
   "source": [
    "The plots are not as clean as we like, because of some outliers due to failed or unlucky training runs. It is easier to see if we use the violin plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed94372-11a1-4246-adcb-f6cd84d288d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "filtered_df = df[df['tag'] == \"Loss/Train\"]\n",
    "network_sizes = filtered_df['size'].unique()\n",
    "\n",
    "data = [np.log(filtered_df[filtered_df['size'] == size]['value']) for size in network_sizes]\n",
    "ax.violinplot(data, np.log(network_sizes.astype('float')),\n",
    "                             showmeans=True, showextrema=True, showmedians=True, \n",
    "                             bw_method='scott')\n",
    "ax.set_xlabel('Log(Network size)')\n",
    "ax.set_ylabel('Log(Accuracy)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f28ea-ad92-4e5e-82f6-67ec30a30f6a",
   "metadata": {},
   "source": [
    "Now, create a plot that contains all 4 plots as subplots, for the training loss, validation loss, training accuracy, and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0035486e-022d-449d-818f-fa5a46e5aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 16))\n",
    "############################################################################\n",
    "# TODO: create the diagram\n",
    "############################################################################\n",
    "############################################################################\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3fbeb-612f-4ed3-a3b4-6eeeff349273",
   "metadata": {},
   "source": [
    "It looks pretty good. There are some small outliers, which are due to the unsuccessful training runs. We could have manually removed the unsuccessful training runs, but it's less effort to just run quantil regression, which can ignore the outliers quite well.\n",
    "\n",
    "With this, we can find the scaling law slopes $\\alpha$ by $0.5$-quantile regression (median regression) and compare $\\frac 4\\alpha$ with the theoretical expectation of around 16--18.\n",
    "\n",
    "Read up on [quantile regression from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html), then implement the function below. For reference, here is one of the plots we expect you to create:\n",
    "\n",
    "![](quantile_regression_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77c74a-00b7-4c57-b34f-e4d98113f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor\n",
    "quantiles = [0.2, 0.5, 0.8]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 16))\n",
    "############################################################################\n",
    "# TODO: perform quantile regression, and plot it on the graph\n",
    "############################################################################\n",
    "############################################################################\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc357ec3",
   "metadata": {},
   "source": [
    "Now that you have the figures, compare them with the published figures. Have we successfully reproduced the paper? According to our graphs, what appears to be the intrinsic dimension of the CIFAR10 data manifold?\n",
    "\n",
    "############################################################################\n",
    "\n",
    "TODO: answer the question\n",
    "\n",
    "############################################################################\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb5208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
