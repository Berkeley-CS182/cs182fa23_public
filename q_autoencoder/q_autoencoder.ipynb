{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "In this notebook, you will explore various design choices for AutoEncoders, including pretraining models with unsupervised learning and evaluating the learned representations with a linear classifier. Specifically, we will examine three different architectures:\n",
    "\n",
    "- Vanilla Autoencoder\n",
    "- Denoising Autoencoder\n",
    "- Masked Autoencoder\n",
    "\n",
    "By the end of this assignment, you will have gained a deep understanding of these techniques and their potential applications in real-world scenarios.\n",
    "\n",
    "**Note:** You have to run this notebook with a CUDA GPU. Otherwise, the training will be very very slow. For example, you can run it on a GPU instance on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import packages\n",
    "\n",
    "import time\n",
    "import json\n",
    "import inspect\n",
    "import random\n",
    "import argparse\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def _set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "TO_SAVE = {\"time\": time.time()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Datasets\n",
    "\n",
    "###Synthetic Dataset\n",
    "\n",
    "This is the definition for a synthetic dataset. The purpose of the dataset is to generate input data with a specified mean and covariance matrix, where the covariance is *high along a small fraction of the dimensions*. The class label of each example only depends on those high-variance dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset:\n",
    "    \"\"\"\n",
    "    Create a torch compatible dataset by sampling \n",
    "    features from a multivariate normal distribution with \n",
    "    specified mean and covariance matrix. In particular,\n",
    "    the covariance is high along a small fraction of the directions.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 samples=10000,\n",
    "                 splits=None,\n",
    "                 num_high_var_dims=2,\n",
    "                 var_scale=100,\n",
    "                 batch_size=100,\n",
    "                 eval_batch_size=200):\n",
    "        \"\"\"\n",
    "        input_size: (int) size of inputs\n",
    "        samples: (int) number of samples to generate\n",
    "        splits: list(float) of splitting dataset for [#train, #valid, #test]\n",
    "        num_high_var_dims : (int) #dimensions with scaled variance\n",
    "        var_scale : (float)\n",
    "        \"\"\"\n",
    "        train_split, valid_split, test_split = splits\n",
    "        self.input_size = input_size\n",
    "        self.samples = samples\n",
    "        self.num_high_var_dims = num_high_var_dims\n",
    "        self.var_scale = var_scale\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.num_train_samples = int(samples * train_split)\n",
    "        self.num_valid_samples = int(samples * valid_split)\n",
    "        self.num_test_samples = int(samples * test_split)\n",
    "        self._build()\n",
    "    \n",
    "    def _build(self):\n",
    "        \"\"\"\n",
    "        Covariance is scaled along num_high_var_dims. \n",
    "        Create torch compatible dataset.\n",
    "        \"\"\"\n",
    "        self.mean = np.zeros(self.input_size)\n",
    "        self.cov = np.eye(self.input_size)\n",
    "        self.cov[:self.num_high_var_dims, :self.num_high_var_dims] *= self.var_scale\n",
    "        self.X = np.random.multivariate_normal(self.mean, self.cov, self.samples)\n",
    "\n",
    "        # generate random rotation matrix with SVD\n",
    "        u, _, v = np.linalg.svd(np.random.randn(self.input_size, self.input_size))\n",
    "        sample = self.X @ u\n",
    "\n",
    "        # create classification labels that depend only on the high-variance dimensions\n",
    "        target = self.X[:, :self.num_high_var_dims].sum(axis=1) > 0\n",
    "\n",
    "        self.train_sample = torch.from_numpy(sample[:self.num_train_samples]).float()\n",
    "        self.train_target = torch.from_numpy(target[:self.num_train_samples]).long()\n",
    "\n",
    "        # create validation set\n",
    "        valid_sample_end = self.num_train_samples+self.num_valid_samples\n",
    "        self.valid_sample = torch.from_numpy(\n",
    "            sample[self.num_train_samples:valid_sample_end]).float()\n",
    "        self.valid_target = torch.from_numpy(\n",
    "            target[self.num_train_samples:valid_sample_end]).long()\n",
    "\n",
    "        # create test set\n",
    "        self.test_sample = torch.from_numpy(sample[valid_sample_end:]).float()\n",
    "        self.test_target = torch.from_numpy(target[valid_sample_end:]).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    def get_num_samples(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            return self.num_train_samples\n",
    "        elif split == \"valid\":\n",
    "            return self.num_valid_samples\n",
    "        elif split == \"test\":\n",
    "            return self.num_test_samples\n",
    "\n",
    "    def get_batch(self, batch_idx, split=\"train\"):\n",
    "        batch_size = (\n",
    "            self.batch_size\n",
    "            if split == \"train\"\n",
    "            else self.eval_batch_size\n",
    "        )\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        if split == \"train\":\n",
    "            return self.train_sample[start_idx:end_idx], self.train_target[start_idx:end_idx]\n",
    "        elif split == \"valid\":\n",
    "            return self.valid_sample[start_idx:end_idx], self.valid_target[start_idx:end_idx]\n",
    "        elif split == \"test\":\n",
    "            return self.test_sample[start_idx:end_idx], self.test_target[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###MNIST Dataset\n",
    "\n",
    "The MNIST dataset is defined in this code snippet. It loads each image in the dataset as a flattened vector of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    def __init__(self, batch_size, splits=None, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          batch_size : number of samples per batch\n",
    "          splits : [train_frac, valid_frac]\n",
    "          shuffle : (bool)\n",
    "        \"\"\"\n",
    "        # flatten the images\n",
    "        self.transform = torchvision.transforms.Compose(\n",
    "            [torchvision.transforms.ToTensor(),\n",
    "             torchvision.transforms.Lambda(lambda x: x.view(-1))])       \n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = 200\n",
    "        self.splits = splits\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self._build()\n",
    "      \n",
    "    def _build(self):\n",
    "        train_split, valid_split = self.splits\n",
    "        trainset = torchvision.datasets.MNIST(\n",
    "                root=\"data\", train=True, download=True, transform=self.transform)\n",
    "        num_samples = len(trainset)\n",
    "        self.num_train_samples = int(train_split * num_samples)\n",
    "        self.num_valid_samples = int(valid_split * num_samples)\n",
    "\n",
    "        # create training set \n",
    "        self.train_dataset = torch.utils.data.Subset(\n",
    "            trainset, range(0, self.num_train_samples))\n",
    "        self.train_loader = list(iter(torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )))\n",
    "        \n",
    "        # create validation set\n",
    "        self.valid_dataset = torch.utils.data.Subset(\n",
    "            trainset, range(self.num_train_samples, num_samples))\n",
    "        self.valid_loader = list(iter(torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.eval_batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )))\n",
    "\n",
    "        # create test set\n",
    "        test_dataset = torchvision.datasets.MNIST(\n",
    "            root=\"data\", train=False, download=True, transform=self.transform\n",
    "        )\n",
    "        self.test_loader = list(iter(torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.eval_batch_size,\n",
    "            shuffle=False,\n",
    "        )))\n",
    "        self.num_test_samples = len(test_dataset)\n",
    "\n",
    "    def get_num_samples(self, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            return self.num_train_samples\n",
    "        elif split == \"valid\":\n",
    "            return self.num_valid_samples\n",
    "        elif split == \"test\":\n",
    "            return self.num_test_samples\n",
    "\n",
    "    def get_batch(self, idx, split=\"train\"):\n",
    "        if split == \"train\":\n",
    "            return self.train_loader[idx]\n",
    "        elif split == \"valid\":\n",
    "            return self.valid_loader[idx]\n",
    "        elif split == \"test\":\n",
    "            return self.test_loader[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Autoencoder\n",
    "\n",
    "In this section, you will be implementing a vanilla autoencoder, which comprises of an encoder and a decoder, both of which are fully connected neural networks. The input $x\\in\\mathbb{R}^d$ is mapped to a latent representation $z$ by the encoder, which is then mapped back to $x'$. During training, the mean squared error between $x$ and $x'$ is minimized using the following formula:\n",
    "\n",
    "$$\\mathrm{Loss} = \\dfrac1{n} \\sum_{i=1}^n \\lVert x_{i,j} - x'_{i,j} \\rVert^2$$\n",
    "\n",
    "Here, $n$ is the number of samples in the dataset, $d$ is the dimensionality of each sample, $x_{i,j}$ is the $j$-th feature of the $i$-th sample, and $x'_{i,j}$ is the predicted value of the $j$-th feature of the $i$-th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder defines a general class of NN architectures\n",
    "           _____________                                     ___________\n",
    "          |             |                                   |           |                    \n",
    "    x --> |   ENCODER   | --> z (latent representation) --> |  DECODER  | --> x'\n",
    "          |_____________|                                   |___________|          \n",
    "\n",
    "    The `Autoencoder` class is a neural network architecture consisting of an\n",
    "    encoder and a decoder, each of which is a fully connected neural network.\n",
    "\n",
    "    The input `x` of size `input_size` is mapped to a latent representation `z`\n",
    "    by the encoder, which is then mapped back to x' by the decoder.\n",
    "    \n",
    "    The architecture is defined by a list of hidden layer sizes for the encoder\n",
    "    and decoder. The encoder and decoder are symmetric. The class provides\n",
    "    methods for encoding, decoding, and computing the loss (mean squared error)\n",
    "    between `x` and `x'`. A training step can be performed by calling the\n",
    "    `train_step` method with an input tensor `x` and an optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int],\n",
    "                 activation_cls: nn.Module = nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation_cls = activation_cls\n",
    "        self.encoder = self._build_encoder()\n",
    "        self.decoder = self._build_decoder()\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        layers = []\n",
    "        prev_size = self.input_size\n",
    "        for layer_id, size in enumerate(self.hidden_sizes):\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            if layer_id < len(self.hidden_sizes)-1:\n",
    "                layers.append(self.activation_cls())\n",
    "            prev_size = size\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        layers = []\n",
    "        ########################################################################\n",
    "        # TODO: Implement the code to construct the decoder. The decoder should\n",
    "        #       be symmetric to the encoder.\n",
    "        #\n",
    "        # Hint: Refer to the `_build_encoder` method above\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ########################################################################\n",
    "        # TODO: Implement the forward pass of the (vanilla) autoencoder\n",
    "        #       according to the diagram and documents above\n",
    "        #       The return value should be `x'`\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "    \n",
    "    def get_loss(self, x):\n",
    "        x_hat = self(x)\n",
    "        return self.loss(x, x_hat)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def loss(self, x: torch.Tensor, x_hat: torch.Tensor) -> torch.Tensor:\n",
    "        ########################################################################\n",
    "        # TODO: Implement the loss function\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "    def train_step(self, x: torch.Tensor, optimizer) -> torch.Tensor:\n",
    "        x_hat = self(x)\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_seed(2017)\n",
    "model = Autoencoder(7, [5, 4], nn.ReLU)\n",
    "assert set(model.state_dict().keys()) == {\n",
    "    'encoder.0.weight',\n",
    "    'encoder.0.bias',\n",
    "    'encoder.2.weight',\n",
    "    'encoder.2.bias',\n",
    "    'decoder.0.weight',\n",
    "    'decoder.0.bias',\n",
    "    'decoder.2.weight',\n",
    "    'decoder.2.bias'\n",
    "}\n",
    "TO_SAVE[\"ae.0\"] = sorted(list(model.state_dict().keys()))\n",
    "_set_seed(2022)\n",
    "x1 = torch.randn(2, 7)\n",
    "x2 = torch.randn(2, 7)\n",
    "assert torch.allclose(\n",
    "    model(x1).view(-1)[7:11],\n",
    "    torch.tensor([-0.10894767940044403, 0.41764578223228455, 0.21026797592639923, 0.08983835577964783]),\n",
    "    rtol=1e-03\n",
    ")\n",
    "TO_SAVE[\"ae.1\"] = model(x2).view(-1)[3:7].tolist()\n",
    "loss1 = model.loss(x1, model(x1))\n",
    "loss2 = model.loss(x2, model(x2))\n",
    "assert np.allclose(loss1.item(), 10.69554328918457, rtol=1e-03)\n",
    "TO_SAVE[\"ae.2\"] = loss2.item()\n",
    "loss1.backward()\n",
    "assert torch.allclose(\n",
    "    model.encoder[0].weight.grad.view(-1)[9:13],\n",
    "    torch.tensor([0.026928527280688286, 0.10433877259492874, -0.023865919560194016, -0.1060301661491394]),\n",
    "    rtol=1e-03\n",
    ")\n",
    "TO_SAVE[\"ae.3\"] = model.encoder[2].weight.grad.view(-1)[11:15].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder\n",
    "\n",
    "In this section, you will be implementing a denoising autoencoder, which inherits vanilla autoencoder you implemented before, but with an added noise reduction part. The input $x\\in\\mathbb{R}^d$ should be corrupted with Gaussian noise during training, and then fed to the encoder to obtain the latent representation $z$. The decoder then maps the latent representation back to the original, noise-free input $x'$. During training, the mean squared error between $x$ and $x'$ is minimized, similar to the vanilla autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(Autoencoder):\n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int],\n",
    "                 activation_cls: nn.Module = nn.ReLU, noise_std: float = 0.5):\n",
    "        super().__init__(input_size, hidden_sizes, activation_cls)\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def train_step(self, x: torch.Tensor, optimizer) -> torch.Tensor:\n",
    "        ########################################################################\n",
    "        # TODO: Implement training step of the denoising autoencoder.\n",
    "        #\n",
    "        # Hint: Add a zero-mean i.i.d. gaussian noise of a standard deviation of\n",
    "        #       `noise_std` to the input of the encoder\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_seed(2017)\n",
    "model = DenoisingAutoencoder(7, [5, 4], nn.ReLU)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
    "_set_seed(2022)\n",
    "x1 = torch.randn(2, 7)\n",
    "model.train_step(x1, optimizer)\n",
    "assert torch.allclose(\n",
    "    model.encoder[0].weight.view(-1)[2:6],\n",
    "    torch.tensor([-0.09830013662576675, -0.08394217491149902, 0.1265936940908432, 0.3746601641178131]),\n",
    "    rtol=1e-03\n",
    ")\n",
    "TO_SAVE[\"dae\"] = model.encoder[2].weight.view(-1)[3:7].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Autoencoder\n",
    "\n",
    "In this section, you will be implementing a masked autoencoder, which is similar to the vanilla autoencoder, but with an added masking feature. During training, the input $x\\in\\mathbb{R}^d$ should be masked with some binary mask, which zeros-out some random features in the input. The masked input is then fed to the encoder to obtain the latent representation $z$, and the decoder maps the latent representation back to the original input $x'$. During training, the mean squared error between the unmasked part of $x$ and the corresponding part of $x'$ is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(Autoencoder):\n",
    "    def __init__(self, input_size: int, hidden_sizes: List[int],\n",
    "                 activation_cls: nn.Module = nn.ReLU, mask_prob: float = 0.25):\n",
    "        super().__init__(input_size, hidden_sizes, activation_cls)\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def train_step(self, x: torch.Tensor, optimizer) -> torch.Tensor:\n",
    "        ########################################################################\n",
    "        # TODO: Implement training step of the masked autoencoder.\n",
    "        #\n",
    "        # Hint: Generate a mask with i.i.d. probabilities of mask_prob for each\n",
    "        #       entry, and apply it to the input of the encoder, setting to zero\n",
    "        #       the entries where the mask is activated.\n",
    "        ########################################################################\n",
    "        raise NotImplementedError()\n",
    "        ########################################################################\n",
    "        loss = self.loss(x, x_hat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_seed(2017)\n",
    "model = MaskedAutoencoder(7, [5, 4], nn.ReLU)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
    "_set_seed(2022)\n",
    "x1 = torch.randn(2, 7)\n",
    "model.train_step(x1, optimizer)\n",
    "assert torch.allclose(\n",
    "    model.encoder[0].weight.view(-1)[2:6],\n",
    "    torch.tensor([-0.09830013662576675, -0.22797375917434692, 0.004662647843360901, 0.3852387070655823]),\n",
    "    rtol=1e-03\n",
    ")\n",
    "TO_SAVE[\"mae\"] = model.encoder[2].weight.view(-1)[3:7].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Autoencoders\n",
    "\n",
    "In this section, you will learn how to train and evaluate autoencoders. After each training epoch, you will calculate the linear probe accuracy on the test split of your dataset.\n",
    "\n",
    "To achieve this, you will first use your trained autoencoder to encode each example in the dataset $x_i$ into its latent representation $z_i$. Then, you will use these latent representations $z_i$, along with their corresponding labels $y_i$, to train a simple linear classifier called a linear probe.\n",
    "\n",
    "The linear probe accuracy is the classification accuracy of this linear classifier on the test split of the dataset. By calculating this accuracy, you can evaluate how well your autoencoder is able to capture the important features of the data and how useful those features are for downstream tasks like classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, dataset, model: nn.Module,\n",
    "                 batch_size: int, num_classes: int, lr: float,\n",
    "                 probe_train_batch = \"full\", probe_train_epochs: int = 50):\n",
    "        self.train_batch_size = batch_size\n",
    "        self.eval_batch_size = 200\n",
    "        self.dataset = dataset\n",
    "        self.model = model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.num_classes = num_classes\n",
    "        self.probe_train_batch = probe_train_batch\n",
    "        self.probe_train_epochs = probe_train_epochs\n",
    "\n",
    "    def train(self, num_epochs: int) -> dict:\n",
    "        self.model.train()\n",
    "        train_losses, valid_losses, probe_accs = [], [], []\n",
    "\n",
    "        pbar = tqdm(range(num_epochs))\n",
    "        num_batches = self.dataset.num_train_samples // self.train_batch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            valid_loss = self.get_loss(split=\"valid\")\n",
    "        valid_losses.append(valid_loss)\n",
    "        probe_accs.append(\n",
    "            self.evaluate_w_linear_probe(self.model.hidden_sizes[-1]))\n",
    "        for epoch in pbar:\n",
    "            for batch_idx in range(num_batches):\n",
    "                x, y = self.dataset.get_batch(batch_idx, split=\"train\")\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "                loss = self.model.train_step(x, self.optimizer)\n",
    "                train_losses.append(loss.item())\n",
    "                pbar.set_description(f\"Epoch {epoch}, Loss {loss.item():.4f}\")\n",
    "            with torch.no_grad():\n",
    "                valid_loss = self.get_loss(split=\"valid\")\n",
    "            valid_losses.append(valid_loss)\n",
    "            probe_accs.append(\n",
    "                self.evaluate_w_linear_probe(self.model.hidden_sizes[-1]))\n",
    "\n",
    "        return {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"valid_losses\": valid_losses,\n",
    "            \"valid_accs\": probe_accs\n",
    "        }\n",
    "\n",
    "    def get_loss(self, split=\"train\") -> float:\n",
    "        \"\"\"\n",
    "        Compute the average loss of the model on a specified dataset split.\n",
    "\n",
    "        Parameters:\n",
    "        - split (str, optional): The dataset split to compute the loss on.\n",
    "        \n",
    "        Returns:\n",
    "        The average loss of the model on the specified dataset split.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        num_samples = self.dataset.get_num_samples(split=split)\n",
    "        num_batches = num_samples // self.eval_batch_size\n",
    "        assert num_samples % self.eval_batch_size == 0\n",
    "        losses = []\n",
    "        for batch_idx in range(num_batches):\n",
    "            x, y = self.dataset.get_batch(batch_idx, split=split)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            loss = self.model.get_loss(x)\n",
    "            losses.append(loss.item())\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def _get_model_accuracy(self, classifier: nn.Module, split=\"test\") -> float:\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the model on a specified dataset split using a\n",
    "        given linear classifier (a.k.a. a linear probe). This method is invoked\n",
    "        by `eval_w_linear_probe` that is defined below.\n",
    "\n",
    "        Parameters:\n",
    "        - classifier (nn.Module): The linear classifier to use for computing the\n",
    "          accuracy.\n",
    "        - split (str, optional): The dataset split to compute the accuracy on.\n",
    "        \n",
    "        Returns:\n",
    "        The accuracy of the model on the specified dataset split.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        num_samples, num_correct = 0, 0\n",
    "        num_batches = self.dataset.num_test_samples // self.eval_batch_size\n",
    "        assert num_samples % self.eval_batch_size == 0\n",
    "        for batch_idx in range(num_batches):\n",
    "            x, y = self.dataset.get_batch(batch_idx, split=\"test\")\n",
    "            ####################################################################\n",
    "            # TODO: Implement the following code in the evaluation loop to\n",
    "            #       calculate accuracy using the autoencoder and the given\n",
    "            #       classifier\n",
    "            ####################################################################\n",
    "            raise NotImplementedError()\n",
    "            num_samples += 0\n",
    "            num_correct += 0\n",
    "            ####################################################################\n",
    "        return num_correct / num_samples * 100\n",
    "        \n",
    "    def evaluate_w_linear_probe(self, feats_dim) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model using a linear probe on a small subset of the labeled\n",
    "        data.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        - feats_dim (int): The number of features in the model's output.\n",
    "        - num_epochs (int, optional): The number of epochs to train the linear\n",
    "          probe for. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "        The accuracy of the model computed using the linear probe.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        probe = nn.Linear(feats_dim, self.num_classes)\n",
    "        probe.cuda()\n",
    "        probe.train()\n",
    "        probe_opt = optim.Adam(probe.parameters(), lr=1e-3)\n",
    "\n",
    "        if self.probe_train_batch == \"full\":\n",
    "            num_batches = (\n",
    "                self.dataset.get_num_samples(split=\"valid\")\n",
    "                // self.eval_batch_size\n",
    "            )\n",
    "        else:\n",
    "            num_batches = self.probe_train_batch\n",
    "\n",
    "        frozen_batch = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx in np.random.permutation(num_batches):\n",
    "                x, y = self.dataset.get_batch(batch_idx, split=\"valid\")\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "                feat = self.model.encode(x)\n",
    "                frozen_batch.append((feat.cpu(), y.cpu()))\n",
    "\n",
    "        for epoch in range(self.probe_train_epochs):\n",
    "            for feat, y in frozen_batch:\n",
    "                feat, y = feat.cuda(), y.cuda()\n",
    "                y_hat = probe(feat)\n",
    "                loss = F.cross_entropy(y_hat, y)\n",
    "                probe_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                probe_opt.step()\n",
    "\n",
    "        # Evaluate linear probe\n",
    "        probe.eval()\n",
    "        with torch.no_grad():\n",
    "            accuracy = self._get_model_accuracy(classifier=probe)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_seed(2017)\n",
    "exp = Experiment(SyntheticDataset(7, 800, [0.4, 0.1, 0.5], 2, 15, 10), Autoencoder(7, [5, 4], nn.ReLU), 10, 2, 1e-03)\n",
    "classifier = nn.Linear(4, 2).cuda()\n",
    "assert np.allclose(exp._get_model_accuracy(classifier), 53.00)\n",
    "_set_seed(2022)\n",
    "exp = Experiment(SyntheticDataset(7, 800, [0.4, 0.1, 0.5], 2, 15, 10), Autoencoder(7, [5, 4], nn.ReLU), 10, 2, 1e-03)\n",
    "classifier = nn.Linear(4, 2).cuda()\n",
    "TO_SAVE[\"exp\"] = exp._get_model_accuracy(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear AutoEncoders on Synthetic Dataset\n",
    "\n",
    "In this experiment, we aim to investigate the performance of linear autoencoders/DAEs/MAEs on a synthetic dataset where there are 20 significant dimensions. Specifically, we will train four different autoencoder architectures with bottleneck sizes of 5, 20, 100, and 500. We will evaluate their performance on the synthetic dataset and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = {\n",
    "    \"vanilla\": Autoencoder,\n",
    "    \"denoise\": DenoisingAutoencoder,\n",
    "    \"masking\": MaskedAutoencoder,\n",
    "}\n",
    "\n",
    "# we repeat each experiment and report mean performance\n",
    "NUM_REPEATS = 3\n",
    "\n",
    "data_cfg = argparse.Namespace(\n",
    "    input_dims=100,\n",
    "    num_samples=20000,\n",
    "    data_splits=[0.7, 0.2, 0.1],\n",
    "    num_high_var_dims=20,\n",
    "    var_scale=10,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "hparams = argparse.Namespace(\n",
    "    batch_size=100,\n",
    "    num_epochs=10,\n",
    "    hidden_dims=[0],  # placeholder\n",
    "    activation=\"Identity\",  # linear AE\n",
    "    lr=5e-4\n",
    ")\n",
    "\n",
    "dataset = SyntheticDataset(\n",
    "    data_cfg.input_dims,\n",
    "    data_cfg.num_samples,\n",
    "    data_cfg.data_splits,\n",
    "    data_cfg.num_high_var_dims,\n",
    "    data_cfg.var_scale,\n",
    "    hparams.batch_size\n",
    ")\n",
    "\n",
    "# logging metrics\n",
    "train_losses, valid_losses = {}, {}\n",
    "accuracy = {}\n",
    "\n",
    "# run experiment w/ different models\n",
    "for model_idx, model_cls in MODELS.items():\n",
    "    for hidden_dim in [5, 20, 100, 500]:\n",
    "        hparams.hidden_dims = [hidden_dim]\n",
    "        feats_dim = hparams.hidden_dims[-1]\n",
    "        _train_loss, _valid_loss, _acc = [], [], []\n",
    "        for expid in range(NUM_REPEATS):\n",
    "            _set_seed(expid * 227)\n",
    "            print(\"run : {},  model : {},  hidden_dim : {}\".format(\n",
    "                expid, model_idx, feats_dim))\n",
    "            model = model_cls(\n",
    "                data_cfg.input_dims,\n",
    "                hparams.hidden_dims,\n",
    "                activation_cls=getattr(nn, hparams.activation)\n",
    "            )\n",
    "            experiment = Experiment(\n",
    "                dataset,\n",
    "                model,\n",
    "                batch_size=hparams.batch_size,\n",
    "                num_classes=data_cfg.num_classes,\n",
    "                lr=hparams.lr\n",
    "            )\n",
    "            _set_seed(1998 + expid * 227)\n",
    "            train_stats = experiment.train(num_epochs=hparams.num_epochs)\n",
    "            _train_loss.append(train_stats[\"train_losses\"])\n",
    "            _valid_loss.append(train_stats[\"valid_losses\"])\n",
    "            _acc.append(train_stats[\"valid_accs\"])\n",
    "\n",
    "        train_losses[(model_idx, feats_dim)] = _train_loss\n",
    "        valid_losses[(model_idx, feats_dim)] = _valid_loss\n",
    "        accuracy[(model_idx, feats_dim)] = _acc\n",
    "\n",
    "TO_SAVE[\"train1\"] = {\n",
    "    \"train_losses\": {f\"{k[0]}.{k[1]}\": v for k, v in train_losses.items()},\n",
    "    \"valid_losses\": {f\"{k[0]}.{k[1]}\": v for k, v in valid_losses.items()},\n",
    "    \"accuracy\": {f\"{k[0]}.{k[1]}\": v for k, v in accuracy.items()}\n",
    "}\n",
    "\n",
    "# report accuracy\n",
    "for model_idx, acc in accuracy.items():\n",
    "    print(\"Model : {}, Avg Accuracy : {}\".format(\n",
    "        model_idx, np.array(acc).mean(axis=0)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Training Curves\n",
    "\n",
    "You have saved all the training logs for the autoencoder models with different feature dimensions. In this section, you will implement a function to visualize the training curves and accuracy using linear probes. This visualization will help you compare the performance of autoencoder models of different hidden sizes.\n",
    "\n",
    "To begin with, you need to visualize the training curves of the **vanilla** autoencoder with latent representations of different feature dimensions. You will need to complete the following code to draw two plots:\n",
    "\n",
    "- The x-axis of both plots should represent the training epochs, while the y-axis of the first plot should display the validation loss (reconstruction error) and the y-axis of the second plot should display the linear probe accuracy.\n",
    "- Both plots should be line plots with four curves, each representing a feature dimension of 5, 20, 100, and 1000, respectively.\n",
    "- Each curve should have a major line and an area around the line:\n",
    "  - The major line should have one dot for each epoch showing the average validation loss/accuracy of that epoch over three runs.\n",
    "  - The area should be filled between the minimum and maximum validation loss/accuracy of that epoch across the three runs.\n",
    "  - The color of the line, dots, and area should be the same, with the area being translucent.\n",
    "- Both plots should include axis labels (for the x and y axis), a legend, and a title.\n",
    "\n",
    "Ensure that your implementation accurately reflects the requirements outlined above.\n",
    "\n",
    "**Documents for reference:**\n",
    "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html\n",
    "- https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single(values, feats_dim):\n",
    "    ############################################################################\n",
    "    # TODO: Implement the following code to draw a single curve with a filled\n",
    "    #       area around it.\n",
    "    #\n",
    "    # Hint 1: `values` is a list of three lists, where each list corresponds to\n",
    "    #         one run and each entry in the list corresponds to an epoch\n",
    "    # Hint 2: `feats_dim` is useful for showing legends\n",
    "    ############################################################################\n",
    "    raise NotImplementedError()\n",
    "    ############################################################################\n",
    "\n",
    "# Visualize valid losses\n",
    "for model_idx in valid_losses.keys():\n",
    "    if model_idx[0] == 'vanilla':\n",
    "        plot_single(valid_losses[model_idx], model_idx[1])\n",
    "################################################################################\n",
    "# TODO: Implement the following code to draw legends, axis labels, and the title\n",
    "################################################################################\n",
    "raise NotImplementedError()\n",
    "################################################################################\n",
    "plt.show()\n",
    "\n",
    "# Visualize valid (linear probe) accuracy\n",
    "for model_idx in accuracy.keys():\n",
    "    if model_idx[0] == 'vanilla':\n",
    "        plot_single(accuracy[model_idx], model_idx[1])\n",
    "################################################################################\n",
    "# TODO: Implement the following code to draw legends, axis labels, and the title\n",
    "################################################################################\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Linear Prob Accuracy\")\n",
    "################################################################################\n",
    "plt.show()\n",
    "\n",
    "\n",
    "TO_SAVE[\"vis_fn\"] = inspect.getsource(plot_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "**Screenshot your visualization above** and include it in your submission of the written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "In your written assignment submission, please answer the following question: **How does changing the latent representation size of the autoencoder affect the model's performance in terms of reconstruction accuracy and linear probe accuracy? Why?** Hint: each datapoint in the synthetic dataset has 100 dimensions, with 20 high-variance dimensions that affect the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Dimensionality Reduction on MNIST\n",
    "\n",
    "In the previous section, we observed that there is no advantage in terms of linear probe accuracy when we perform dimension reduction. The reason for this is that we use the entire *labeled* validation dataset to train the linear probe, rendering the use of autoencoders and self-supervised learning less useful in cases where we have ample labeled training data.\n",
    "\n",
    "In this part, we will consider a different scenario where we have an abundance of *unlabeled* training data, but only a limited number of *labeled* examples. Specifically, we will train a non-linear autoencoder on the MNIST dataset using all images in the dataset, but only **200** labeled examples will be used to train the linear probe.\n",
    "\n",
    "Your task is to train a non-linear autoencoder on the MNIST dataset. The objective is to achieve a few-shot linear probe accuracy of at least **79%** on the last epoch, averaged over two random runs. You may use any type of autoencoder that you have previously implemented, choose any latent representation sizes, and your grade will be evaluated on a linear scale, ranging from 0 to the maximum score.\n",
    "\n",
    "The validation accuracy achieved on the last epoch should range from 70% to 79%. If the accuracy is less than 70%, you will receive a score of 0, and if it is greater than 79%, you will receive the full score for this autograding item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change these\n",
    "NUM_REPEATS = 2\n",
    "input_dims = 28 * 28\n",
    "num_classes = 10\n",
    "data_splits = [0.9, 0.1]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO: Set the hyperparameters\n",
    "################################################################################\n",
    "hparams = argparse.Namespace(\n",
    "    batch_size=100,\n",
    "    num_epochs=5,\n",
    "    hidden_dims=[20],\n",
    "    activation=\"ReLU\",\n",
    "    lr=1e-4,\n",
    "    noise_std=0.5,\n",
    "    mask_prob=0.25\n",
    ")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO: Define a function to build the model. You are encouraged to experiment\n",
    "#       with different types of autoencoders\n",
    "################################################################################\n",
    "def build_model():\n",
    "    return Autoencoder(\n",
    "        input_dims,\n",
    "        hparams.hidden_dims,\n",
    "        activation_cls=getattr(nn, hparams.activation)\n",
    "    )\n",
    "################################################################################\n",
    "\n",
    "dataset = MNIST(hparams.batch_size, data_splits)\n",
    "\n",
    "feats_dim = hparams.hidden_dims[-1]\n",
    "train_loss, valid_loss, acc = [], [], []\n",
    "for expid in range(NUM_REPEATS):\n",
    "    _set_seed(expid * 227)\n",
    "    model = build_model()\n",
    "    experiment = Experiment(\n",
    "        dataset,\n",
    "        model,\n",
    "        batch_size=hparams.batch_size,\n",
    "        num_classes=num_classes,\n",
    "        lr=hparams.lr,\n",
    "        probe_train_batch=1,  # 1 batch = 200 examples\n",
    "        probe_train_epochs=1000\n",
    "    )\n",
    "    _set_seed(1998 + expid * 227)\n",
    "    train_stats = experiment.train(num_epochs=hparams.num_epochs)\n",
    "    train_loss.append(train_stats[\"train_losses\"])\n",
    "    valid_loss.append(train_stats[\"valid_losses\"])\n",
    "    acc.append(train_stats[\"valid_accs\"])\n",
    "\n",
    "TO_SAVE[\"train2\"] = {\n",
    "    \"train_loss\": train_loss,\n",
    "    \"valid_loss\": valid_loss,\n",
    "    \"acc\": acc,\n",
    "    \"hparams\": hparams.__dict__,\n",
    "    \"build_model_fn\": inspect.getsource(build_model),\n",
    "}\n",
    "\n",
    "print(\"Avg Accuracy:\", np.array(acc).mean(axis=0)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All done\n",
    "\n",
    "You are done with this coding assignment. Please download `submission_log.json` and submit it to Gradescope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"submission_log.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TO_SAVE, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
