{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Last Name Classification\n",
    "\n",
    "Welcome to this assignment where you will train a neural network to predict the probable language of origin for a given last name / family name in Latin alphabets.\n",
    "\n",
    "Throughout this task, you will gain expertise in the following areas:\n",
    "\n",
    "- Preprocessing raw text data for suitable input into an RNN and (Optionally) LSTM.\n",
    "- Utilizing PyTorch to train your recurrent neural network models.\n",
    "- Evaluating your model's performance and making predictions on unseen data.\n",
    "\n",
    "LSTM is out-of-scope this semester and will not be covered in the exams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    !wget https://download.pytorch.org/tutorial/data.zip\n",
    "    !unzip data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, make sure you have all these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"\"\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "sys.path.append(root_folder)\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from utils import validate_to_array\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import IPython\n",
    "from ipywidgets import interactive, widgets, Layout\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this task is to predict the probability of a given class given a last name, represented as\n",
    "\n",
    "$$ \\Pr ( y | x_1, x_2, x_3, ..., x_i),$$\n",
    "\n",
    "where $y$ is the category label and each $x_i$ is a character in the last name. Building a basic character-level NLP model has the advantage of understanding how the preprocessing works at a granular level. The character-level network reads words as a sequence of characters, producing a prediction and \"hidden state\" at each step by feeding its previous hidden state into the next step. The final prediction corresponds to the class to which the word belongs.\n",
    "\n",
    "All models in PyTorch inherit from the nn.Module subclass. In this assignment, you will **implement a custom model named `RecurrentClassifier`** that runs either [nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) or [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) and define its forward function. The implementation of LSTMs is *optional*.\n",
    "\n",
    "The forward pass of the model can be visualized with the following diagram:\n",
    "\n",
    "```\n",
    "[Embedding] -> [RNN Stack] -> [Extract Last Position] -> [Classifier]\n",
    "```\n",
    "\n",
    "- **Embedding:** This component maps each input word (integer) to a vector of real numbers.\n",
    "    - Input: `[batch_size, seq_len]`\n",
    "    - Output: `[batch_size, seq_len, rnn_size]`\n",
    "- **RNN Stack:** This component consists of one or more RNN layers, which process the input sequence of vectors from the Embedding component.\n",
    "    - Input: `[batch_size, seq_len, rnn_size]`\n",
    "    - Output: `[batch_size, seq_len, rnn_size]`\n",
    "- **Extract Last Position:** The RNN Stack component returns a sequence of vectors for each input example. However, for classification purposes, we only need a single vector that captures the full information of the input example. Since the RNN is left-to-right by default, the output state vector at the last position contains the full information of the input example. Therefore, for the $i$-th input example, we extract the output state vector at the last *non-pad* position, which is indicated by `last_pos[i]`.\n",
    "    - Input: `[batch_size, seq_len, rnn_size]`\n",
    "    - Output: `[batch_size, rnn_size]`\n",
    "- **Classifier:** This component is a fully-connected layer that maps the output vectors extracted in the previous step to logits (scores before softmax), which can be used to make predictions about the language of origin for each input example.\n",
    "    - Input: `[batch_size, rnn_size]`\n",
    "    - Output: `[batch_size, n_categories]`\n",
    "\n",
    "\n",
    "These documents would be helpful in this part:\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.gather.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.view.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        rnn_size: int,\n",
    "        n_categories: int,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        model_type: str = 'lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn_size = rnn_size\n",
    "        self.model_type = model_type\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Create an embedding layer of shape [vocab_size, rnn_size]\n",
    "        #\n",
    "        # Hint: Use nn.Embedding\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        # It will map each word into a vector of shape [rnn_size]\n",
    "        ########################################################################\n",
    "        self.embedding = NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Create a RNN stack with `num_layers` layers with tanh\n",
    "        #       nonlinearity. Between each layers, there is a dropout of\n",
    "        #       `dropout`. Implement it with a *single* call to `torch.nn` APIs\n",
    "        # \n",
    "        # Hint: See documentations at\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        # Set the arguments to call `nn.RNN` such that:\n",
    "        # - The shape of the input is [batch_size, seq_len, rnn_size]\n",
    "        # - The shape of the output should be [batch_size, seq_len, rnn_size]\n",
    "        # Make sure that the dimension ordering is correct. One of the argument\n",
    "        #   in the constructor of `nn.RNN` (or `nn.LSTM`) is helpful here\n",
    "        #\n",
    "        # Optional: Implement one LSTM layer when `model_type` is `lstm`\n",
    "        ########################################################################\n",
    "        if model_type == 'lstm':\n",
    "            self.lstm = NotImplementedError()\n",
    "        elif model_type == 'rnn':\n",
    "            self.rnn = NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Implement one dropout layer and the fully-connected classifier\n",
    "        #       layer\n",
    "        #\n",
    "        # Hint: We add a dropout layer because neither nn.RNN nor nn.LSTM\n",
    "        #   implements dropout after the last layer in the stack.\n",
    "        # Since the input to the classifier is the output of the last position\n",
    "        #   of the RNN's final layer, it has a shape of [batch_size, rnn_size].\n",
    "        # The expected output should be logits, which correspond to scores\n",
    "        #   before applying softmax, and should have a shape of\n",
    "        #   [batch_size, n_categories].\n",
    "        ########################################################################\n",
    "        self.drop = NotImplementedError()\n",
    "        self.output = NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor, last_pos: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: integer tensor of shape [batch_size, seq_len]\n",
    "        last_pos: integer tensor of shape [batch_size]\n",
    "        \n",
    "        The input tensor `x` is composed of a batch of sequences, where each\n",
    "        sequence contains indices corresponding to characters. As sequences\n",
    "        within the same batch may have different lengths, shorter sequences are\n",
    "        padded on the right side to match the maximum sequence length of the\n",
    "        batch, which is represented by `seq_len`.\n",
    "        \n",
    "        Additionally, the `last_pos` tensor records the position of the last\n",
    "        character in each sequence. For instance, the first sequence in the\n",
    "        batch can be represented as `[x[0, 0], x[0, 1], ..., x[0, last_pos[0]]`.\n",
    "        `last_pos` is useful when extracting the output state associated with\n",
    "        each sequence from the RNNs.\n",
    "        \"\"\"\n",
    "        embeds = self.embedding(x)\n",
    "        if self.model_type == 'lstm':\n",
    "            rnn_out, _ = self.lstm(embeds)\n",
    "        else:\n",
    "            rnn_out, _ = self.rnn(embeds)\n",
    "        \n",
    "        ########################################################################\n",
    "        # TODO: Retrieve the output state associated with each sequence\n",
    "        #\n",
    "        # Hints:\n",
    "        # - The output state of all positions is returned by the RNN stack,\n",
    "        #   but we only need the state in the last position for classification\n",
    "        #   - The shape of `rnn_out` is [batch_size, seq_len, rnn_size]\n",
    "        #   - The expected shape of `out` is [batch_size, rnn_size]\n",
    "        # - For the i-th sequence, we have out[i] == rnn_out[i, last_pos[i]]\n",
    "        # - Try to condense your code into a single line, without using any\n",
    "        #   loops. However, if you find it too challenging to do so, you may use\n",
    "        #   a single layer of for-loop.\n",
    "        ########################################################################\n",
    "        out = NotImplementedError()\n",
    "        ########################################################################\n",
    "\n",
    "        out = self.drop(out)\n",
    "        logits = self.output(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing your implementation, ensure that it passes the following tests. If your implementation fails some tests, but you believe that your implementation is correct, please post the error message along with a brief description on Ed. Please refrain from posting your actual code on Ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 227\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "model = RecurrentClassifier(11, 13, 17, 2, 0.1, 'rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(model.state_dict().keys()) == ['embedding.weight',\n",
    " 'rnn.weight_ih_l0',\n",
    " 'rnn.weight_hh_l0',\n",
    " 'rnn.bias_ih_l0',\n",
    " 'rnn.bias_hh_l0',\n",
    " 'rnn.weight_ih_l1',\n",
    " 'rnn.weight_hh_l1',\n",
    " 'rnn.bias_ih_l1',\n",
    " 'rnn.bias_hh_l1',\n",
    " 'output.weight',\n",
    " 'output.bias']\n",
    "assert model.embedding.weight.shape == torch.Size([11, 13])\n",
    "assert (\n",
    "    model.rnn.weight_ih_l0.shape\n",
    "    == model.rnn.weight_hh_l0.shape\n",
    "    == model.rnn.weight_ih_l1.shape\n",
    "    == model.rnn.weight_hh_l1.shape\n",
    "    == torch.Size([13, 13])\n",
    ")\n",
    "assert (\n",
    "    model.rnn.bias_ih_l0.shape\n",
    "    == model.rnn.bias_hh_l0.shape\n",
    "    == model.rnn.bias_ih_l1.shape\n",
    "    == model.rnn.bias_hh_l1.shape\n",
    "    == torch.Size([13])\n",
    ")\n",
    "assert model.output.weight.shape == torch.Size([17, 13])\n",
    "assert model.output.bias.shape == torch.Size([17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(20).view(5, 4) % 11\n",
    "last_pos = torch.tensor([2, 3, 1, 2, 3])\n",
    "seed = 1025\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "logits = model(x, last_pos)\n",
    "assert logits.shape == torch.Size([5, 17])\n",
    "assert torch.allclose(\n",
    "    logits.view(-1)[40:45],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            -0.27393126487731934,\n",
    "            0.28421181440353394,\n",
    "            0.2342953234910965,\n",
    "            0.23580458760261536,\n",
    "            0.06812290847301483\n",
    "        ],\n",
    "        dtype=torch.float\n",
    "    )\n",
    ")\n",
    "\n",
    "model.zero_grad()\n",
    "logits.sum().backward()\n",
    "assert torch.allclose(\n",
    "    model.rnn.weight_hh_l0.grad.view(-1)[40:45],\n",
    "    torch.tensor(\n",
    "        [\n",
    "            -0.9424352645874023,\n",
    "            -0.488606333732605,\n",
    "            0.6905138492584229,\n",
    "            -0.0017577260732650757,\n",
    "            1.1024625301361084\n",
    "        ],\n",
    "        dtype=torch.float\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset\n",
    "\n",
    "The [dataset](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html) contains a few thousand surnames from 18 languages of origin. Included in the data/names directory are 18 text files named as \"[Language].txt\". Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\n",
    "\n",
    "We'll end up with a dictionary of lists of names per language, {language: [names ...]}.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "assert findFiles('data/names/*.txt'), \"Data not found!\"\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(\"The normalized form of\", 'Ślusàrski', \"is\", unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the function to encode a letter to an integer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    ############################################################################\n",
    "    # TODO: implement the function to map a letter (a character) into its index\n",
    "    #       in `all_letters`\n",
    "    #\n",
    "    # e.g. letterToIndex(\"a\") == 0\n",
    "    # Don't worry about efficiency here\n",
    "    ############################################################################\n",
    "    return NotImplementedError()\n",
    "    ############################################################################\n",
    "\n",
    "assert letterToIndex(\"a\") == 0\n",
    "assert letterToIndex(\"'\") == 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labels, we must have numbers instead of a string. These dictionaries convert\n",
    "# between these two ways of representing the labels.\n",
    "num_to_cat = dict(enumerate(category_lines.keys()))\n",
    "cat_to_num = dict((v,k) for k,v in num_to_cat.items())\n",
    "\n",
    "pad = 57 # this is the next available character \n",
    "vocab_size = 58 # number of characters used in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data():\n",
    "  '''\n",
    "  category_lines: a dictionary of lists of names per language, {language: [names ...]}. \n",
    "\n",
    "  We want to translate our dictionary into a dataset that has one entry per name. \n",
    "  Each datapoint is a 3-tuple consisting of: \n",
    "  - x: a length-19 array with each character in the name as an element,\n",
    "   padded with zeros at the end if the name is less than 19 characters.\n",
    "  - y: the numerical representation of the language the name corresponds to.\n",
    "  - index: the index of the last non-pad token\n",
    "  '''\n",
    "  data = []\n",
    "  for cat in category_lines:\n",
    "    for name in category_lines[cat]:\n",
    "      token = np.ones(19, dtype=np.int64) * pad\n",
    "      numerized = np.array([letterToIndex(l) for l in name])\n",
    "      n = len(numerized)\n",
    "      token[:n] = numerized\n",
    "      data.append((token, cat_to_num[cat], n-1))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_data()\n",
    "seed = 227\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(data) * 0.8)\n",
    "train_data = data[:n_train]\n",
    "test_data = data[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will be faster if you use the Colab GPU. If it's not already enabled, do so with Runtime -> Change runtime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, indices):\n",
    "    '''\n",
    "    Helper function for creating a batch during training. Builds a batch \n",
    "    of source and target elements from the dataset. See the next cell for \n",
    "    when and how it's used. \n",
    "    \n",
    "    Arguments:\n",
    "        dataset: List[db_element] -- A list of dataset elements\n",
    "        indices: List[int] -- A list of indices of the dataset to sample\n",
    "    Returns:\n",
    "        batch_input: List[List[int]] -- List of tensorized names\n",
    "        batch_target: List[int] -- List of numerical categories\n",
    "        batch_indices: List[int] -- List of starting indices of padding\n",
    "    '''\n",
    "    # Recover what the entries for the batch are\n",
    "    batch = [dataset[i] for i in indices]\n",
    "    batch_input = np.array(list(zip(*batch))[0])\n",
    "    batch_target = np.array(list(zip(*batch))[1])\n",
    "    batch_indices = np.array(list(zip(*batch))[2])\n",
    "    return batch_input, batch_target, batch_indices # lines, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_batch(train_data, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjust the hyperparameters listed below** to train an RNN with a minimum evaluation accuracy of 80% after 20 epochs. Your score will be graded on a linear scale, ranging from 0 to the maximum score, as the validation accuracy achieved after the last epoch changes from 70% to 80% (i.e., you get 0 if the accuracy is less than 70%, and get the full score if the accuracy is greater than 80% for this autograding item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# The build_batch function outputs numpy, but our model is built in pytorch,\n",
    "# so we need to convert numpy to pytorch with the correct types. \n",
    "batch_to_torch = lambda b_in,b_target,b_mask: (torch.tensor(b_in).long(),\n",
    "                                               torch.tensor(b_target).long(), \n",
    "                                               torch.tensor(b_mask).long())\n",
    "\n",
    "############################################################################\n",
    "# TODO: Tune these hyperparameters for a better performance\n",
    "############################################################################\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "optimizer_class = optim.Adam\n",
    "lr = 1e-4\n",
    "batch_size = 256\n",
    "############################################################################\n",
    "\n",
    "# Do not change the number of epochs\n",
    "epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"You are using\", device, \"for training\")\n",
    "list_to_device = lambda th_obj: [tensor.to(device) for tensor in th_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# lstm_model = RecurrentClassifier(vocab_size=vocab_size, rnn_size=hidden_size, n_categories=n_categories, num_layers=num_layers, dropout=dropout, model_type='lstm')\n",
    "# lstm_optimizer = optimizer_class(lstm_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1998\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rnn_model = RecurrentClassifier(vocab_size=vocab_size, rnn_size=hidden_size, n_categories=n_categories, num_layers=num_layers, dropout=dropout, model_type='rnn')\n",
    "rnn_optimizer = optimizer_class(rnn_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs, batch_size, seed):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        random.seed(seed + epoch)\n",
    "        np.random.seed(seed + epoch)\n",
    "        torch.manual_seed(seed + epoch)\n",
    "        indices = np.random.permutation(range(len(train_data)))\n",
    "        n_correct, n_total = 0, 0\n",
    "        progress_bar = tqdm(range(0, (len(train_data) // batch_size) + 1))\n",
    "        for i in progress_bar:\n",
    "            batch = build_batch(train_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "            (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "            (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "            logits = model(batch_input, batch_indices)\n",
    "            loss = criterion(logits, batch_target)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            n_correct += (predictions == batch_target).sum().item()\n",
    "            n_total += batch_target.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                progress_bar.set_description(f\"Epoch: {epoch}  Iteration: {i}  Loss: {np.mean(train_losses[-10:])}\")\n",
    "        train_accuracies.append(n_correct / n_total * 100)\n",
    "        print(f\"Epoch: {epoch}  Train Accuracy: {n_correct / n_total * 100}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            indices = list(range(len(test_data)))\n",
    "            n_correct, n_total = 0, 0\n",
    "            for i in range(0, (len(test_data) // batch_size) + 1):\n",
    "                batch = build_batch(test_data, indices[i*batch_size:(i+1)*batch_size])\n",
    "                (batch_input, batch_target, batch_indices) = batch_to_torch(*batch)\n",
    "                (batch_input, batch_target, batch_indices) = list_to_device((batch_input, batch_target, batch_indices))\n",
    "\n",
    "                logits = model(batch_input, batch_indices)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                n_correct += (predictions == batch_target).sum().item()\n",
    "                n_total += batch_target.size(0)\n",
    "            eval_accuracies.append(n_correct / n_total * 100)\n",
    "            print(f\"Epoch: {epoch}  Eval Accuracy: {n_correct / n_total * 100}\")\n",
    "    \n",
    "    to_save = {\n",
    "        \"history\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"eval_accuracies\": eval_accuracies,\n",
    "        },\n",
    "        \"hparams\": {\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dropout\": dropout,\n",
    "            \"optimizer_class\": optimizer_class.__name__,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": epochs,\n",
    "            \"seed\": seed\n",
    "        },\n",
    "        \"model\": [\n",
    "            (name, list(param.shape))\n",
    "            for name, param in rnn_model.named_parameters()\n",
    "        ]\n",
    "    }\n",
    "    return to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_log = train(rnn_model, rnn_optimizer, criterion, epochs, batch_size, 1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = len(rnn_log[\"history\"][\"train_losses\"])\n",
    "plt.plot(range(n_steps), rnn_log[\"history\"][\"train_losses\"], alpha=0.4, color=\"blue\")\n",
    "moving_avg = np.convolve(np.array(rnn_log[\"history\"][\"train_losses\"]), np.ones(10), 'valid') / 10\n",
    "plt.plot(range(9, n_steps), moving_avg.tolist(), color=\"blue\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.title(\"RNN Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn_log[\"history\"][\"train_accuracies\"], marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"RNN Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn_log[\"history\"][\"eval_accuracies\"], marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"RNN Eval Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# train(lstm_model, lstm_optimizer, criterion, epochs, batch_size, 1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Your RNN: Try Your Own Name\n",
    "\n",
    "Attempt to use the code cells below to **predict the origin of your own last name**.\n",
    "\n",
    "Please refrain from entering the last names of your classmates, as the names you enter will be logged for anti-plagiarism purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_name(name, model):\n",
    "    '''\n",
    "    Numerize the name and return the most likely number representation of the \n",
    "    predicted class.\n",
    "    '''\n",
    "    # change this if your last name is longer than 19 characters\n",
    "    token = np.ones(19, dtype=np.int64) * pad\n",
    "    numerized = np.array([letterToIndex(l) for l in name])\n",
    "    n = len(numerized)\n",
    "    token[:n] = numerized\n",
    "    print(token)\n",
    "    logits = model(\n",
    "        torch.tensor(token, dtype=torch.long)[None, :],\n",
    "        torch.tensor([n - 1], dtype=torch.long)\n",
    "    )\n",
    "    return logits.argmax(dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn_model\n",
    "model.eval()\n",
    "model.cpu()\n",
    "############################################################################\n",
    "# TODO: Enter your last name\n",
    "############################################################################\n",
    "name = NotImplementedError()\n",
    "############################################################################\n",
    "rnn_log[\"last_name\"] = name\n",
    "rnn_log[\"source_init\"] = inspect.getsource(RecurrentClassifier.__init__)\n",
    "rnn_log[\"source_forward\"] = inspect.getsource(RecurrentClassifier.forward)\n",
    "print(\"Predicting origin language for name: \"+ name)\n",
    "c = classify_name(name, model)\n",
    "print(num_to_cat[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Although the neural network you have trained is intended to predict the language of origin for a given last name, it could potentially be misused. **In what ways do you think this could be problematic in real-world applications?** Include your answer in your submission of the written assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
